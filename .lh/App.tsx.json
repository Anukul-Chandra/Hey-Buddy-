{
    "sourceFile": "App.tsx",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 21,
            "patches": [
                {
                    "date": 1768924820751,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1768927092014,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,14 +6,8 @@\n import {\n   Mic,\n   MicOff,\n   Heart,\n-  Menu,\n-  X,\n-  Plus,\n-  Trash2,\n-  User,\n-  Sparkles,\n   Zap\n } from 'lucide-react';\n \n /* -------------------- TYPES -------------------- */\n@@ -33,16 +27,12 @@\n   const [currentInput, setCurrentInput] = useState('');\n   const [currentOutput, setCurrentOutput] = useState('');\n   const [error, setError] = useState<string | null>(null);\n \n-  const [bondScore, setBondScore] = useState(10);\n-  const [proLevel, setProLevel] = useState(5);\n+  const [bondScore] = useState(10);\n+  const [proLevel] = useState(5);\n   const [currentMood, setCurrentMood] = useState('NEUTRAL');\n \n-  const [isSidebarOpen, setIsSidebarOpen] = useState(false);\n-  const [chatHistory, setChatHistory] = useState<SavedChat[]>([]);\n-  const [activeChatId, setActiveChatId] = useState<string | null>(null);\n-\n   /* -------- AUDIO & SESSION REFS -------- */\n   const inputAudioCtx = useRef<AudioContext | null>(null);\n   const outputAudioCtx = useRef<AudioContext | null>(null);\n   const micStream = useRef<MediaStream | null>(null);\n@@ -54,44 +44,21 @@\n   const activeSessionRef = useRef<any>(null);\n \n   const transcriptEndRef = useRef<HTMLDivElement>(null);\n \n-  /* -------------------- EFFECTS -------------------- */\n   useEffect(() => {\n-    const saved = localStorage.getItem('hey_buddy_history_v2');\n-    if (saved) setChatHistory(JSON.parse(saved));\n-  }, []);\n-\n-  useEffect(() => {\n     transcriptEndRef.current?.scrollIntoView({ behavior: 'smooth' });\n   }, [messages, currentInput, currentOutput]);\n \n-  /* -------------------- HELPERS -------------------- */\n-  const parseMetadata = (text: string) => {\n-    const match = text.match(/\\[METADATA\\]([\\s\\S]*?)\\[\\/METADATA\\]/);\n-    if (!match) return text;\n-\n-    const meta = match[1];\n-    const mood = meta.match(/MOOD:\\s*(\\w+)/);\n-    const bond = meta.match(/BOND_SCORE:\\s*(\\d+)/);\n-    const level = meta.match(/PRO_LEVEL:\\s*(\\d+)/);\n-\n-    if (mood) setCurrentMood(mood[1]);\n-    if (bond) setBondScore(+bond[1]);\n-    if (level) setProLevel(+level[1]);\n-\n-    return text.replace(match[0], '').trim();\n+  /* -------------------- PERMISSION CHECK -------------------- */\n+  const getMicPermissionState = async (): Promise<'granted' | 'prompt' | 'denied'> => {\n+    if (!navigator.permissions) return 'prompt';\n+    const status = await navigator.permissions.query({\n+      name: 'microphone' as PermissionName,\n+    });\n+    return status.state;\n   };\n \n-  const moodColor = useMemo(() => {\n-    switch (currentMood) {\n-      case 'ROMANTIC': return 'rose';\n-      case 'DEEP': return 'indigo';\n-      case 'HAPPY': return 'amber';\n-      default: return 'rose';\n-    }\n-  }, [currentMood]);\n-\n   /* -------------------- CLEANUP -------------------- */\n   const cleanup = () => {\n     setIsConnected(false);\n     setIsProcessing(false);\n@@ -126,22 +93,30 @@\n     try {\n       setError(null);\n       setIsProcessing(true);\n \n-      /* üîê VITE ENV FIX */\n+      // üîç CHECK PERMISSION STATE FIRST\n+      const permission = await getMicPermissionState();\n+      if (permission === 'denied') {\n+        setError(\n+          'Microphone is blocked. Please enable microphone access from browser or system settings and reload the page.'\n+        );\n+        setIsProcessing(false);\n+        return;\n+      }\n+\n+      /* üîê GEMINI INIT */\n       const ai = new GoogleGenAI({\n         apiKey: import.meta.env.VITE_GEMINI_API_KEY,\n       });\n \n-      /* üéß AUDIO CONTEXT (USER GESTURE SAFE) */\n+      /* üéß AUDIO CONTEXT (must be resumed on click) */\n       inputAudioCtx.current = new AudioContext({ sampleRate: 16000 });\n       outputAudioCtx.current = new AudioContext({ sampleRate: 24000 });\n-\n-      /* üî• CRITICAL FIX */\n       await inputAudioCtx.current.resume();\n       await outputAudioCtx.current.resume();\n \n-      /* üé§ MIC ACCESS */\n+      /* üé§ THIS LINE TRIGGERS THE BROWSER POPUP */\n       micStream.current = await navigator.mediaDevices.getUserMedia({\n         audio: {\n           channelCount: 1,\n           sampleRate: 16000,\n@@ -198,9 +173,9 @@\n             if (m.serverContent?.turnComplete) {\n               setMessages(prev => [\n                 ...prev,\n                 { role: 'user', text: currentInput.trim() },\n-                { role: 'model', text: parseMetadata(currentOutput.trim()) },\n+                { role: 'model', text: currentOutput.trim() },\n               ]);\n               setCurrentInput('');\n               setCurrentOutput('');\n             }\n@@ -226,10 +201,10 @@\n       });\n \n       sessionPromiseRef.current = sessionPromise;\n       sessionPromise.then((s: any) => (activeSessionRef.current = s));\n-    } catch (err) {\n-      setError('Microphone permission blocked');\n+    } catch {\n+      setError('Microphone permission was not granted.');\n       setIsProcessing(false);\n     }\n   };\n \n@@ -249,38 +224,37 @@\n           </span>\n         </div>\n       </header>\n \n-      <main className=\"flex-1 overflow-y-auto px-6 space-y-6\">\n-        {messages.map((m, i) => (\n-          <div key={i} className={`flex ${m.role === 'user' ? 'justify-end' : 'justify-start'}`}>\n-            <div className={`max-w-[80%] p-5 rounded-3xl ${\n-              m.role === 'user' ? 'bg-rose-600' : 'bg-white/10'\n-            }`}>\n-              {m.text}\n-            </div>\n-          </div>\n-        ))}\n-        {currentInput && <div className=\"italic opacity-50\">{currentInput}</div>}\n-        {currentOutput && <div className=\"opacity-70\">{currentOutput}</div>}\n-        <div ref={transcriptEndRef} />\n-      </main>\n-\n-      <footer className=\"p-8 flex flex-col items-center\">\n+      <main className=\"flex-1 flex flex-col justify-center items-center gap-6\">\n         <button\n           onClick={handleConnect}\n           disabled={isProcessing}\n-          className={`w-24 h-24 rounded-full flex items-center justify-center transition-all ${\n-            isConnected ? 'bg-neutral-800' : 'bg-rose-600'\n+          className={`w-28 h-28 rounded-full flex items-center justify-center transition-all ${\n+            isConnected ? 'bg-neutral-800' : 'bg-rose-600 hover:scale-110'\n           }`}\n         >\n-          {isConnected ? <MicOff size={40} /> : <Mic size={40} />}\n+          {isConnected ? <MicOff size={42} /> : <Mic size={42} />}\n         </button>\n-        <p className=\"mt-4 text-xs uppercase tracking-widest\">\n+\n+        <p className=\"text-xs uppercase tracking-widest opacity-80\">\n           {isConnected ? 'Listening...' : 'Connect with Buddy'}\n         </p>\n-        {error && <p className=\"text-red-400 text-xs mt-2\">{error}</p>}\n-      </footer>\n+\n+        {!isConnected && (\n+          <p className=\"text-[11px] opacity-50 text-center max-w-xs\">\n+            Click the button. Your browser will ask for microphone permission.\n+          </p>\n+        )}\n+\n+        {error && (\n+          <p className=\"text-red-400 text-xs text-center max-w-xs\">\n+            {error}\n+          </p>\n+        )}\n+      </main>\n+\n+      <div ref={transcriptEndRef} />\n     </div>\n   );\n };\n \n"
                },
                {
                    "date": 1768927331387,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,8 +6,14 @@\n import {\n   Mic,\n   MicOff,\n   Heart,\n+  Menu,\n+  X,\n+  Plus,\n+  Trash2,\n+  User,\n+  Sparkles,\n   Zap\n } from 'lucide-react';\n \n /* -------------------- TYPES -------------------- */\n@@ -27,12 +33,16 @@\n   const [currentInput, setCurrentInput] = useState('');\n   const [currentOutput, setCurrentOutput] = useState('');\n   const [error, setError] = useState<string | null>(null);\n \n-  const [bondScore] = useState(10);\n-  const [proLevel] = useState(5);\n+  const [bondScore, setBondScore] = useState(10);\n+  const [proLevel, setProLevel] = useState(5);\n   const [currentMood, setCurrentMood] = useState('NEUTRAL');\n \n+  const [isSidebarOpen, setIsSidebarOpen] = useState(false);\n+  const [chatHistory, setChatHistory] = useState<SavedChat[]>([]);\n+  const [activeChatId, setActiveChatId] = useState<string | null>(null);\n+\n   /* -------- AUDIO & SESSION REFS -------- */\n   const inputAudioCtx = useRef<AudioContext | null>(null);\n   const outputAudioCtx = useRef<AudioContext | null>(null);\n   const micStream = useRef<MediaStream | null>(null);\n@@ -44,13 +54,45 @@\n   const activeSessionRef = useRef<any>(null);\n \n   const transcriptEndRef = useRef<HTMLDivElement>(null);\n \n+  /* -------------------- EFFECTS -------------------- */\n   useEffect(() => {\n+    const saved = localStorage.getItem('hey_buddy_history_v2');\n+    if (saved) setChatHistory(JSON.parse(saved));\n+  }, []);\n+\n+  useEffect(() => {\n     transcriptEndRef.current?.scrollIntoView({ behavior: 'smooth' });\n   }, [messages, currentInput, currentOutput]);\n \n-  /* -------------------- PERMISSION CHECK -------------------- */\n+  /* -------------------- HELPERS -------------------- */\n+  const parseMetadata = (text: string) => {\n+    const match = text.match(/\\[METADATA\\]([\\s\\S]*?)\\[\\/METADATA\\]/);\n+    if (!match) return text;\n+\n+    const meta = match[1];\n+    const mood = meta.match(/MOOD:\\s*(\\w+)/);\n+    const bond = meta.match(/BOND_SCORE:\\s*(\\d+)/);\n+    const level = meta.match(/PRO_LEVEL:\\s*(\\d+)/);\n+\n+    if (mood) setCurrentMood(mood[1]);\n+    if (bond) setBondScore(+bond[1]);\n+    if (level) setProLevel(+level[1]);\n+\n+    return text.replace(match[0], '').trim();\n+  };\n+\n+  const moodColor = useMemo(() => {\n+    switch (currentMood) {\n+      case 'ROMANTIC': return 'rose';\n+      case 'DEEP': return 'indigo';\n+      case 'HAPPY': return 'amber';\n+      default: return 'rose';\n+    }\n+  }, [currentMood]);\n+\n+  /* -------------------- MIC PERMISSION HELPER -------------------- */\n   const getMicPermissionState = async (): Promise<'granted' | 'prompt' | 'denied'> => {\n     if (!navigator.permissions) return 'prompt';\n     const status = await navigator.permissions.query({\n       name: 'microphone' as PermissionName,\n@@ -93,30 +135,29 @@\n     try {\n       setError(null);\n       setIsProcessing(true);\n \n-      // üîç CHECK PERMISSION STATE FIRST\n+      // ‚úÖ Permission state check\n       const permission = await getMicPermissionState();\n       if (permission === 'denied') {\n         setError(\n-          'Microphone is blocked. Please enable microphone access from browser or system settings and reload the page.'\n+          'Microphone is blocked. Please enable microphone access from browser or system settings and reload.'\n         );\n         setIsProcessing(false);\n         return;\n       }\n \n-      /* üîê GEMINI INIT */\n       const ai = new GoogleGenAI({\n         apiKey: import.meta.env.VITE_GEMINI_API_KEY,\n       });\n \n-      /* üéß AUDIO CONTEXT (must be resumed on click) */\n       inputAudioCtx.current = new AudioContext({ sampleRate: 16000 });\n       outputAudioCtx.current = new AudioContext({ sampleRate: 24000 });\n+\n       await inputAudioCtx.current.resume();\n       await outputAudioCtx.current.resume();\n \n-      /* üé§ THIS LINE TRIGGERS THE BROWSER POPUP */\n+      // üé§ THIS triggers browser popup\n       micStream.current = await navigator.mediaDevices.getUserMedia({\n         audio: {\n           channelCount: 1,\n           sampleRate: 16000,\n@@ -139,9 +180,8 @@\n \n       source.connect(processorNode.current);\n       processorNode.current.connect(inputAudioCtx.current.destination);\n \n-      /* ü§ñ GEMINI LIVE SESSION */\n       const sessionPromise = ai.live.connect({\n         model: 'gemini-2.5-flash-native-audio-preview-12-2025',\n         callbacks: {\n           onopen: () => {\n@@ -173,9 +213,9 @@\n             if (m.serverContent?.turnComplete) {\n               setMessages(prev => [\n                 ...prev,\n                 { role: 'user', text: currentInput.trim() },\n-                { role: 'model', text: currentOutput.trim() },\n+                { role: 'model', text: parseMetadata(currentOutput.trim()) },\n               ]);\n               setCurrentInput('');\n               setCurrentOutput('');\n             }\n@@ -207,9 +247,9 @@\n       setIsProcessing(false);\n     }\n   };\n \n-  /* -------------------- UI -------------------- */\n+  /* -------------------- UI (UNCHANGED) -------------------- */\n   return (\n     <div className=\"flex flex-col h-screen bg-black text-white\">\n       <header className=\"p-6 flex justify-between items-center\">\n         <h1 className=\"text-2xl font-black\">\n@@ -224,37 +264,38 @@\n           </span>\n         </div>\n       </header>\n \n-      <main className=\"flex-1 flex flex-col justify-center items-center gap-6\">\n+      <main className=\"flex-1 overflow-y-auto px-6 space-y-6\">\n+        {messages.map((m, i) => (\n+          <div key={i} className={`flex ${m.role === 'user' ? 'justify-end' : 'justify-start'}`}>\n+            <div className={`max-w-[80%] p-5 rounded-3xl ${\n+              m.role === 'user' ? 'bg-rose-600' : 'bg-white/10'\n+            }`}>\n+              {m.text}\n+            </div>\n+          </div>\n+        ))}\n+        {currentInput && <div className=\"italic opacity-50\">{currentInput}</div>}\n+        {currentOutput && <div className=\"opacity-70\">{currentOutput}</div>}\n+        <div ref={transcriptEndRef} />\n+      </main>\n+\n+      <footer className=\"p-8 flex flex-col items-center\">\n         <button\n           onClick={handleConnect}\n           disabled={isProcessing}\n-          className={`w-28 h-28 rounded-full flex items-center justify-center transition-all ${\n-            isConnected ? 'bg-neutral-800' : 'bg-rose-600 hover:scale-110'\n+          className={`w-24 h-24 rounded-full flex items-center justify-center transition-all ${\n+            isConnected ? 'bg-neutral-800' : 'bg-rose-600'\n           }`}\n         >\n-          {isConnected ? <MicOff size={42} /> : <Mic size={42} />}\n+          {isConnected ? <MicOff size={40} /> : <Mic size={40} />}\n         </button>\n-\n-        <p className=\"text-xs uppercase tracking-widest opacity-80\">\n+        <p className=\"mt-4 text-xs uppercase tracking-widest\">\n           {isConnected ? 'Listening...' : 'Connect with Buddy'}\n         </p>\n-\n-        {!isConnected && (\n-          <p className=\"text-[11px] opacity-50 text-center max-w-xs\">\n-            Click the button. Your browser will ask for microphone permission.\n-          </p>\n-        )}\n-\n-        {error && (\n-          <p className=\"text-red-400 text-xs text-center max-w-xs\">\n-            {error}\n-          </p>\n-        )}\n-      </main>\n-\n-      <div ref={transcriptEndRef} />\n+        {error && <p className=\"text-red-400 text-xs mt-2\">{error}</p>}\n+      </footer>\n     </div>\n   );\n };\n \n"
                },
                {
                    "date": 1768927572717,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,14 +6,8 @@\n import {\n   Mic,\n   MicOff,\n   Heart,\n-  Menu,\n-  X,\n-  Plus,\n-  Trash2,\n-  User,\n-  Sparkles,\n   Zap\n } from 'lucide-react';\n \n /* -------------------- TYPES -------------------- */\n@@ -37,12 +31,8 @@\n   const [bondScore, setBondScore] = useState(10);\n   const [proLevel, setProLevel] = useState(5);\n   const [currentMood, setCurrentMood] = useState('NEUTRAL');\n \n-  const [isSidebarOpen, setIsSidebarOpen] = useState(false);\n-  const [chatHistory, setChatHistory] = useState<SavedChat[]>([]);\n-  const [activeChatId, setActiveChatId] = useState<string | null>(null);\n-\n   /* -------- AUDIO & SESSION REFS -------- */\n   const inputAudioCtx = useRef<AudioContext | null>(null);\n   const outputAudioCtx = useRef<AudioContext | null>(null);\n   const micStream = useRef<MediaStream | null>(null);\n@@ -56,17 +46,12 @@\n   const transcriptEndRef = useRef<HTMLDivElement>(null);\n \n   /* -------------------- EFFECTS -------------------- */\n   useEffect(() => {\n-    const saved = localStorage.getItem('hey_buddy_history_v2');\n-    if (saved) setChatHistory(JSON.parse(saved));\n-  }, []);\n-\n-  useEffect(() => {\n     transcriptEndRef.current?.scrollIntoView({ behavior: 'smooth' });\n   }, [messages, currentInput, currentOutput]);\n \n-  /* -------------------- HELPERS -------------------- */\n+  /* -------------------- METADATA PARSER -------------------- */\n   const parseMetadata = (text: string) => {\n     const match = text.match(/\\[METADATA\\]([\\s\\S]*?)\\[\\/METADATA\\]/);\n     if (!match) return text;\n \n@@ -81,26 +66,8 @@\n \n     return text.replace(match[0], '').trim();\n   };\n \n-  const moodColor = useMemo(() => {\n-    switch (currentMood) {\n-      case 'ROMANTIC': return 'rose';\n-      case 'DEEP': return 'indigo';\n-      case 'HAPPY': return 'amber';\n-      default: return 'rose';\n-    }\n-  }, [currentMood]);\n-\n-  /* -------------------- MIC PERMISSION HELPER -------------------- */\n-  const getMicPermissionState = async (): Promise<'granted' | 'prompt' | 'denied'> => {\n-    if (!navigator.permissions) return 'prompt';\n-    const status = await navigator.permissions.query({\n-      name: 'microphone' as PermissionName,\n-    });\n-    return status.state;\n-  };\n-\n   /* -------------------- CLEANUP -------------------- */\n   const cleanup = () => {\n     setIsConnected(false);\n     setIsProcessing(false);\n@@ -135,29 +102,20 @@\n     try {\n       setError(null);\n       setIsProcessing(true);\n \n-      // ‚úÖ Permission state check\n-      const permission = await getMicPermissionState();\n-      if (permission === 'denied') {\n-        setError(\n-          'Microphone is blocked. Please enable microphone access from browser or system settings and reload.'\n-        );\n-        setIsProcessing(false);\n-        return;\n-      }\n-\n       const ai = new GoogleGenAI({\n         apiKey: import.meta.env.VITE_GEMINI_API_KEY,\n       });\n \n+      /* üéß AUDIO CONTEXTS (must be inside click) */\n       inputAudioCtx.current = new AudioContext({ sampleRate: 16000 });\n       outputAudioCtx.current = new AudioContext({ sampleRate: 24000 });\n \n       await inputAudioCtx.current.resume();\n       await outputAudioCtx.current.resume();\n \n-      // üé§ THIS triggers browser popup\n+      /* üî• THIS LINE TRIGGERS BROWSER POPUP */\n       micStream.current = await navigator.mediaDevices.getUserMedia({\n         audio: {\n           channelCount: 1,\n           sampleRate: 16000,\n@@ -242,9 +200,9 @@\n \n       sessionPromiseRef.current = sessionPromise;\n       sessionPromise.then((s: any) => (activeSessionRef.current = s));\n     } catch {\n-      setError('Microphone permission was not granted.');\n+      setError('Please allow microphone access to continue.');\n       setIsProcessing(false);\n     }\n   };\n \n@@ -267,11 +225,13 @@\n \n       <main className=\"flex-1 overflow-y-auto px-6 space-y-6\">\n         {messages.map((m, i) => (\n           <div key={i} className={`flex ${m.role === 'user' ? 'justify-end' : 'justify-start'}`}>\n-            <div className={`max-w-[80%] p-5 rounded-3xl ${\n-              m.role === 'user' ? 'bg-rose-600' : 'bg-white/10'\n-            }`}>\n+            <div\n+              className={`max-w-[80%] p-5 rounded-3xl ${\n+                m.role === 'user' ? 'bg-rose-600' : 'bg-white/10'\n+              }`}\n+            >\n               {m.text}\n             </div>\n           </div>\n         ))}\n"
                },
                {
                    "date": 1768928127059,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,9 +6,9 @@\n import {\n   Mic,\n   MicOff,\n   Heart,\n-  Zap\n+  Zap,\n } from 'lucide-react';\n \n /* -------------------- TYPES -------------------- */\n interface SavedChat {\n@@ -31,8 +31,11 @@\n   const [bondScore, setBondScore] = useState(10);\n   const [proLevel, setProLevel] = useState(5);\n   const [currentMood, setCurrentMood] = useState('NEUTRAL');\n \n+  // üîê SOFT MIC GATE\n+  const [showMicGate, setShowMicGate] = useState(false);\n+\n   /* -------- AUDIO & SESSION REFS -------- */\n   const inputAudioCtx = useRef<AudioContext | null>(null);\n   const outputAudioCtx = useRef<AudioContext | null>(null);\n   const micStream = useRef<MediaStream | null>(null);\n@@ -49,9 +52,9 @@\n   useEffect(() => {\n     transcriptEndRef.current?.scrollIntoView({ behavior: 'smooth' });\n   }, [messages, currentInput, currentOutput]);\n \n-  /* -------------------- METADATA PARSER -------------------- */\n+  /* -------------------- HELPERS -------------------- */\n   const parseMetadata = (text: string) => {\n     const match = text.match(/\\[METADATA\\]([\\s\\S]*?)\\[\\/METADATA\\]/);\n     if (!match) return text;\n \n@@ -102,30 +105,25 @@\n     try {\n       setError(null);\n       setIsProcessing(true);\n \n-      const ai = new GoogleGenAI({\n-        apiKey: import.meta.env.VITE_GEMINI_API_KEY,\n-      });\n-\n-      /* üéß AUDIO CONTEXTS (must be inside click) */\n-      inputAudioCtx.current = new AudioContext({ sampleRate: 16000 });\n-      outputAudioCtx.current = new AudioContext({ sampleRate: 24000 });\n-\n-      await inputAudioCtx.current.resume();\n-      await outputAudioCtx.current.resume();\n-\n-      /* üî• THIS LINE TRIGGERS BROWSER POPUP */\n+      // üé§ STEP 1: ASK MIC PERMISSION FIRST (popup here)\n       micStream.current = await navigator.mediaDevices.getUserMedia({\n         audio: {\n           channelCount: 1,\n-          sampleRate: 16000,\n           echoCancellation: true,\n           noiseSuppression: true,\n           autoGainControl: true,\n         },\n       });\n \n+      // üéß STEP 2: AUDIO CONTEXTS (AFTER permission)\n+      inputAudioCtx.current = new AudioContext({ sampleRate: 16000 });\n+      outputAudioCtx.current = new AudioContext({ sampleRate: 24000 });\n+\n+      await inputAudioCtx.current.resume();\n+      await outputAudioCtx.current.resume();\n+\n       const source = inputAudioCtx.current.createMediaStreamSource(micStream.current);\n       processorNode.current = inputAudioCtx.current.createScriptProcessor(256, 1, 1);\n \n       processorNode.current.onaudioprocess = e => {\n@@ -138,8 +136,13 @@\n \n       source.connect(processorNode.current);\n       processorNode.current.connect(inputAudioCtx.current.destination);\n \n+      // ü§ñ GEMINI\n+      const ai = new GoogleGenAI({\n+        apiKey: import.meta.env.VITE_GEMINI_API_KEY,\n+      });\n+\n       const sessionPromise = ai.live.connect({\n         model: 'gemini-2.5-flash-native-audio-preview-12-2025',\n         callbacks: {\n           onopen: () => {\n@@ -205,9 +208,9 @@\n       setIsProcessing(false);\n     }\n   };\n \n-  /* -------------------- UI (UNCHANGED) -------------------- */\n+  /* -------------------- UI -------------------- */\n   return (\n     <div className=\"flex flex-col h-screen bg-black text-white\">\n       <header className=\"p-6 flex justify-between items-center\">\n         <h1 className=\"text-2xl font-black\">\n@@ -225,13 +228,11 @@\n \n       <main className=\"flex-1 overflow-y-auto px-6 space-y-6\">\n         {messages.map((m, i) => (\n           <div key={i} className={`flex ${m.role === 'user' ? 'justify-end' : 'justify-start'}`}>\n-            <div\n-              className={`max-w-[80%] p-5 rounded-3xl ${\n-                m.role === 'user' ? 'bg-rose-600' : 'bg-white/10'\n-              }`}\n-            >\n+            <div className={`max-w-[80%] p-5 rounded-3xl ${\n+              m.role === 'user' ? 'bg-rose-600' : 'bg-white/10'\n+            }`}>\n               {m.text}\n             </div>\n           </div>\n         ))}\n@@ -241,21 +242,50 @@\n       </main>\n \n       <footer className=\"p-8 flex flex-col items-center\">\n         <button\n-          onClick={handleConnect}\n+          onClick={() => setShowMicGate(true)}\n           disabled={isProcessing}\n-          className={`w-24 h-24 rounded-full flex items-center justify-center transition-all ${\n-            isConnected ? 'bg-neutral-800' : 'bg-rose-600'\n-          }`}\n+          className=\"w-24 h-24 rounded-full bg-rose-600 flex items-center justify-center\"\n         >\n           {isConnected ? <MicOff size={40} /> : <Mic size={40} />}\n         </button>\n         <p className=\"mt-4 text-xs uppercase tracking-widest\">\n           {isConnected ? 'Listening...' : 'Connect with Buddy'}\n         </p>\n         {error && <p className=\"text-red-400 text-xs mt-2\">{error}</p>}\n       </footer>\n+\n+      {/* üé§ SOFT PRE-SCREEN */}\n+      {showMicGate && (\n+        <div className=\"fixed inset-0 bg-black/80 backdrop-blur-sm z-50 flex items-center justify-center\">\n+          <div className=\"bg-[#0b0b0b] border border-white/10 rounded-3xl p-8 w-[90%] max-w-sm text-center space-y-6\">\n+            <div className=\"text-5xl\">üé§</div>\n+            <h2 className=\"text-xl font-black\">Microphone Access Needed</h2>\n+            <p className=\"text-sm text-neutral-400\">\n+              To talk with Buddy, your browser will ask for microphone permission\n+              on the next step.\n+            </p>\n+            <div className=\"flex gap-4 justify-center\">\n+              <button\n+                onClick={() => setShowMicGate(false)}\n+                className=\"px-5 py-2 rounded-full bg-white/5\"\n+              >\n+                Cancel\n+              </button>\n+              <button\n+                onClick={() => {\n+                  setShowMicGate(false);\n+                  handleConnect();\n+                }}\n+                className=\"px-6 py-2 rounded-full bg-rose-600 font-bold\"\n+              >\n+                Continue\n+              </button>\n+            </div>\n+          </div>\n+        </div>\n+      )}\n     </div>\n   );\n };\n \n"
                },
                {
                    "date": 1768928541050,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,25 +1,11 @@\n-import React, { useState, useEffect, useRef, useMemo } from 'react';\n+import React, { useState, useEffect, useRef } from 'react';\n import { GoogleGenAI, Modality, LiveServerMessage } from '@google/genai';\n import { SYSTEM_INSTRUCTION, SAFETY_SETTINGS } from './constants';\n import { decode, decodeAudioData, createPcmBlob } from './services/audioUtils';\n import { ChatMessage } from './types';\n-import {\n-  Mic,\n-  MicOff,\n-  Heart,\n-  Zap,\n-} from 'lucide-react';\n+import { Mic, MicOff, Heart, Zap } from 'lucide-react';\n \n-/* -------------------- TYPES -------------------- */\n-interface SavedChat {\n-  id: string;\n-  timestamp: number;\n-  preview: string;\n-  messages: ChatMessage[];\n-  stats?: { bond: number; level: number; mood: string };\n-}\n-\n /* -------------------- APP -------------------- */\n const App: React.FC = () => {\n   const [isConnected, setIsConnected] = useState(false);\n   const [isProcessing, setIsProcessing] = useState(false);\n@@ -27,11 +13,10 @@\n   const [currentInput, setCurrentInput] = useState('');\n   const [currentOutput, setCurrentOutput] = useState('');\n   const [error, setError] = useState<string | null>(null);\n \n-  const [bondScore, setBondScore] = useState(10);\n-  const [proLevel, setProLevel] = useState(5);\n-  const [currentMood, setCurrentMood] = useState('NEUTRAL');\n+  const [bondScore] = useState(10);\n+  const [proLevel] = useState(5);\n \n   // üîê SOFT MIC GATE\n   const [showMicGate, setShowMicGate] = useState(false);\n \n@@ -47,30 +32,12 @@\n   const activeSessionRef = useRef<any>(null);\n \n   const transcriptEndRef = useRef<HTMLDivElement>(null);\n \n-  /* -------------------- EFFECTS -------------------- */\n   useEffect(() => {\n     transcriptEndRef.current?.scrollIntoView({ behavior: 'smooth' });\n   }, [messages, currentInput, currentOutput]);\n \n-  /* -------------------- HELPERS -------------------- */\n-  const parseMetadata = (text: string) => {\n-    const match = text.match(/\\[METADATA\\]([\\s\\S]*?)\\[\\/METADATA\\]/);\n-    if (!match) return text;\n-\n-    const meta = match[1];\n-    const mood = meta.match(/MOOD:\\s*(\\w+)/);\n-    const bond = meta.match(/BOND_SCORE:\\s*(\\d+)/);\n-    const level = meta.match(/PRO_LEVEL:\\s*(\\d+)/);\n-\n-    if (mood) setCurrentMood(mood[1]);\n-    if (bond) setBondScore(+bond[1]);\n-    if (level) setProLevel(+level[1]);\n-\n-    return text.replace(match[0], '').trim();\n-  };\n-\n   /* -------------------- CLEANUP -------------------- */\n   const cleanup = () => {\n     setIsConnected(false);\n     setIsProcessing(false);\n@@ -101,23 +68,29 @@\n       cleanup();\n       return;\n     }\n \n+    setError(null);\n+    setIsProcessing(true);\n+\n+    // 1Ô∏è‚É£ MIC PERMISSION (ONLY mic error here)\n     try {\n-      setError(null);\n-      setIsProcessing(true);\n-\n-      // üé§ STEP 1: ASK MIC PERMISSION FIRST (popup here)\n       micStream.current = await navigator.mediaDevices.getUserMedia({\n         audio: {\n           channelCount: 1,\n           echoCancellation: true,\n           noiseSuppression: true,\n           autoGainControl: true,\n         },\n       });\n+    } catch {\n+      setError('Microphone access was denied.');\n+      setIsProcessing(false);\n+      return;\n+    }\n \n-      // üéß STEP 2: AUDIO CONTEXTS (AFTER permission)\n+    try {\n+      // 2Ô∏è‚É£ AUDIO CONTEXT (AFTER permission)\n       inputAudioCtx.current = new AudioContext({ sampleRate: 16000 });\n       outputAudioCtx.current = new AudioContext({ sampleRate: 24000 });\n \n       await inputAudioCtx.current.resume();\n@@ -136,17 +109,18 @@\n \n       source.connect(processorNode.current);\n       processorNode.current.connect(inputAudioCtx.current.destination);\n \n-      // ü§ñ GEMINI\n+      // 3Ô∏è‚É£ GEMINI CONNECT\n       const ai = new GoogleGenAI({\n         apiKey: import.meta.env.VITE_GEMINI_API_KEY,\n       });\n \n       const sessionPromise = ai.live.connect({\n         model: 'gemini-2.5-flash-native-audio-preview-12-2025',\n         callbacks: {\n           onopen: () => {\n+            setError(null);              // üî• FIX: clear old error\n             setIsConnected(true);\n             setIsProcessing(false);\n           },\n           onmessage: async (m: LiveServerMessage) => {\n@@ -174,17 +148,17 @@\n             if (m.serverContent?.turnComplete) {\n               setMessages(prev => [\n                 ...prev,\n                 { role: 'user', text: currentInput.trim() },\n-                { role: 'model', text: parseMetadata(currentOutput.trim()) },\n+                { role: 'model', text: currentOutput.trim() },\n               ]);\n               setCurrentInput('');\n               setCurrentOutput('');\n             }\n           },\n           onclose: cleanup,\n           onerror: () => {\n-            setError('Connection error');\n+            setError('Voice service is currently unavailable.');\n             cleanup();\n           },\n         },\n         config: {\n@@ -202,11 +176,12 @@\n       });\n \n       sessionPromiseRef.current = sessionPromise;\n       sessionPromise.then((s: any) => (activeSessionRef.current = s));\n-    } catch {\n-      setError('Please allow microphone access to continue.');\n-      setIsProcessing(false);\n+    } catch (err) {\n+      console.error(err);\n+      setError('Voice service is currently unavailable.');\n+      cleanup();\n     }\n   };\n \n   /* -------------------- UI -------------------- */\n@@ -254,17 +229,16 @@\n         </p>\n         {error && <p className=\"text-red-400 text-xs mt-2\">{error}</p>}\n       </footer>\n \n-      {/* üé§ SOFT PRE-SCREEN */}\n+      {/* üé§ SOFT MIC GATE */}\n       {showMicGate && (\n         <div className=\"fixed inset-0 bg-black/80 backdrop-blur-sm z-50 flex items-center justify-center\">\n           <div className=\"bg-[#0b0b0b] border border-white/10 rounded-3xl p-8 w-[90%] max-w-sm text-center space-y-6\">\n             <div className=\"text-5xl\">üé§</div>\n             <h2 className=\"text-xl font-black\">Microphone Access Needed</h2>\n             <p className=\"text-sm text-neutral-400\">\n-              To talk with Buddy, your browser will ask for microphone permission\n-              on the next step.\n+              Your browser will ask for microphone permission on the next step.\n             </p>\n             <div className=\"flex gap-4 justify-center\">\n               <button\n                 onClick={() => setShowMicGate(false)}\n"
                },
                {
                    "date": 1768928854438,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -4,41 +4,32 @@\n import { decode, decodeAudioData, createPcmBlob } from './services/audioUtils';\n import { ChatMessage } from './types';\n import { Mic, MicOff, Heart, Zap } from 'lucide-react';\n \n-/* -------------------- APP -------------------- */\n const App: React.FC = () => {\n   const [isConnected, setIsConnected] = useState(false);\n   const [isProcessing, setIsProcessing] = useState(false);\n   const [messages, setMessages] = useState<ChatMessage[]>([]);\n   const [currentInput, setCurrentInput] = useState('');\n   const [currentOutput, setCurrentOutput] = useState('');\n   const [error, setError] = useState<string | null>(null);\n \n-  const [bondScore] = useState(10);\n-  const [proLevel] = useState(5);\n-\n-  // üîê SOFT MIC GATE\n   const [showMicGate, setShowMicGate] = useState(false);\n \n-  /* -------- AUDIO & SESSION REFS -------- */\n   const inputAudioCtx = useRef<AudioContext | null>(null);\n   const outputAudioCtx = useRef<AudioContext | null>(null);\n   const micStream = useRef<MediaStream | null>(null);\n   const processorNode = useRef<ScriptProcessorNode | null>(null);\n-  const nextStartTime = useRef(0);\n-  const audioSources = useRef<Set<AudioBufferSourceNode>>(new Set());\n \n-  const sessionPromiseRef = useRef<any>(null);\n+  const sessionRef = useRef<any>(null);\n   const activeSessionRef = useRef<any>(null);\n \n   const transcriptEndRef = useRef<HTMLDivElement>(null);\n \n   useEffect(() => {\n     transcriptEndRef.current?.scrollIntoView({ behavior: 'smooth' });\n   }, [messages, currentInput, currentOutput]);\n \n-  /* -------------------- CLEANUP -------------------- */\n   const cleanup = () => {\n     setIsConnected(false);\n     setIsProcessing(false);\n \n@@ -52,18 +43,13 @@\n     outputAudioCtx.current?.close();\n     inputAudioCtx.current = null;\n     outputAudioCtx.current = null;\n \n-    audioSources.current.forEach(s => s.stop());\n-    audioSources.current.clear();\n-    nextStartTime.current = 0;\n-\n-    sessionPromiseRef.current?.then((s: any) => s.close?.());\n-    sessionPromiseRef.current = null;\n+    sessionRef.current?.then((s: any) => s.close?.());\n+    sessionRef.current = null;\n     activeSessionRef.current = null;\n   };\n \n-  /* -------------------- MAIN CONNECT -------------------- */\n   const handleConnect = async () => {\n     if (isConnected) {\n       cleanup();\n       return;\n@@ -71,13 +57,12 @@\n \n     setError(null);\n     setIsProcessing(true);\n \n-    // 1Ô∏è‚É£ MIC PERMISSION (ONLY mic error here)\n+    /* -------- MIC PERMISSION -------- */\n     try {\n       micStream.current = await navigator.mediaDevices.getUserMedia({\n         audio: {\n-          channelCount: 1,\n           echoCancellation: true,\n           noiseSuppression: true,\n           autoGainControl: true,\n         },\n@@ -88,9 +73,9 @@\n       return;\n     }\n \n     try {\n-      // 2Ô∏è‚É£ AUDIO CONTEXT (AFTER permission)\n+      /* -------- AUDIO CONTEXT -------- */\n       inputAudioCtx.current = new AudioContext({ sampleRate: 16000 });\n       outputAudioCtx.current = new AudioContext({ sampleRate: 24000 });\n \n       await inputAudioCtx.current.resume();\n@@ -109,43 +94,26 @@\n \n       source.connect(processorNode.current);\n       processorNode.current.connect(inputAudioCtx.current.destination);\n \n-      // 3Ô∏è‚É£ GEMINI CONNECT\n+      /* -------- GEMINI (PUBLIC STABLE MODEL) -------- */\n       const ai = new GoogleGenAI({\n         apiKey: import.meta.env.VITE_GEMINI_API_KEY,\n       });\n \n       const sessionPromise = ai.live.connect({\n-        model: 'gemini-2.5-flash-native-audio-preview-12-2025',\n+        // üî• FIXED MODEL (PUBLIC + STABLE)\n+        model: 'gemini-1.5-pro',\n         callbacks: {\n           onopen: () => {\n-            setError(null);              // üî• FIX: clear old error\n+            setError(null);\n             setIsConnected(true);\n             setIsProcessing(false);\n           },\n           onmessage: async (m: LiveServerMessage) => {\n-            if (m.serverContent?.outputTranscription)\n-              setCurrentOutput(p => p + m.serverContent.outputTranscription!.text);\n+            if (m.serverContent?.outputText)\n+              setCurrentOutput(p => p + m.serverContent.outputText);\n \n-            if (m.serverContent?.inputTranscription)\n-              setCurrentInput(p => p + m.serverContent.inputTranscription!.text);\n-\n-            const audio = m.serverContent?.modelTurn?.parts?.[0]?.inlineData?.data;\n-            if (audio && outputAudioCtx.current) {\n-              const ctx = outputAudioCtx.current;\n-              nextStartTime.current = Math.max(ctx.currentTime, nextStartTime.current);\n-\n-              const buf = await decodeAudioData(decode(audio), ctx, 24000, 1);\n-              const src = ctx.createBufferSource();\n-              src.buffer = buf;\n-              src.connect(ctx.destination);\n-              src.start(nextStartTime.current);\n-              nextStartTime.current += buf.duration;\n-              audioSources.current.add(src);\n-              src.onended = () => audioSources.current.delete(src);\n-            }\n-\n             if (m.serverContent?.turnComplete) {\n               setMessages(prev => [\n                 ...prev,\n                 { role: 'user', text: currentInput.trim() },\n@@ -163,41 +131,33 @@\n         },\n         config: {\n           systemInstruction: SYSTEM_INSTRUCTION,\n           safetySettings: SAFETY_SETTINGS,\n-          responseModalities: [Modality.AUDIO],\n-          inputAudioTranscription: {},\n-          outputAudioTranscription: {},\n-          speechConfig: {\n-            voiceConfig: {\n-              prebuiltVoiceConfig: { voiceName: 'Kore' },\n-            },\n-          },\n+          responseModalities: [Modality.TEXT],\n         },\n       });\n \n-      sessionPromiseRef.current = sessionPromise;\n+      sessionRef.current = sessionPromise;\n       sessionPromise.then((s: any) => (activeSessionRef.current = s));\n     } catch (err) {\n       console.error(err);\n       setError('Voice service is currently unavailable.');\n       cleanup();\n     }\n   };\n \n-  /* -------------------- UI -------------------- */\n   return (\n     <div className=\"flex flex-col h-screen bg-black text-white\">\n       <header className=\"p-6 flex justify-between items-center\">\n         <h1 className=\"text-2xl font-black\">\n           Hey <span className=\"text-rose-500 italic\">Buddy</span>\n         </h1>\n         <div className=\"flex gap-3\">\n           <span className=\"text-xs bg-rose-500/10 px-3 py-1 rounded-full\">\n-            <Zap className=\"inline w-3 h-3 mr-1\" /> Level {proLevel}%\n+            <Zap className=\"inline w-3 h-3 mr-1\" /> Level 5%\n           </span>\n           <span className=\"text-xs bg-indigo-500/10 px-3 py-1 rounded-full\">\n-            <Heart className=\"inline w-3 h-3 mr-1\" /> Bond {bondScore}%\n+            <Heart className=\"inline w-3 h-3 mr-1\" /> Bond 10%\n           </span>\n         </div>\n       </header>\n \n@@ -210,10 +170,8 @@\n               {m.text}\n             </div>\n           </div>\n         ))}\n-        {currentInput && <div className=\"italic opacity-50\">{currentInput}</div>}\n-        {currentOutput && <div className=\"opacity-70\">{currentOutput}</div>}\n         <div ref={transcriptEndRef} />\n       </main>\n \n       <footer className=\"p-8 flex flex-col items-center\">\n@@ -229,9 +187,9 @@\n         </p>\n         {error && <p className=\"text-red-400 text-xs mt-2\">{error}</p>}\n       </footer>\n \n-      {/* üé§ SOFT MIC GATE */}\n+      {/* MIC PRE-SCREEN */}\n       {showMicGate && (\n         <div className=\"fixed inset-0 bg-black/80 backdrop-blur-sm z-50 flex items-center justify-center\">\n           <div className=\"bg-[#0b0b0b] border border-white/10 rounded-3xl p-8 w-[90%] max-w-sm text-center space-y-6\">\n             <div className=\"text-5xl\">üé§</div>\n"
                },
                {
                    "date": 1768929095114,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,153 +1,115 @@\n-import React, { useState, useEffect, useRef } from 'react';\n-import { GoogleGenAI, Modality, LiveServerMessage } from '@google/genai';\n-import { SYSTEM_INSTRUCTION, SAFETY_SETTINGS } from './constants';\n-import { decode, decodeAudioData, createPcmBlob } from './services/audioUtils';\n-import { ChatMessage } from './types';\n-import { Mic, MicOff, Heart, Zap } from 'lucide-react';\n+import React, { useEffect, useRef, useState } from \"react\";\n+import { GoogleGenAI } from \"@google/genai\";\n+import { Mic, MicOff, Heart, Zap } from \"lucide-react\";\n \n+type ChatMessage = {\n+  role: \"user\" | \"model\";\n+  text: string;\n+};\n+\n const App: React.FC = () => {\n-  const [isConnected, setIsConnected] = useState(false);\n-  const [isProcessing, setIsProcessing] = useState(false);\n+  const [isListening, setIsListening] = useState(false);\n+  const [error, setError] = useState<string | null>(null);\n   const [messages, setMessages] = useState<ChatMessage[]>([]);\n-  const [currentInput, setCurrentInput] = useState('');\n-  const [currentOutput, setCurrentOutput] = useState('');\n-  const [error, setError] = useState<string | null>(null);\n \n-  const [showMicGate, setShowMicGate] = useState(false);\n+  const recognitionRef = useRef<any>(null);\n \n-  const inputAudioCtx = useRef<AudioContext | null>(null);\n-  const outputAudioCtx = useRef<AudioContext | null>(null);\n-  const micStream = useRef<MediaStream | null>(null);\n-  const processorNode = useRef<ScriptProcessorNode | null>(null);\n+  /* ---------------- Gemini ---------------- */\n+  const ai = new GoogleGenAI({\n+    apiKey: import.meta.env.VITE_GEMINI_API_KEY,\n+  });\n \n-  const sessionRef = useRef<any>(null);\n-  const activeSessionRef = useRef<any>(null);\n-\n-  const transcriptEndRef = useRef<HTMLDivElement>(null);\n-\n+  /* ---------------- INIT SPEECH RECOGNITION ---------------- */\n   useEffect(() => {\n-    transcriptEndRef.current?.scrollIntoView({ behavior: 'smooth' });\n-  }, [messages, currentInput, currentOutput]);\n+    const SpeechRecognition =\n+      (window as any).SpeechRecognition ||\n+      (window as any).webkitSpeechRecognition;\n \n-  const cleanup = () => {\n-    setIsConnected(false);\n-    setIsProcessing(false);\n+    if (!SpeechRecognition) {\n+      setError(\"Speech Recognition not supported in this browser.\");\n+      return;\n+    }\n \n-    processorNode.current?.disconnect();\n-    processorNode.current = null;\n+    const recognition = new SpeechRecognition();\n+    recognition.lang = \"en-US\";\n+    recognition.interimResults = false;\n+    recognition.continuous = false;\n \n-    micStream.current?.getTracks().forEach(t => t.stop());\n-    micStream.current = null;\n+    recognition.onresult = async (event: any) => {\n+      const userText = event.results[0][0].transcript;\n \n-    inputAudioCtx.current?.close();\n-    outputAudioCtx.current?.close();\n-    inputAudioCtx.current = null;\n-    outputAudioCtx.current = null;\n+      setMessages((prev) => [...prev, { role: \"user\", text: userText }]);\n \n-    sessionRef.current?.then((s: any) => s.close?.());\n-    sessionRef.current = null;\n-    activeSessionRef.current = null;\n-  };\n+      try {\n+        const model = ai.getGenerativeModel({\n+          model: \"gemini-1.5-flash\",\n+        });\n \n-  const handleConnect = async () => {\n-    if (isConnected) {\n-      cleanup();\n-      return;\n-    }\n+        const result = await model.generateContent(userText);\n+        const reply = result.response.text();\n \n-    setError(null);\n-    setIsProcessing(true);\n+        setMessages((prev) => [\n+          ...prev,\n+          { role: \"model\", text: reply },\n+        ]);\n \n-    /* -------- MIC PERMISSION -------- */\n-    try {\n-      micStream.current = await navigator.mediaDevices.getUserMedia({\n-        audio: {\n-          echoCancellation: true,\n-          noiseSuppression: true,\n-          autoGainControl: true,\n-        },\n-      });\n-    } catch {\n-      setError('Microphone access was denied.');\n-      setIsProcessing(false);\n-      return;\n-    }\n+        speak(reply);\n+      } catch (e) {\n+        setError(\"AI response failed.\");\n+      }\n+    };\n \n-    try {\n-      /* -------- AUDIO CONTEXT -------- */\n-      inputAudioCtx.current = new AudioContext({ sampleRate: 16000 });\n-      outputAudioCtx.current = new AudioContext({ sampleRate: 24000 });\n+    recognition.onerror = () => {\n+      setError(\"Microphone permission denied or unavailable.\");\n+      setIsListening(false);\n+    };\n \n-      await inputAudioCtx.current.resume();\n-      await outputAudioCtx.current.resume();\n+    recognition.onend = () => {\n+      setIsListening(false);\n+    };\n \n-      const source = inputAudioCtx.current.createMediaStreamSource(micStream.current);\n-      processorNode.current = inputAudioCtx.current.createScriptProcessor(256, 1, 1);\n+    recognitionRef.current = recognition;\n+  }, []);\n \n-      processorNode.current.onaudioprocess = e => {\n-        if (!activeSessionRef.current) return;\n-        const input = e.inputBuffer.getChannelData(0);\n-        activeSessionRef.current.sendRealtimeInput({\n-          media: createPcmBlob(input),\n-        });\n-      };\n+  /* ---------------- TEXT TO SPEECH ---------------- */\n+  const speak = (text: string) => {\n+    const utterance = new SpeechSynthesisUtterance(text);\n+    utterance.lang = \"en-US\";\n+    utterance.rate = 0.95;\n+    utterance.pitch = 1;\n \n-      source.connect(processorNode.current);\n-      processorNode.current.connect(inputAudioCtx.current.destination);\n+    window.speechSynthesis.cancel();\n+    window.speechSynthesis.speak(utterance);\n+  };\n \n-      /* -------- GEMINI (PUBLIC STABLE MODEL) -------- */\n-      const ai = new GoogleGenAI({\n-        apiKey: import.meta.env.VITE_GEMINI_API_KEY,\n-      });\n+  /* ---------------- BUTTON HANDLER ---------------- */\n+  const handleConnect = () => {\n+    setError(null);\n \n-      const sessionPromise = ai.live.connect({\n-        // üî• FIXED MODEL (PUBLIC + STABLE)\n-        model: 'gemini-1.5-pro',\n-        callbacks: {\n-          onopen: () => {\n-            setError(null);\n-            setIsConnected(true);\n-            setIsProcessing(false);\n-          },\n-          onmessage: async (m: LiveServerMessage) => {\n-            if (m.serverContent?.outputText)\n-              setCurrentOutput(p => p + m.serverContent.outputText);\n+    if (!recognitionRef.current) {\n+      setError(\"Speech Recognition not available.\");\n+      return;\n+    }\n \n-            if (m.serverContent?.turnComplete) {\n-              setMessages(prev => [\n-                ...prev,\n-                { role: 'user', text: currentInput.trim() },\n-                { role: 'model', text: currentOutput.trim() },\n-              ]);\n-              setCurrentInput('');\n-              setCurrentOutput('');\n-            }\n-          },\n-          onclose: cleanup,\n-          onerror: () => {\n-            setError('Voice service is currently unavailable.');\n-            cleanup();\n-          },\n-        },\n-        config: {\n-          systemInstruction: SYSTEM_INSTRUCTION,\n-          safetySettings: SAFETY_SETTINGS,\n-          responseModalities: [Modality.TEXT],\n-        },\n-      });\n+    if (isListening) {\n+      recognitionRef.current.stop();\n+      setIsListening(false);\n+      return;\n+    }\n \n-      sessionRef.current = sessionPromise;\n-      sessionPromise.then((s: any) => (activeSessionRef.current = s));\n-    } catch (err) {\n-      console.error(err);\n-      setError('Voice service is currently unavailable.');\n-      cleanup();\n+    try {\n+      recognitionRef.current.start();\n+      setIsListening(true);\n+    } catch {\n+      setError(\"Unable to start microphone.\");\n     }\n   };\n \n+  /* ---------------- UI ---------------- */\n   return (\n     <div className=\"flex flex-col h-screen bg-black text-white\">\n+      {/* Header */}\n       <header className=\"p-6 flex justify-between items-center\">\n         <h1 className=\"text-2xl font-black\">\n           Hey <span className=\"text-rose-500 italic\">Buddy</span>\n         </h1>\n@@ -160,64 +122,49 @@\n           </span>\n         </div>\n       </header>\n \n+      {/* Chat */}\n       <main className=\"flex-1 overflow-y-auto px-6 space-y-6\">\n         {messages.map((m, i) => (\n-          <div key={i} className={`flex ${m.role === 'user' ? 'justify-end' : 'justify-start'}`}>\n-            <div className={`max-w-[80%] p-5 rounded-3xl ${\n-              m.role === 'user' ? 'bg-rose-600' : 'bg-white/10'\n-            }`}>\n+          <div\n+            key={i}\n+            className={`flex ${\n+              m.role === \"user\" ? \"justify-end\" : \"justify-start\"\n+            }`}\n+          >\n+            <div\n+              className={`max-w-[80%] p-5 rounded-3xl ${\n+                m.role === \"user\"\n+                  ? \"bg-rose-600\"\n+                  : \"bg-white/10\"\n+              }`}\n+            >\n               {m.text}\n             </div>\n           </div>\n         ))}\n-        <div ref={transcriptEndRef} />\n       </main>\n \n+      {/* Footer */}\n       <footer className=\"p-8 flex flex-col items-center\">\n         <button\n-          onClick={() => setShowMicGate(true)}\n-          disabled={isProcessing}\n-          className=\"w-24 h-24 rounded-full bg-rose-600 flex items-center justify-center\"\n+          onClick={handleConnect}\n+          className={`w-24 h-24 rounded-full flex items-center justify-center transition-all ${\n+            isListening ? \"bg-neutral-800\" : \"bg-rose-600\"\n+          }`}\n         >\n-          {isConnected ? <MicOff size={40} /> : <Mic size={40} />}\n+          {isListening ? <MicOff size={40} /> : <Mic size={40} />}\n         </button>\n+\n         <p className=\"mt-4 text-xs uppercase tracking-widest\">\n-          {isConnected ? 'Listening...' : 'Connect with Buddy'}\n+          {isListening ? \"Listening...\" : \"Connect with Buddy\"}\n         </p>\n-        {error && <p className=\"text-red-400 text-xs mt-2\">{error}</p>}\n+\n+        {error && (\n+          <p className=\"text-red-400 text-xs mt-2\">{error}</p>\n+        )}\n       </footer>\n-\n-      {/* MIC PRE-SCREEN */}\n-      {showMicGate && (\n-        <div className=\"fixed inset-0 bg-black/80 backdrop-blur-sm z-50 flex items-center justify-center\">\n-          <div className=\"bg-[#0b0b0b] border border-white/10 rounded-3xl p-8 w-[90%] max-w-sm text-center space-y-6\">\n-            <div className=\"text-5xl\">üé§</div>\n-            <h2 className=\"text-xl font-black\">Microphone Access Needed</h2>\n-            <p className=\"text-sm text-neutral-400\">\n-              Your browser will ask for microphone permission on the next step.\n-            </p>\n-            <div className=\"flex gap-4 justify-center\">\n-              <button\n-                onClick={() => setShowMicGate(false)}\n-                className=\"px-5 py-2 rounded-full bg-white/5\"\n-              >\n-                Cancel\n-              </button>\n-              <button\n-                onClick={() => {\n-                  setShowMicGate(false);\n-                  handleConnect();\n-                }}\n-                className=\"px-6 py-2 rounded-full bg-rose-600 font-bold\"\n-              >\n-                Continue\n-              </button>\n-            </div>\n-          </div>\n-        </div>\n-      )}\n     </div>\n   );\n };\n \n"
                },
                {
                    "date": 1768929464574,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,23 +1,32 @@\n import React, { useEffect, useRef, useState } from \"react\";\n import { GoogleGenAI } from \"@google/genai\";\n import { Mic, MicOff, Heart, Zap } from \"lucide-react\";\n \n+/* ---------------- TYPES ---------------- */\n type ChatMessage = {\n   role: \"user\" | \"model\";\n   text: string;\n };\n \n+/* ---------------- API KEY SAFE GUARD ---------------- */\n+const API_KEY = import.meta.env.VITE_GEMINI_API_KEY;\n+\n+if (!API_KEY) {\n+  console.error(\"‚ùå VITE_GEMINI_API_KEY is missing\");\n+}\n+\n+/* ---------------- APP ---------------- */\n const App: React.FC = () => {\n   const [isListening, setIsListening] = useState(false);\n   const [error, setError] = useState<string | null>(null);\n   const [messages, setMessages] = useState<ChatMessage[]>([]);\n \n   const recognitionRef = useRef<any>(null);\n \n-  /* ---------------- Gemini ---------------- */\n+  /* ---------------- INIT GEMINI ---------------- */\n   const ai = new GoogleGenAI({\n-    apiKey: import.meta.env.VITE_GEMINI_API_KEY,\n+    apiKey: API_KEY,\n   });\n \n   /* ---------------- INIT SPEECH RECOGNITION ---------------- */\n   useEffect(() => {\n@@ -25,16 +34,16 @@\n       (window as any).SpeechRecognition ||\n       (window as any).webkitSpeechRecognition;\n \n     if (!SpeechRecognition) {\n-      setError(\"Speech Recognition not supported in this browser.\");\n+      setError(\"Speech recognition is not supported in this browser.\");\n       return;\n     }\n \n     const recognition = new SpeechRecognition();\n     recognition.lang = \"en-US\";\n+    recognition.continuous = false;\n     recognition.interimResults = false;\n-    recognition.continuous = false;\n \n     recognition.onresult = async (event: any) => {\n       const userText = event.results[0][0].transcript;\n \n@@ -53,9 +62,10 @@\n           { role: \"model\", text: reply },\n         ]);\n \n         speak(reply);\n-      } catch (e) {\n+      } catch (err) {\n+        console.error(err);\n         setError(\"AI response failed.\");\n       }\n     };\n \n@@ -81,14 +91,19 @@\n     window.speechSynthesis.cancel();\n     window.speechSynthesis.speak(utterance);\n   };\n \n-  /* ---------------- BUTTON HANDLER ---------------- */\n+  /* ---------------- MIC BUTTON ---------------- */\n   const handleConnect = () => {\n     setError(null);\n \n+    if (!API_KEY) {\n+      setError(\"API key is missing. Please check .env.local\");\n+      return;\n+    }\n+\n     if (!recognitionRef.current) {\n-      setError(\"Speech Recognition not available.\");\n+      setError(\"Speech recognition not available.\");\n       return;\n     }\n \n     if (isListening) {\n@@ -100,9 +115,9 @@\n     try {\n       recognitionRef.current.start();\n       setIsListening(true);\n     } catch {\n-      setError(\"Unable to start microphone.\");\n+      setError(\"Unable to access microphone.\");\n     }\n   };\n \n   /* ---------------- UI ---------------- */\n@@ -160,9 +175,11 @@\n           {isListening ? \"Listening...\" : \"Connect with Buddy\"}\n         </p>\n \n         {error && (\n-          <p className=\"text-red-400 text-xs mt-2\">{error}</p>\n+          <p className=\"text-red-400 text-xs mt-2 text-center\">\n+            {error}\n+          </p>\n         )}\n       </footer>\n     </div>\n   );\n"
                },
                {
                    "date": 1768929761819,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,131 +1,87 @@\n import React, { useEffect, useRef, useState } from \"react\";\n-import { GoogleGenAI } from \"@google/genai\";\n import { Mic, MicOff, Heart, Zap } from \"lucide-react\";\n \n-/* ---------------- TYPES ---------------- */\n type ChatMessage = {\n   role: \"user\" | \"model\";\n   text: string;\n };\n \n-/* ---------------- API KEY SAFE GUARD ---------------- */\n-const API_KEY = import.meta.env.VITE_GEMINI_API_KEY;\n-\n-if (!API_KEY) {\n-  console.error(\"‚ùå VITE_GEMINI_API_KEY is missing\");\n-}\n-\n-/* ---------------- APP ---------------- */\n const App: React.FC = () => {\n   const [isListening, setIsListening] = useState(false);\n   const [error, setError] = useState<string | null>(null);\n   const [messages, setMessages] = useState<ChatMessage[]>([]);\n \n   const recognitionRef = useRef<any>(null);\n \n-  /* ---------------- INIT GEMINI ---------------- */\n-  const ai = new GoogleGenAI({\n-    apiKey: API_KEY,\n-  });\n-\n-  /* ---------------- INIT SPEECH RECOGNITION ---------------- */\n   useEffect(() => {\n     const SpeechRecognition =\n       (window as any).SpeechRecognition ||\n       (window as any).webkitSpeechRecognition;\n \n     if (!SpeechRecognition) {\n-      setError(\"Speech recognition is not supported in this browser.\");\n+      setError(\"Speech recognition not supported.\");\n       return;\n     }\n \n     const recognition = new SpeechRecognition();\n     recognition.lang = \"en-US\";\n     recognition.continuous = false;\n-    recognition.interimResults = false;\n \n     recognition.onresult = async (event: any) => {\n       const userText = event.results[0][0].transcript;\n+      setMessages((p) => [...p, { role: \"user\", text: userText }]);\n \n-      setMessages((prev) => [...prev, { role: \"user\", text: userText }]);\n-\n       try {\n-        const model = ai.getGenerativeModel({\n-          model: \"gemini-1.5-flash\",\n+        const res = await fetch(\"http://localhost:3001/api/chat\", {\n+          method: \"POST\",\n+          headers: { \"Content-Type\": \"application/json\" },\n+          body: JSON.stringify({ text: userText }),\n         });\n \n-        const result = await model.generateContent(userText);\n-        const reply = result.response.text();\n+        const data = await res.json();\n+        const reply = data.reply;\n \n-        setMessages((prev) => [\n-          ...prev,\n-          { role: \"model\", text: reply },\n-        ]);\n-\n+        setMessages((p) => [...p, { role: \"model\", text: reply }]);\n         speak(reply);\n-      } catch (err) {\n-        console.error(err);\n+      } catch {\n         setError(\"AI response failed.\");\n       }\n     };\n \n     recognition.onerror = () => {\n-      setError(\"Microphone permission denied or unavailable.\");\n+      setError(\"Mic permission issue.\");\n       setIsListening(false);\n     };\n \n-    recognition.onend = () => {\n-      setIsListening(false);\n-    };\n+    recognition.onend = () => setIsListening(false);\n \n     recognitionRef.current = recognition;\n   }, []);\n \n-  /* ---------------- TEXT TO SPEECH ---------------- */\n   const speak = (text: string) => {\n-    const utterance = new SpeechSynthesisUtterance(text);\n-    utterance.lang = \"en-US\";\n-    utterance.rate = 0.95;\n-    utterance.pitch = 1;\n-\n-    window.speechSynthesis.cancel();\n-    window.speechSynthesis.speak(utterance);\n+    const u = new SpeechSynthesisUtterance(text);\n+    u.lang = \"en-US\";\n+    speechSynthesis.cancel();\n+    speechSynthesis.speak(u);\n   };\n \n-  /* ---------------- MIC BUTTON ---------------- */\n   const handleConnect = () => {\n     setError(null);\n+    if (!recognitionRef.current) return;\n \n-    if (!API_KEY) {\n-      setError(\"API key is missing. Please check .env.local\");\n-      return;\n-    }\n-\n-    if (!recognitionRef.current) {\n-      setError(\"Speech recognition not available.\");\n-      return;\n-    }\n-\n     if (isListening) {\n       recognitionRef.current.stop();\n-      setIsListening(false);\n       return;\n     }\n \n-    try {\n-      recognitionRef.current.start();\n-      setIsListening(true);\n-    } catch {\n-      setError(\"Unable to access microphone.\");\n-    }\n+    recognitionRef.current.start();\n+    setIsListening(true);\n   };\n \n-  /* ---------------- UI ---------------- */\n   return (\n     <div className=\"flex flex-col h-screen bg-black text-white\">\n-      {/* Header */}\n-      <header className=\"p-6 flex justify-between items-center\">\n+      <header className=\"p-6 flex justify-between\">\n         <h1 className=\"text-2xl font-black\">\n           Hey <span className=\"text-rose-500 italic\">Buddy</span>\n         </h1>\n         <div className=\"flex gap-3\">\n@@ -137,19 +93,18 @@\n           </span>\n         </div>\n       </header>\n \n-      {/* Chat */}\n-      <main className=\"flex-1 overflow-y-auto px-6 space-y-6\">\n+      <main className=\"flex-1 px-6 space-y-4 overflow-y-auto\">\n         {messages.map((m, i) => (\n           <div\n             key={i}\n             className={`flex ${\n               m.role === \"user\" ? \"justify-end\" : \"justify-start\"\n             }`}\n           >\n             <div\n-              className={`max-w-[80%] p-5 rounded-3xl ${\n+              className={`p-4 rounded-3xl max-w-[75%] ${\n                 m.role === \"user\"\n                   ? \"bg-rose-600\"\n                   : \"bg-white/10\"\n               }`}\n@@ -159,28 +114,21 @@\n           </div>\n         ))}\n       </main>\n \n-      {/* Footer */}\n       <footer className=\"p-8 flex flex-col items-center\">\n         <button\n           onClick={handleConnect}\n-          className={`w-24 h-24 rounded-full flex items-center justify-center transition-all ${\n+          className={`w-24 h-24 rounded-full flex items-center justify-center ${\n             isListening ? \"bg-neutral-800\" : \"bg-rose-600\"\n           }`}\n         >\n           {isListening ? <MicOff size={40} /> : <Mic size={40} />}\n         </button>\n-\n-        <p className=\"mt-4 text-xs uppercase tracking-widest\">\n+        <p className=\"mt-4 text-xs uppercase\">\n           {isListening ? \"Listening...\" : \"Connect with Buddy\"}\n         </p>\n-\n-        {error && (\n-          <p className=\"text-red-400 text-xs mt-2 text-center\">\n-            {error}\n-          </p>\n-        )}\n+        {error && <p className=\"text-red-400 text-xs mt-2\">{error}</p>}\n       </footer>\n     </div>\n   );\n };\n"
                },
                {
                    "date": 1768931850829,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,134 +1,87 @@\n-import React, { useEffect, useRef, useState } from \"react\";\n-import { Mic, MicOff, Heart, Zap } from \"lucide-react\";\n+import React, { useState } from \"react\";\n \n-type ChatMessage = {\n+interface Message {\n   role: \"user\" | \"model\";\n   text: string;\n-};\n+}\n \n const App: React.FC = () => {\n-  const [isListening, setIsListening] = useState(false);\n+  const [messages, setMessages] = useState<Message[]>([]);\n+  const [input, setInput] = useState(\"\");\n+  const [loading, setLoading] = useState(false);\n   const [error, setError] = useState<string | null>(null);\n-  const [messages, setMessages] = useState<ChatMessage[]>([]);\n \n-  const recognitionRef = useRef<any>(null);\n+  const sendMessage = async () => {\n+    if (!input.trim()) return;\n \n-  useEffect(() => {\n-    const SpeechRecognition =\n-      (window as any).SpeechRecognition ||\n-      (window as any).webkitSpeechRecognition;\n+    setLoading(true);\n+    setError(null);\n \n-    if (!SpeechRecognition) {\n-      setError(\"Speech recognition not supported.\");\n-      return;\n-    }\n+    const userMsg: Message = { role: \"user\", text: input };\n+    setMessages(prev => [...prev, userMsg]);\n \n-    const recognition = new SpeechRecognition();\n-    recognition.lang = \"en-US\";\n-    recognition.continuous = false;\n+    try {\n+      const res = await fetch(\"http://localhost:3001/chat\", {\n+        method: \"POST\",\n+        headers: { \"Content-Type\": \"application/json\" },\n+        body: JSON.stringify({ text: input }),\n+      });\n \n-    recognition.onresult = async (event: any) => {\n-      const userText = event.results[0][0].transcript;\n-      setMessages((p) => [...p, { role: \"user\", text: userText }]);\n-\n-      try {\n-        const res = await fetch(\"http://localhost:3001/api/chat\", {\n-          method: \"POST\",\n-          headers: { \"Content-Type\": \"application/json\" },\n-          body: JSON.stringify({ text: userText }),\n-        });\n-\n-        const data = await res.json();\n-        const reply = data.reply;\n-\n-        setMessages((p) => [...p, { role: \"model\", text: reply }]);\n-        speak(reply);\n-      } catch {\n-        setError(\"AI response failed.\");\n+      if (!res.ok) {\n+        throw new Error(\"Backend error\");\n       }\n-    };\n \n-    recognition.onerror = () => {\n-      setError(\"Mic permission issue.\");\n-      setIsListening(false);\n-    };\n+      const data = await res.json();\n \n-    recognition.onend = () => setIsListening(false);\n-\n-    recognitionRef.current = recognition;\n-  }, []);\n-\n-  const speak = (text: string) => {\n-    const u = new SpeechSynthesisUtterance(text);\n-    u.lang = \"en-US\";\n-    speechSynthesis.cancel();\n-    speechSynthesis.speak(u);\n-  };\n-\n-  const handleConnect = () => {\n-    setError(null);\n-    if (!recognitionRef.current) return;\n-\n-    if (isListening) {\n-      recognitionRef.current.stop();\n-      return;\n+      setMessages(prev => [\n+        ...prev,\n+        { role: \"model\", text: data.reply },\n+      ]);\n+    } catch (err) {\n+      setError(\"AI response failed\");\n+    } finally {\n+      setLoading(false);\n+      setInput(\"\");\n     }\n-\n-    recognitionRef.current.start();\n-    setIsListening(true);\n   };\n \n   return (\n-    <div className=\"flex flex-col h-screen bg-black text-white\">\n-      <header className=\"p-6 flex justify-between\">\n-        <h1 className=\"text-2xl font-black\">\n-          Hey <span className=\"text-rose-500 italic\">Buddy</span>\n-        </h1>\n-        <div className=\"flex gap-3\">\n-          <span className=\"text-xs bg-rose-500/10 px-3 py-1 rounded-full\">\n-            <Zap className=\"inline w-3 h-3 mr-1\" /> Level 5%\n-          </span>\n-          <span className=\"text-xs bg-indigo-500/10 px-3 py-1 rounded-full\">\n-            <Heart className=\"inline w-3 h-3 mr-1\" /> Bond 10%\n-          </span>\n-        </div>\n+    <div className=\"min-h-screen bg-black text-white flex flex-col\">\n+      <header className=\"p-6 text-2xl font-bold\">\n+        Hey <span className=\"text-rose-500\">Buddy</span>\n       </header>\n \n-      <main className=\"flex-1 px-6 space-y-4 overflow-y-auto\">\n+      <main className=\"flex-1 p-6 space-y-4\">\n         {messages.map((m, i) => (\n           <div\n             key={i}\n-            className={`flex ${\n-              m.role === \"user\" ? \"justify-end\" : \"justify-start\"\n+            className={`max-w-xl p-4 rounded-2xl ${\n+              m.role === \"user\"\n+                ? \"bg-rose-600 ml-auto\"\n+                : \"bg-white/10\"\n             }`}\n           >\n-            <div\n-              className={`p-4 rounded-3xl max-w-[75%] ${\n-                m.role === \"user\"\n-                  ? \"bg-rose-600\"\n-                  : \"bg-white/10\"\n-              }`}\n-            >\n-              {m.text}\n-            </div>\n+            {m.text}\n           </div>\n         ))}\n+        {error && <p className=\"text-red-400\">{error}</p>}\n       </main>\n \n-      <footer className=\"p-8 flex flex-col items-center\">\n+      <footer className=\"p-6 flex gap-3\">\n+        <input\n+          className=\"flex-1 p-3 rounded-xl text-black\"\n+          value={input}\n+          onChange={e => setInput(e.target.value)}\n+          placeholder=\"Type something...\"\n+        />\n         <button\n-          onClick={handleConnect}\n-          className={`w-24 h-24 rounded-full flex items-center justify-center ${\n-            isListening ? \"bg-neutral-800\" : \"bg-rose-600\"\n-          }`}\n+          onClick={sendMessage}\n+          disabled={loading}\n+          className=\"px-6 py-3 bg-rose-600 rounded-xl\"\n         >\n-          {isListening ? <MicOff size={40} /> : <Mic size={40} />}\n+          {loading ? \"...\" : \"Send\"}\n         </button>\n-        <p className=\"mt-4 text-xs uppercase\">\n-          {isListening ? \"Listening...\" : \"Connect with Buddy\"}\n-        </p>\n-        {error && <p className=\"text-red-400 text-xs mt-2\">{error}</p>}\n       </footer>\n     </div>\n   );\n };\n"
                },
                {
                    "date": 1768932320592,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,89 +1,132 @@\n-import React, { useState } from \"react\";\n+import { useState, useRef } from \"react\";\n \n-interface Message {\n-  role: \"user\" | \"model\";\n-  text: string;\n-}\n+const BACKEND_URL = \"http://localhost:3001/chat\";\n \n-const App: React.FC = () => {\n-  const [messages, setMessages] = useState<Message[]>([]);\n-  const [input, setInput] = useState(\"\");\n-  const [loading, setLoading] = useState(false);\n-  const [error, setError] = useState<string | null>(null);\n+export default function App() {\n+  const [status, setStatus] = useState<\n+    \"idle\" | \"requesting\" | \"listening\" | \"thinking\" | \"error\"\n+  >(\"idle\");\n+  const [message, setMessage] = useState(\"\");\n+  const mediaRecorderRef = useRef<MediaRecorder | null>(null);\n+  const audioChunksRef = useRef<Blob[]>([]);\n \n-  const sendMessage = async () => {\n-    if (!input.trim()) return;\n+  const requestMicAndStart = async () => {\n+    try {\n+      setStatus(\"requesting\");\n \n-    setLoading(true);\n-    setError(null);\n+      const stream = await navigator.mediaDevices.getUserMedia({\n+        audio: true,\n+      });\n \n-    const userMsg: Message = { role: \"user\", text: input };\n-    setMessages(prev => [...prev, userMsg]);\n+      const mediaRecorder = new MediaRecorder(stream);\n+      mediaRecorderRef.current = mediaRecorder;\n+      audioChunksRef.current = [];\n \n+      mediaRecorder.ondataavailable = (e) => {\n+        if (e.data.size > 0) audioChunksRef.current.push(e.data);\n+      };\n+\n+      mediaRecorder.onstop = sendAudioToBackend;\n+\n+      mediaRecorder.start();\n+      setStatus(\"listening\");\n+\n+      // auto stop after 4 seconds\n+      setTimeout(() => {\n+        mediaRecorder.stop();\n+        stream.getTracks().forEach((t) => t.stop());\n+        setStatus(\"thinking\");\n+      }, 4000);\n+    } catch (err) {\n+      console.error(err);\n+      setStatus(\"error\");\n+      setMessage(\"Microphone permission denied.\");\n+    }\n+  };\n+\n+  const sendAudioToBackend = async () => {\n     try {\n-      const res = await fetch(\"http://localhost:3001/chat\", {\n+      const audioBlob = new Blob(audioChunksRef.current, {\n+        type: \"audio/webm\",\n+      });\n+\n+      const formData = new FormData();\n+      formData.append(\"audio\", audioBlob);\n+\n+      const res = await fetch(BACKEND_URL, {\n         method: \"POST\",\n-        headers: { \"Content-Type\": \"application/json\" },\n-        body: JSON.stringify({ text: input }),\n+        body: formData,\n       });\n \n-      if (!res.ok) {\n-        throw new Error(\"Backend error\");\n-      }\n+      if (!res.ok) throw new Error(\"Backend error\");\n \n       const data = await res.json();\n-\n-      setMessages(prev => [\n-        ...prev,\n-        { role: \"model\", text: data.reply },\n-      ]);\n+      setMessage(data.reply || \"No response\");\n+      setStatus(\"idle\");\n     } catch (err) {\n-      setError(\"AI response failed\");\n-    } finally {\n-      setLoading(false);\n-      setInput(\"\");\n+      console.error(err);\n+      setStatus(\"error\");\n+      setMessage(\"AI response failed.\");\n     }\n   };\n \n   return (\n-    <div className=\"min-h-screen bg-black text-white flex flex-col\">\n-      <header className=\"p-6 text-2xl font-bold\">\n-        Hey <span className=\"text-rose-500\">Buddy</span>\n-      </header>\n+    <div\n+      style={{\n+        background: \"#000\",\n+        color: \"#fff\",\n+        minHeight: \"100vh\",\n+        display: \"flex\",\n+        flexDirection: \"column\",\n+        alignItems: \"center\",\n+        justifyContent: \"center\",\n+        gap: 20,\n+        fontFamily: \"sans-serif\",\n+      }}\n+    >\n+      <h1>\n+        Hey <span style={{ color: \"#ff2d55\" }}>Buddy</span>\n+      </h1>\n \n-      <main className=\"flex-1 p-6 space-y-4\">\n-        {messages.map((m, i) => (\n-          <div\n-            key={i}\n-            className={`max-w-xl p-4 rounded-2xl ${\n-              m.role === \"user\"\n-                ? \"bg-rose-600 ml-auto\"\n-                : \"bg-white/10\"\n-            }`}\n-          >\n-            {m.text}\n-          </div>\n-        ))}\n-        {error && <p className=\"text-red-400\">{error}</p>}\n-      </main>\n+      <button\n+        onClick={requestMicAndStart}\n+        disabled={status === \"listening\" || status === \"thinking\"}\n+        style={{\n+          width: 120,\n+          height: 120,\n+          borderRadius: \"50%\",\n+          border: \"none\",\n+          background: \"#ff2d55\",\n+          color: \"#fff\",\n+          fontSize: 30,\n+          cursor: \"pointer\",\n+        }}\n+      >\n+        üé§\n+      </button>\n \n-      <footer className=\"p-6 flex gap-3\">\n-        <input\n-          className=\"flex-1 p-3 rounded-xl text-black\"\n-          value={input}\n-          onChange={e => setInput(e.target.value)}\n-          placeholder=\"Type something...\"\n-        />\n-        <button\n-          onClick={sendMessage}\n-          disabled={loading}\n-          className=\"px-6 py-3 bg-rose-600 rounded-xl\"\n+      {status === \"idle\" && <p>CONNECT WITH BUDDY</p>}\n+      {status === \"requesting\" && <p>Requesting microphone‚Ä¶</p>}\n+      {status === \"listening\" && <p>Listening‚Ä¶</p>}\n+      {status === \"thinking\" && <p>Thinking‚Ä¶</p>}\n+      {status === \"error\" && (\n+        <p style={{ color: \"red\" }}>{message}</p>\n+      )}\n+\n+      {message && status === \"idle\" && (\n+        <div\n+          style={{\n+            marginTop: 20,\n+            padding: 16,\n+            background: \"#111\",\n+            borderRadius: 8,\n+            maxWidth: 400,\n+            textAlign: \"center\",\n+          }}\n         >\n-          {loading ? \"...\" : \"Send\"}\n-        </button>\n-      </footer>\n+          ü§ñ {message}\n+        </div>\n+      )}\n     </div>\n   );\n-};\n-\n-export default App;\n+}\n"
                },
                {
                    "date": 1768933004000,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,132 +1,73 @@\n-import { useState, useRef } from \"react\";\n+import { useState } from \"react\";\n \n-const BACKEND_URL = \"http://localhost:3001/chat\";\n+const BACKEND_URL = \"http://localhost:3001\";\n \n export default function App() {\n-  const [status, setStatus] = useState<\n-    \"idle\" | \"requesting\" | \"listening\" | \"thinking\" | \"error\"\n-  >(\"idle\");\n-  const [message, setMessage] = useState(\"\");\n-  const mediaRecorderRef = useRef<MediaRecorder | null>(null);\n-  const audioChunksRef = useRef<Blob[]>([]);\n+  const [input, setInput] = useState(\"\");\n+  const [reply, setReply] = useState(\"\");\n+  const [error, setError] = useState(\"\");\n+  const [loading, setLoading] = useState(false);\n \n-  const requestMicAndStart = async () => {\n-    try {\n-      setStatus(\"requesting\");\n+  const sendMessage = async () => {\n+    setError(\"\");\n+    setReply(\"\");\n+    setLoading(true);\n \n-      const stream = await navigator.mediaDevices.getUserMedia({\n-        audio: true,\n-      });\n-\n-      const mediaRecorder = new MediaRecorder(stream);\n-      mediaRecorderRef.current = mediaRecorder;\n-      audioChunksRef.current = [];\n-\n-      mediaRecorder.ondataavailable = (e) => {\n-        if (e.data.size > 0) audioChunksRef.current.push(e.data);\n-      };\n-\n-      mediaRecorder.onstop = sendAudioToBackend;\n-\n-      mediaRecorder.start();\n-      setStatus(\"listening\");\n-\n-      // auto stop after 4 seconds\n-      setTimeout(() => {\n-        mediaRecorder.stop();\n-        stream.getTracks().forEach((t) => t.stop());\n-        setStatus(\"thinking\");\n-      }, 4000);\n-    } catch (err) {\n-      console.error(err);\n-      setStatus(\"error\");\n-      setMessage(\"Microphone permission denied.\");\n-    }\n-  };\n-\n-  const sendAudioToBackend = async () => {\n     try {\n-      const audioBlob = new Blob(audioChunksRef.current, {\n-        type: \"audio/webm\",\n+      const res = await fetch(`${BACKEND_URL}/chat`, {\n+        method: \"POST\",\n+        headers: {\n+          \"Content-Type\": \"application/json\",\n+        },\n+        body: JSON.stringify({\n+          text: input,\n+        }),\n       });\n \n-      const formData = new FormData();\n-      formData.append(\"audio\", audioBlob);\n+      const data = await res.json();\n \n-      const res = await fetch(BACKEND_URL, {\n-        method: \"POST\",\n-        body: formData,\n-      });\n+      if (!res.ok) {\n+        throw new Error(data.error || \"Request failed\");\n+      }\n \n-      if (!res.ok) throw new Error(\"Backend error\");\n-\n-      const data = await res.json();\n-      setMessage(data.reply || \"No response\");\n-      setStatus(\"idle\");\n-    } catch (err) {\n-      console.error(err);\n-      setStatus(\"error\");\n-      setMessage(\"AI response failed.\");\n+      setReply(data.reply);\n+    } catch (err: any) {\n+      setError(err.message || \"AI response failed\");\n+    } finally {\n+      setLoading(false);\n     }\n   };\n \n   return (\n-    <div\n-      style={{\n-        background: \"#000\",\n-        color: \"#fff\",\n-        minHeight: \"100vh\",\n-        display: \"flex\",\n-        flexDirection: \"column\",\n-        alignItems: \"center\",\n-        justifyContent: \"center\",\n-        gap: 20,\n-        fontFamily: \"sans-serif\",\n-      }}\n-    >\n-      <h1>\n-        Hey <span style={{ color: \"#ff2d55\" }}>Buddy</span>\n+    <div className=\"min-h-screen bg-black text-white flex flex-col items-center justify-center gap-6 p-6\">\n+      <h1 className=\"text-3xl font-bold\">\n+        Hey <span className=\"text-rose-500\">Buddy</span>\n       </h1>\n \n+      <textarea\n+        className=\"w-full max-w-md p-3 rounded bg-neutral-900 text-white outline-none\"\n+        rows={3}\n+        placeholder=\"Type something...\"\n+        value={input}\n+        onChange={(e) => setInput(e.target.value)}\n+      />\n+\n       <button\n-        onClick={requestMicAndStart}\n-        disabled={status === \"listening\" || status === \"thinking\"}\n-        style={{\n-          width: 120,\n-          height: 120,\n-          borderRadius: \"50%\",\n-          border: \"none\",\n-          background: \"#ff2d55\",\n-          color: \"#fff\",\n-          fontSize: 30,\n-          cursor: \"pointer\",\n-        }}\n+        onClick={sendMessage}\n+        disabled={loading || !input.trim()}\n+        className=\"px-6 py-3 rounded-full bg-rose-600 hover:bg-rose-700 disabled:opacity-50\"\n       >\n-        üé§\n+        {loading ? \"Thinking...\" : \"Send\"}\n       </button>\n \n-      {status === \"idle\" && <p>CONNECT WITH BUDDY</p>}\n-      {status === \"requesting\" && <p>Requesting microphone‚Ä¶</p>}\n-      {status === \"listening\" && <p>Listening‚Ä¶</p>}\n-      {status === \"thinking\" && <p>Thinking‚Ä¶</p>}\n-      {status === \"error\" && (\n-        <p style={{ color: \"red\" }}>{message}</p>\n+      {reply && (\n+        <div className=\"max-w-md bg-white/10 p-4 rounded-xl\">\n+          <strong>Buddy:</strong>\n+          <p className=\"mt-2\">{reply}</p>\n+        </div>\n       )}\n \n-      {message && status === \"idle\" && (\n-        <div\n-          style={{\n-            marginTop: 20,\n-            padding: 16,\n-            background: \"#111\",\n-            borderRadius: 8,\n-            maxWidth: 400,\n-            textAlign: \"center\",\n-          }}\n-        >\n-          ü§ñ {message}\n-        </div>\n-      )}\n+      {error && <p className=\"text-red-500\">{error}</p>}\n     </div>\n   );\n }\n"
                },
                {
                    "date": 1769097560138,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,73 +1,93 @@\n-import { useState } from \"react\";\n+import React, { useState } from \"react\";\n \n-const BACKEND_URL = \"http://localhost:3001\";\n+const API_BASE = \"http://localhost:3001\";\n \n-export default function App() {\n+const App: React.FC = () => {\n   const [input, setInput] = useState(\"\");\n-  const [reply, setReply] = useState(\"\");\n-  const [error, setError] = useState(\"\");\n+  const [messages, setMessages] = useState<\n+    { role: \"user\" | \"ai\"; text: string }[]\n+  >([]);\n   const [loading, setLoading] = useState(false);\n+  const [error, setError] = useState<string | null>(null);\n \n   const sendMessage = async () => {\n-    setError(\"\");\n-    setReply(\"\");\n+    if (!input.trim()) return;\n+\n+    setError(null);\n     setLoading(true);\n \n+    const userText = input;\n+    setInput(\"\");\n+\n+    setMessages((prev) => [...prev, { role: \"user\", text: userText }]);\n+\n     try {\n-      const res = await fetch(`${BACKEND_URL}/chat`, {\n+      const res = await fetch(`${API_BASE}/chat`, {\n         method: \"POST\",\n         headers: {\n           \"Content-Type\": \"application/json\",\n         },\n-        body: JSON.stringify({\n-          text: input,\n-        }),\n+        body: JSON.stringify({ text: userText }),\n       });\n \n-      const data = await res.json();\n-\n       if (!res.ok) {\n-        throw new Error(data.error || \"Request failed\");\n+        throw new Error(`Server error ${res.status}`);\n       }\n \n-      setReply(data.reply);\n-    } catch (err: any) {\n-      setError(err.message || \"AI response failed\");\n+      const data = await res.json();\n+\n+      setMessages((prev) => [\n+        ...prev,\n+        { role: \"ai\", text: data.reply },\n+      ]);\n+    } catch (err) {\n+      console.error(err);\n+      setError(\"AI response failed\");\n     } finally {\n       setLoading(false);\n     }\n   };\n \n   return (\n-    <div className=\"min-h-screen bg-black text-white flex flex-col items-center justify-center gap-6 p-6\">\n-      <h1 className=\"text-3xl font-bold\">\n+    <div className=\"min-h-screen bg-black text-white flex flex-col items-center justify-center px-4\">\n+      <h1 className=\"text-4xl font-bold mb-6\">\n         Hey <span className=\"text-rose-500\">Buddy</span>\n       </h1>\n \n+      <div className=\"w-full max-w-xl space-y-4 mb-4\">\n+        {messages.map((m, i) => (\n+          <div\n+            key={i}\n+            className={`p-4 rounded-xl ${\n+              m.role === \"user\"\n+                ? \"bg-rose-600 text-right\"\n+                : \"bg-white/10 text-left\"\n+            }`}\n+          >\n+            {m.text}\n+          </div>\n+        ))}\n+      </div>\n+\n       <textarea\n-        className=\"w-full max-w-md p-3 rounded bg-neutral-900 text-white outline-none\"\n-        rows={3}\n-        placeholder=\"Type something...\"\n         value={input}\n         onChange={(e) => setInput(e.target.value)}\n+        placeholder=\"Type something...\"\n+        className=\"w-full max-w-xl p-4 rounded-xl bg-neutral-900 outline-none resize-none\"\n+        rows={3}\n       />\n \n       <button\n         onClick={sendMessage}\n-        disabled={loading || !input.trim()}\n-        className=\"px-6 py-3 rounded-full bg-rose-600 hover:bg-rose-700 disabled:opacity-50\"\n+        disabled={loading}\n+        className=\"mt-4 px-8 py-3 rounded-full bg-rose-600 hover:bg-rose-700 disabled:opacity-50\"\n       >\n         {loading ? \"Thinking...\" : \"Send\"}\n       </button>\n \n-      {reply && (\n-        <div className=\"max-w-md bg-white/10 p-4 rounded-xl\">\n-          <strong>Buddy:</strong>\n-          <p className=\"mt-2\">{reply}</p>\n-        </div>\n-      )}\n-\n-      {error && <p className=\"text-red-500\">{error}</p>}\n+      {error && <p className=\"text-red-500 mt-4\">{error}</p>}\n     </div>\n   );\n-}\n+};\n+\n+export default App;\n"
                },
                {
                    "date": 1769098378209,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,46 +1,44 @@\n-import React, { useState } from \"react\";\n+import { useState } from \"react\";\n \n-const API_BASE = \"http://localhost:3001\";\n+const BACKEND_URL = \"http://localhost:3001\";\n \n-const App: React.FC = () => {\n+export default function App() {\n   const [input, setInput] = useState(\"\");\n-  const [messages, setMessages] = useState<\n-    { role: \"user\" | \"ai\"; text: string }[]\n-  >([]);\n+  const [messages, setMessages] = useState<string[]>([]);\n+  const [error, setError] = useState(\"\");\n   const [loading, setLoading] = useState(false);\n-  const [error, setError] = useState<string | null>(null);\n \n   const sendMessage = async () => {\n     if (!input.trim()) return;\n \n-    setError(null);\n+    setError(\"\");\n     setLoading(true);\n \n     const userText = input;\n     setInput(\"\");\n+    setMessages((prev) => [...prev, `You: ${userText}`]);\n \n-    setMessages((prev) => [...prev, { role: \"user\", text: userText }]);\n-\n     try {\n-      const res = await fetch(`${API_BASE}/chat`, {\n+      const res = await fetch(`${BACKEND_URL}/chat`, {\n         method: \"POST\",\n         headers: {\n           \"Content-Type\": \"application/json\",\n         },\n         body: JSON.stringify({ text: userText }),\n       });\n \n       if (!res.ok) {\n-        throw new Error(`Server error ${res.status}`);\n+        throw new Error(`Server error: ${res.status}`);\n       }\n \n       const data = await res.json();\n \n-      setMessages((prev) => [\n-        ...prev,\n-        { role: \"ai\", text: data.reply },\n-      ]);\n+      if (!data.reply) {\n+        throw new Error(\"No reply from AI\");\n+      }\n+\n+      setMessages((prev) => [...prev, `Buddy: ${data.reply}`]);\n     } catch (err) {\n       console.error(err);\n       setError(\"AI response failed\");\n     } finally {\n@@ -48,46 +46,77 @@\n     }\n   };\n \n   return (\n-    <div className=\"min-h-screen bg-black text-white flex flex-col items-center justify-center px-4\">\n-      <h1 className=\"text-4xl font-bold mb-6\">\n-        Hey <span className=\"text-rose-500\">Buddy</span>\n+    <div\n+      style={{\n+        minHeight: \"100vh\",\n+        background: \"#000\",\n+        color: \"#fff\",\n+        display: \"flex\",\n+        flexDirection: \"column\",\n+        alignItems: \"center\",\n+        justifyContent: \"center\",\n+        gap: \"20px\",\n+        padding: \"20px\",\n+      }}\n+    >\n+      <h1>\n+        Hey <span style={{ color: \"#ff0055\" }}>Buddy</span>\n       </h1>\n \n-      <div className=\"w-full max-w-xl space-y-4 mb-4\">\n-        {messages.map((m, i) => (\n+      <div style={{ width: \"100%\", maxWidth: \"600px\" }}>\n+        {messages.map((msg, i) => (\n           <div\n             key={i}\n-            className={`p-4 rounded-xl ${\n-              m.role === \"user\"\n-                ? \"bg-rose-600 text-right\"\n-                : \"bg-white/10 text-left\"\n-            }`}\n+            style={{\n+              background: msg.startsWith(\"You\")\n+                ? \"#ff0055\"\n+                : \"#222\",\n+              padding: \"14px\",\n+              borderRadius: \"14px\",\n+              marginBottom: \"10px\",\n+            }}\n           >\n-            {m.text}\n+            {msg}\n           </div>\n         ))}\n       </div>\n \n       <textarea\n         value={input}\n         onChange={(e) => setInput(e.target.value)}\n         placeholder=\"Type something...\"\n-        className=\"w-full max-w-xl p-4 rounded-xl bg-neutral-900 outline-none resize-none\"\n-        rows={3}\n+        style={{\n+          width: \"100%\",\n+          maxWidth: \"600px\",\n+          padding: \"14px\",\n+          borderRadius: \"14px\",\n+          background: \"#111\",\n+          color: \"#fff\",\n+          border: \"none\",\n+          resize: \"none\",\n+        }}\n       />\n \n       <button\n         onClick={sendMessage}\n         disabled={loading}\n-        className=\"mt-4 px-8 py-3 rounded-full bg-rose-600 hover:bg-rose-700 disabled:opacity-50\"\n+        style={{\n+          background: \"#ff0055\",\n+          color: \"#fff\",\n+          border: \"none\",\n+          padding: \"14px 30px\",\n+          borderRadius: \"30px\",\n+          cursor: \"pointer\",\n+          fontSize: \"16px\",\n+        }}\n       >\n         {loading ? \"Thinking...\" : \"Send\"}\n       </button>\n \n-      {error && <p className=\"text-red-500 mt-4\">{error}</p>}\n+      {error && (\n+        <p style={{ color: \"red\", marginTop: \"10px\" }}>{error}</p>\n+      )}\n     </div>\n   );\n-};\n-\n-export default App;\n+}\n"
                },
                {
                    "date": 1769098757311,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,8 +1,7 @@\n import { useState } from \"react\";\n \n-const BACKEND_URL = \"http://localhost:3001\";\n-\n+const BACKEND_URL = \"http://127.0.0.1:3001\";\n export default function App() {\n   const [input, setInput] = useState(\"\");\n   const [messages, setMessages] = useState<string[]>([]);\n   const [error, setError] = useState(\"\");\n"
                },
                {
                    "date": 1769101384978,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,6 +1,7 @@\n import { useState } from \"react\";\n \n+// App.tsx ‡¶è‡¶∞ ‡¶â‡¶™‡¶∞‡ßá:\n const BACKEND_URL = \"http://127.0.0.1:3001\";\n export default function App() {\n   const [input, setInput] = useState(\"\");\n   const [messages, setMessages] = useState<string[]>([]);\n"
                },
                {
                    "date": 1769103333295,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,122 +1,166 @@\n-import { useState } from \"react\";\n+import { useState, useEffect, useRef } from \"react\";\n \n-// App.tsx ‡¶è‡¶∞ ‡¶â‡¶™‡¶∞‡ßá:\n const BACKEND_URL = \"http://127.0.0.1:3001\";\n+\n+// ‡¶¨‡ßç‡¶∞‡¶æ‡¶â‡¶ú‡¶æ‡¶∞‡ßá‡¶∞ Speech Recognition ‡¶∏‡¶æ‡¶™‡ßã‡¶∞‡ßç‡¶ü ‡¶ö‡ßá‡¶ï\n+const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;\n+const recognition = SpeechRecognition ? new SpeechRecognition() : null;\n+\n+if (recognition) {\n+  recognition.continuous = false;\n+  recognition.lang = \"en-US\"; // ‡¶°‡¶ø‡¶´‡¶≤‡ßç‡¶ü ‡¶≠‡¶æ‡¶∑‡¶æ ‡¶á‡¶Ç‡¶≤‡¶ø‡¶∂, ‡¶§‡¶¨‡ßá ‡¶∏‡ßá ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶ì ‡¶¨‡ßÅ‡¶ù‡¶¨‡ßá\n+  recognition.interimResults = false;\n+}\n+\n export default function App() {\n   const [input, setInput] = useState(\"\");\n   const [messages, setMessages] = useState<string[]>([]);\n   const [error, setError] = useState(\"\");\n   const [loading, setLoading] = useState(false);\n+  const [isListening, setIsListening] = useState(false);\n+  const scrollRef = useRef<HTMLDivElement>(null);\n \n-  const sendMessage = async () => {\n-    if (!input.trim()) return;\n+  // ‡¶Ö‡¶ü‡ßã ‡¶∏‡ßç‡¶ï‡ßç‡¶∞‡¶≤ ‡¶®‡¶ø‡¶ö‡ßá‡¶∞ ‡¶¶‡¶ø‡¶ï‡ßá ‡¶∞‡¶æ‡¶ñ‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø\n+  useEffect(() => {\n+    scrollRef.current?.scrollIntoView({ behavior: \"smooth\" });\n+  }, [messages]);\n \n+  // AI-‡¶ï‡ßá ‡¶ï‡¶•‡¶æ ‡¶¨‡¶≤‡¶æ‡¶®‡ßã‡¶∞ ‡¶´‡¶æ‡¶Ç‡¶∂‡¶® (Text-to-Speech)\n+  const speak = (text: string) => {\n+    // ‡¶Ü‡¶ó‡ßá‡¶∞ ‡¶ï‡ßã‡¶®‡ßã ‡¶ï‡¶•‡¶æ ‡¶ö‡¶≤‡¶§‡ßá ‡¶•‡¶æ‡¶ï‡¶≤‡ßá ‡¶∏‡ßá‡¶ü‡¶æ ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡¶æ\n+    window.speechSynthesis.cancel();\n+\n+    const utterance = new SpeechSynthesisUtterance(text);\n+    \n+    // ‡¶≠‡¶Ø‡¶º‡ßá‡¶∏ ‡¶≤‡¶ø‡¶∏‡ßç‡¶ü ‡¶•‡ßá‡¶ï‡ßá ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßÄ ‡¶¨‡¶æ ‡¶á‡¶®‡ßç‡¶°‡¶ø‡¶Ø‡¶º‡¶æ‡¶® ‡¶´‡¶ø‡¶Æ‡ßá‡¶á‡¶≤ ‡¶≠‡¶Ø‡¶º‡ßá‡¶∏ ‡¶ñ‡ßã‡¶Å‡¶ú‡¶æ\n+    const voices = window.speechSynthesis.getVoices();\n+    const femaleVoice = voices.find(v => \n+      (v.name.includes(\"Google\") || v.name.includes(\"Female\") || v.name.includes(\"Bengali\") || v.name.includes(\"India\")) && \n+      (v.lang.startsWith(\"bn\") || v.lang.startsWith(\"en-IN\"))\n+    );\n+\n+    if (femaleVoice) {\n+      utterance.voice = femaleVoice;\n+    }\n+\n+    utterance.pitch = 1.2; // ‡¶è‡¶ï‡¶ü‡ßÅ ‡¶Æ‡¶ø‡¶∑‡ßç‡¶ü‡¶ø ‡¶ó‡¶≤‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶™‡¶ø‡¶ö ‡¶¨‡¶æ‡¶°‡¶º‡¶æ‡¶®‡ßã\n+    utterance.rate = 1.0;  // ‡¶ï‡¶•‡¶æ ‡¶¨‡¶≤‡¶æ‡¶∞ ‡¶ó‡¶§‡¶ø\n+    window.speechSynthesis.speak(utterance);\n+  };\n+\n+  // ‡¶ï‡¶•‡¶æ ‡¶∞‡ßá‡¶ï‡¶∞‡ßç‡¶° ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶´‡¶æ‡¶Ç‡¶∂‡¶® (Speech-to-Text)\n+  const startListening = () => {\n+    if (!recognition) {\n+      alert(\"Your browser does not support Speech Recognition.\");\n+      return;\n+    }\n+    setIsListening(true);\n+    recognition.start();\n+  };\n+\n+  if (recognition) {\n+    recognition.onresult = (event: any) => {\n+      const transcript = event.results[0][0].transcript;\n+      setInput(transcript);\n+      setIsListening(false);\n+      // ‡¶ï‡¶•‡¶æ ‡¶∂‡ßá‡¶∑ ‡¶π‡¶≤‡ßá ‡¶Ö‡¶ü‡ßã‡¶Æ‡ßá‡¶ü‡¶ø‡¶ï ‡¶Æ‡ßá‡¶∏‡ßá‡¶ú ‡¶™‡¶æ‡¶†‡¶ø‡ßü‡ßá ‡¶¶‡¶ø‡¶¨‡ßá\n+      handleSendMessage(transcript);\n+    };\n+\n+    recognition.onerror = () => {\n+      setIsListening(false);\n+      setError(\"Mic error. Please try again.\");\n+    };\n+  }\n+\n+  const handleSendMessage = async (textToSend: string) => {\n+    if (!textToSend.trim()) return;\n+\n     setError(\"\");\n     setLoading(true);\n+    setMessages((prev) => [...prev, `You: ${textToSend}`]);\n \n-    const userText = input;\n-    setInput(\"\");\n-    setMessages((prev) => [...prev, `You: ${userText}`]);\n-\n     try {\n       const res = await fetch(`${BACKEND_URL}/chat`, {\n         method: \"POST\",\n-        headers: {\n-          \"Content-Type\": \"application/json\",\n-        },\n-        body: JSON.stringify({ text: userText }),\n+        headers: { \"Content-Type\": \"application/json\" },\n+        body: JSON.stringify({ text: textToSend }),\n       });\n \n-      if (!res.ok) {\n-        throw new Error(`Server error: ${res.status}`);\n-      }\n-\n       const data = await res.json();\n-\n-      if (!data.reply) {\n-        throw new Error(\"No reply from AI\");\n+      if (data.reply) {\n+        setMessages((prev) => [...prev, `Buddy: ${data.reply}`]);\n+        // AI-‡¶è‡¶∞ ‡¶∞‡¶ø‡¶™‡ßç‡¶≤‡¶æ‡¶á ‡¶∂‡ßã‡¶®‡¶æ‡¶®‡ßã\n+        speak(data.reply);\n+      } else {\n+        throw new Error(\"No reply\");\n       }\n-\n-      setMessages((prev) => [...prev, `Buddy: ${data.reply}`]);\n     } catch (err) {\n-      console.error(err);\n-      setError(\"AI response failed\");\n+      setError(\"AI failed to respond.\");\n     } finally {\n       setLoading(false);\n+      setInput(\"\");\n     }\n   };\n \n   return (\n-    <div\n-      style={{\n-        minHeight: \"100vh\",\n-        background: \"#000\",\n-        color: \"#fff\",\n-        display: \"flex\",\n-        flexDirection: \"column\",\n-        alignItems: \"center\",\n-        justifyContent: \"center\",\n-        gap: \"20px\",\n-        padding: \"20px\",\n-      }}\n-    >\n-      <h1>\n-        Hey <span style={{ color: \"#ff0055\" }}>Buddy</span>\n+    <div style={{ minHeight: \"100vh\", background: \"#000\", color: \"#fff\", display: \"flex\", flexDirection: \"column\", alignItems: \"center\", padding: \"20px\", fontFamily: \"sans-serif\" }}>\n+      \n+      <h1 style={{ fontSize: \"2rem\", marginBottom: \"30px\" }}>\n+        Hey <span style={{ color: \"#ff0055\" }}>Buddy</span> üé§\n       </h1>\n \n-      <div style={{ width: \"100%\", maxWidth: \"600px\" }}>\n+      {/* ‡¶ö‡ßç‡¶Ø‡¶æ‡¶ü ‡¶è‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ */}\n+      <div style={{ width: \"100%\", maxWidth: \"600px\", flex: 1, overflowY: \"auto\", marginBottom: \"20px\", padding: \"10px\" }}>\n         {messages.map((msg, i) => (\n-          <div\n-            key={i}\n-            style={{\n-              background: msg.startsWith(\"You\")\n-                ? \"#ff0055\"\n-                : \"#222\",\n-              padding: \"14px\",\n-              borderRadius: \"14px\",\n-              marginBottom: \"10px\",\n-            }}\n-          >\n-            {msg}\n+          <div key={i} style={{\n+            background: msg.startsWith(\"You\") ? \"#ff0055\" : \"#222\",\n+            padding: \"15px\", borderRadius: \"15px\", marginBottom: \"15px\", alignSelf: msg.startsWith(\"You\") ? \"flex-end\" : \"flex-start\",\n+            lineHeight: \"1.5\", border: msg.includes(\"[Coach's Corner]\") ? \"1px solid #555\" : \"none\"\n+          }}>\n+            {msg.split(\"\\n\").map((line, index) => (\n+              <p key={index} style={{ margin: 0 }}>{line}</p>\n+            ))}\n           </div>\n         ))}\n+        <div ref={scrollRef} />\n       </div>\n \n-      <textarea\n-        value={input}\n-        onChange={(e) => setInput(e.target.value)}\n-        placeholder=\"Type something...\"\n-        style={{\n-          width: \"100%\",\n-          maxWidth: \"600px\",\n-          padding: \"14px\",\n-          borderRadius: \"14px\",\n-          background: \"#111\",\n-          color: \"#fff\",\n-          border: \"none\",\n-          resize: \"none\",\n-        }}\n-      />\n+      {/* ‡¶á‡¶®‡¶™‡ßÅ‡¶ü ‡¶è‡¶¨‡¶Ç ‡¶ï‡¶®‡ßç‡¶ü‡ßç‡¶∞‡ßã‡¶≤‡¶∏ */}\n+      <div style={{ width: \"100%\", maxWidth: \"600px\", display: \"flex\", flexDirection: \"column\", gap: \"15px\", position: \"sticky\", bottom: \"20px\", background: \"#000\", padding: \"10px\" }}>\n+        \n+        {error && <p style={{ color: \"#ff4444\", textAlign: \"center\" }}>{error}</p>}\n \n-      <button\n-        onClick={sendMessage}\n-        disabled={loading}\n-        style={{\n-          background: \"#ff0055\",\n-          color: \"#fff\",\n-          border: \"none\",\n-          padding: \"14px 30px\",\n-          borderRadius: \"30px\",\n-          cursor: \"pointer\",\n-          fontSize: \"16px\",\n-        }}\n-      >\n-        {loading ? \"Thinking...\" : \"Send\"}\n-      </button>\n+        <div style={{ display: \"flex\", gap: \"10px\" }}>\n+          <textarea\n+            value={input}\n+            onChange={(e) => setInput(e.target.value)}\n+            placeholder=\"Type or use the mic...\"\n+            style={{ flex: 1, padding: \"15px\", borderRadius: \"15px\", background: \"#111\", color: \"#fff\", border: \"1px solid #333\", resize: \"none\", fontSize: \"16px\" }}\n+          />\n+          \n+          <button\n+            onClick={startListening}\n+            style={{\n+              background: isListening ? \"#fff\" : \"#ff0055\",\n+              color: isListening ? \"#000\" : \"#fff\",\n+              border: \"none\", width: \"60px\", height: \"60px\", borderRadius: \"50%\", cursor: \"pointer\", fontSize: \"24px\", transition: \"0.3s\"\n+            }}\n+          >\n+            {isListening ? \"üõë\" : \"üé§\"}\n+          </button>\n+        </div>\n \n-      {error && (\n-        <p style={{ color: \"red\", marginTop: \"10px\" }}>{error}</p>\n-      )}\n+        <button\n+          onClick={() => handleSendMessage(input)}\n+          disabled={loading}\n+          style={{\n+            background: \"#fff\", color: \"#000\", border: \"none\", padding: \"15px\", borderRadius: \"30px\", cursor: \"pointer\", fontWeight: \"bold\", fontSize: \"16px\"\n+          }}\n+        >\n+          {loading ? \"Thinking...\" : \"Send Message\"}\n+        </button>\n+      </div>\n     </div>\n   );\n-}\n+}\n\\ No newline at end of file\n"
                },
                {
                    "date": 1769103819685,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,166 +1,110 @@\n import { useState, useEffect, useRef } from \"react\";\n \n const BACKEND_URL = \"http://127.0.0.1:3001\";\n \n-// ‡¶¨‡ßç‡¶∞‡¶æ‡¶â‡¶ú‡¶æ‡¶∞‡ßá‡¶∞ Speech Recognition ‡¶∏‡¶æ‡¶™‡ßã‡¶∞‡ßç‡¶ü ‡¶ö‡ßá‡¶ï\n+// Speech Recognition Setup\n const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;\n const recognition = SpeechRecognition ? new SpeechRecognition() : null;\n \n-if (recognition) {\n-  recognition.continuous = false;\n-  recognition.lang = \"en-US\"; // ‡¶°‡¶ø‡¶´‡¶≤‡ßç‡¶ü ‡¶≠‡¶æ‡¶∑‡¶æ ‡¶á‡¶Ç‡¶≤‡¶ø‡¶∂, ‡¶§‡¶¨‡ßá ‡¶∏‡ßá ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶ì ‡¶¨‡ßÅ‡¶ù‡¶¨‡ßá\n-  recognition.interimResults = false;\n-}\n-\n export default function App() {\n   const [input, setInput] = useState(\"\");\n-  const [messages, setMessages] = useState<string[]>([]);\n-  const [error, setError] = useState(\"\");\n+  const [messages, setMessages] = useState<{sender: string, text: string}[]>([]);\n   const [loading, setLoading] = useState(false);\n   const [isListening, setIsListening] = useState(false);\n   const scrollRef = useRef<HTMLDivElement>(null);\n \n-  // ‡¶Ö‡¶ü‡ßã ‡¶∏‡ßç‡¶ï‡ßç‡¶∞‡¶≤ ‡¶®‡¶ø‡¶ö‡ßá‡¶∞ ‡¶¶‡¶ø‡¶ï‡ßá ‡¶∞‡¶æ‡¶ñ‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø\n   useEffect(() => {\n     scrollRef.current?.scrollIntoView({ behavior: \"smooth\" });\n   }, [messages]);\n \n-  // AI-‡¶ï‡ßá ‡¶ï‡¶•‡¶æ ‡¶¨‡¶≤‡¶æ‡¶®‡ßã‡¶∞ ‡¶´‡¶æ‡¶Ç‡¶∂‡¶® (Text-to-Speech)\n-  const speak = (text: string) => {\n-    // ‡¶Ü‡¶ó‡ßá‡¶∞ ‡¶ï‡ßã‡¶®‡ßã ‡¶ï‡¶•‡¶æ ‡¶ö‡¶≤‡¶§‡ßá ‡¶•‡¶æ‡¶ï‡¶≤‡ßá ‡¶∏‡ßá‡¶ü‡¶æ ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡¶æ\n-    window.speechSynthesis.cancel();\n-\n-    const utterance = new SpeechSynthesisUtterance(text);\n-    \n-    // ‡¶≠‡¶Ø‡¶º‡ßá‡¶∏ ‡¶≤‡¶ø‡¶∏‡ßç‡¶ü ‡¶•‡ßá‡¶ï‡ßá ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßÄ ‡¶¨‡¶æ ‡¶á‡¶®‡ßç‡¶°‡¶ø‡¶Ø‡¶º‡¶æ‡¶® ‡¶´‡¶ø‡¶Æ‡ßá‡¶á‡¶≤ ‡¶≠‡¶Ø‡¶º‡ßá‡¶∏ ‡¶ñ‡ßã‡¶Å‡¶ú‡¶æ\n-    const voices = window.speechSynthesis.getVoices();\n-    const femaleVoice = voices.find(v => \n-      (v.name.includes(\"Google\") || v.name.includes(\"Female\") || v.name.includes(\"Bengali\") || v.name.includes(\"India\")) && \n-      (v.lang.startsWith(\"bn\") || v.lang.startsWith(\"en-IN\"))\n-    );\n-\n-    if (femaleVoice) {\n-      utterance.voice = femaleVoice;\n+  // --- Natural Voice Output (ElevenLabs) ---\n+  const playNaturalVoice = async (text: string) => {\n+    try {\n+      const res = await fetch(`${BACKEND_URL}/tts`, {\n+        method: \"POST\",\n+        headers: { \"Content-Type\": \"application/json\" },\n+        body: JSON.stringify({ text }),\n+      });\n+      if (!res.ok) throw new Error(\"TTS failed\");\n+      \n+      const audioBlob = await res.blob();\n+      const audioUrl = URL.createObjectURL(audioBlob);\n+      const audio = new Audio(audioUrl);\n+      audio.play();\n+    } catch (err) {\n+      console.error(\"TTS Error:\", err);\n     }\n-\n-    utterance.pitch = 1.2; // ‡¶è‡¶ï‡¶ü‡ßÅ ‡¶Æ‡¶ø‡¶∑‡ßç‡¶ü‡¶ø ‡¶ó‡¶≤‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶™‡¶ø‡¶ö ‡¶¨‡¶æ‡¶°‡¶º‡¶æ‡¶®‡ßã\n-    utterance.rate = 1.0;  // ‡¶ï‡¶•‡¶æ ‡¶¨‡¶≤‡¶æ‡¶∞ ‡¶ó‡¶§‡¶ø\n-    window.speechSynthesis.speak(utterance);\n   };\n \n-  // ‡¶ï‡¶•‡¶æ ‡¶∞‡ßá‡¶ï‡¶∞‡ßç‡¶° ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶´‡¶æ‡¶Ç‡¶∂‡¶® (Speech-to-Text)\n+  // --- Speech to Text Logic ---\n   const startListening = () => {\n-    if (!recognition) {\n-      alert(\"Your browser does not support Speech Recognition.\");\n-      return;\n-    }\n+    if (!recognition) return alert(\"Browser not supported\");\n     setIsListening(true);\n     recognition.start();\n-  };\n-\n-  if (recognition) {\n+    \n     recognition.onresult = (event: any) => {\n       const transcript = event.results[0][0].transcript;\n-      setInput(transcript);\n       setIsListening(false);\n-      // ‡¶ï‡¶•‡¶æ ‡¶∂‡ßá‡¶∑ ‡¶π‡¶≤‡ßá ‡¶Ö‡¶ü‡ßã‡¶Æ‡ßá‡¶ü‡¶ø‡¶ï ‡¶Æ‡ßá‡¶∏‡ßá‡¶ú ‡¶™‡¶æ‡¶†‡¶ø‡ßü‡ßá ‡¶¶‡¶ø‡¶¨‡ßá\n       handleSendMessage(transcript);\n     };\n+    recognition.onerror = () => setIsListening(false);\n+  };\n \n-    recognition.onerror = () => {\n-      setIsListening(false);\n-      setError(\"Mic error. Please try again.\");\n-    };\n-  }\n-\n   const handleSendMessage = async (textToSend: string) => {\n     if (!textToSend.trim()) return;\n-\n-    setError(\"\");\n     setLoading(true);\n-    setMessages((prev) => [...prev, `You: ${textToSend}`]);\n+    setMessages(prev => [...prev, { sender: \"You\", text: textToSend }]);\n \n     try {\n       const res = await fetch(`${BACKEND_URL}/chat`, {\n         method: \"POST\",\n         headers: { \"Content-Type\": \"application/json\" },\n         body: JSON.stringify({ text: textToSend }),\n       });\n-\n       const data = await res.json();\n+      \n       if (data.reply) {\n-        setMessages((prev) => [...prev, `Buddy: ${data.reply}`]);\n-        // AI-‡¶è‡¶∞ ‡¶∞‡¶ø‡¶™‡ßç‡¶≤‡¶æ‡¶á ‡¶∂‡ßã‡¶®‡¶æ‡¶®‡ßã\n-        speak(data.reply);\n-      } else {\n-        throw new Error(\"No reply\");\n+        setMessages(prev => [...prev, { sender: \"Buddy\", text: data.reply }]);\n+        playNaturalVoice(data.reply); // ElevenLabs ‡¶≠‡ßü‡ßá‡¶∏ ‡¶™‡ßç‡¶≤‡ßá ‡¶π‡¶¨‡ßá\n       }\n     } catch (err) {\n-      setError(\"AI failed to respond.\");\n+      console.error(\"Chat Error:\", err);\n     } finally {\n       setLoading(false);\n       setInput(\"\");\n     }\n   };\n \n   return (\n-    <div style={{ minHeight: \"100vh\", background: \"#000\", color: \"#fff\", display: \"flex\", flexDirection: \"column\", alignItems: \"center\", padding: \"20px\", fontFamily: \"sans-serif\" }}>\n-      \n-      <h1 style={{ fontSize: \"2rem\", marginBottom: \"30px\" }}>\n-        Hey <span style={{ color: \"#ff0055\" }}>Buddy</span> üé§\n-      </h1>\n+    <div style={{ minHeight: \"100vh\", background: \"#000\", color: \"#fff\", display: \"flex\", flexDirection: \"column\", alignItems: \"center\", padding: \"20px\" }}>\n+      <h1 style={{ color: \"#ff0055\" }}>Hey Buddy üé§</h1>\n \n-      {/* ‡¶ö‡ßç‡¶Ø‡¶æ‡¶ü ‡¶è‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ */}\n-      <div style={{ width: \"100%\", maxWidth: \"600px\", flex: 1, overflowY: \"auto\", marginBottom: \"20px\", padding: \"10px\" }}>\n+      <div style={{ width: \"100%\", maxWidth: \"600px\", flex: 1, overflowY: \"auto\", padding: \"10px\" }}>\n         {messages.map((msg, i) => (\n           <div key={i} style={{\n-            background: msg.startsWith(\"You\") ? \"#ff0055\" : \"#222\",\n-            padding: \"15px\", borderRadius: \"15px\", marginBottom: \"15px\", alignSelf: msg.startsWith(\"You\") ? \"flex-end\" : \"flex-start\",\n-            lineHeight: \"1.5\", border: msg.includes(\"[Coach's Corner]\") ? \"1px solid #555\" : \"none\"\n+            background: msg.sender === \"You\" ? \"#ff0055\" : \"#222\",\n+            padding: \"15px\", borderRadius: \"15px\", marginBottom: \"15px\", whiteSpace: \"pre-wrap\"\n           }}>\n-            {msg.split(\"\\n\").map((line, index) => (\n-              <p key={index} style={{ margin: 0 }}>{line}</p>\n-            ))}\n+            <strong>{msg.sender}:</strong> {msg.text}\n           </div>\n         ))}\n         <div ref={scrollRef} />\n       </div>\n \n-      {/* ‡¶á‡¶®‡¶™‡ßÅ‡¶ü ‡¶è‡¶¨‡¶Ç ‡¶ï‡¶®‡ßç‡¶ü‡ßç‡¶∞‡ßã‡¶≤‡¶∏ */}\n-      <div style={{ width: \"100%\", maxWidth: \"600px\", display: \"flex\", flexDirection: \"column\", gap: \"15px\", position: \"sticky\", bottom: \"20px\", background: \"#000\", padding: \"10px\" }}>\n-        \n-        {error && <p style={{ color: \"#ff4444\", textAlign: \"center\" }}>{error}</p>}\n-\n-        <div style={{ display: \"flex\", gap: \"10px\" }}>\n-          <textarea\n-            value={input}\n-            onChange={(e) => setInput(e.target.value)}\n-            placeholder=\"Type or use the mic...\"\n-            style={{ flex: 1, padding: \"15px\", borderRadius: \"15px\", background: \"#111\", color: \"#fff\", border: \"1px solid #333\", resize: \"none\", fontSize: \"16px\" }}\n-          />\n-          \n-          <button\n-            onClick={startListening}\n-            style={{\n-              background: isListening ? \"#fff\" : \"#ff0055\",\n-              color: isListening ? \"#000\" : \"#fff\",\n-              border: \"none\", width: \"60px\", height: \"60px\", borderRadius: \"50%\", cursor: \"pointer\", fontSize: \"24px\", transition: \"0.3s\"\n-            }}\n-          >\n-            {isListening ? \"üõë\" : \"üé§\"}\n-          </button>\n-        </div>\n-\n-        <button\n-          onClick={() => handleSendMessage(input)}\n-          disabled={loading}\n-          style={{\n-            background: \"#fff\", color: \"#000\", border: \"none\", padding: \"15px\", borderRadius: \"30px\", cursor: \"pointer\", fontWeight: \"bold\", fontSize: \"16px\"\n-          }}\n-        >\n-          {loading ? \"Thinking...\" : \"Send Message\"}\n+      <div style={{ width: \"100%\", maxWidth: \"600px\", display: \"flex\", gap: \"10px\", padding: \"20px\", background: \"#000\", position: \"sticky\", bottom: 0 }}>\n+        <textarea \n+          value={input} \n+          onChange={(e) => setInput(e.target.value)}\n+          placeholder=\"Type something...\"\n+          style={{ flex: 1, padding: \"12px\", borderRadius: \"10px\", background: \"#111\", color: \"#fff\", border: \"1px solid #333\" }}\n+        />\n+        <button onClick={startListening} style={{ width: \"50px\", height: \"50px\", borderRadius: \"50%\", border: \"none\", background: isListening ? \"#fff\" : \"#ff0055\", cursor: \"pointer\", fontSize: \"20px\" }}>\n+          {isListening ? \"üõë\" : \"üé§\"}\n         </button>\n+        <button onClick={() => handleSendMessage(input)} disabled={loading} style={{ padding: \"0 20px\", borderRadius: \"10px\", border: \"none\", background: \"#fff\", fontWeight: \"bold\", cursor: \"pointer\" }}>\n+          {loading ? \"...\" : \"Send\"}\n+        </button>\n       </div>\n     </div>\n   );\n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1769104598276,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,110 +1,198 @@\n-import { useState, useEffect, useRef } from \"react\";\n+import React, { useState, useEffect, useRef, useMemo } from 'react';\n+import { GoogleGenAI, Modality, LiveServerMessage } from '@google/genai';\n+import { SYSTEM_INSTRUCTION, SAFETY_SETTINGS } from './constants';\n+import { decode, decodeAudioData, createPcmBlob } from './services/audioUtils';\n+import { Mic, MicOff, Heart, Zap, User, Sparkles, MessageSquare } from 'lucide-react';\n \n-const BACKEND_URL = \"http://127.0.0.1:3001\";\n+const App: React.FC = () => {\n+  const [isConnected, setIsConnected] = useState(false);\n+  const [messages, setMessages] = useState<{role: string, text: string}[]>([]);\n+  const [currentOutput, setCurrentOutput] = useState('');\n+  const [isProcessing, setIsProcessing] = useState(false);\n+  \n+  // Stats\n+  const [bondScore, setBondScore] = useState(10);\n+  const [proLevel, setProLevel] = useState(5);\n+  const [currentMood, setCurrentMood] = useState('HAPPY');\n \n-// Speech Recognition Setup\n-const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;\n-const recognition = SpeechRecognition ? new SpeechRecognition() : null;\n+  // Audio Refs\n+  const inputAudioContextRef = useRef<AudioContext | null>(null);\n+  const outputAudioContextRef = useRef<AudioContext | null>(null);\n+  const nextStartTimeRef = useRef(0);\n+  const sourcesRef = useRef<Set<AudioBufferSourceNode>>(new Set());\n+  const activeSessionRef = useRef<any>(null);\n+  const streamRef = useRef<MediaStream | null>(null);\n+  const transcriptEndRef = useRef<HTMLDivElement>(null);\n \n-export default function App() {\n-  const [input, setInput] = useState(\"\");\n-  const [messages, setMessages] = useState<{sender: string, text: string}[]>([]);\n-  const [loading, setLoading] = useState(false);\n-  const [isListening, setIsListening] = useState(false);\n-  const scrollRef = useRef<HTMLDivElement>(null);\n+  // Metadata Parsing (Stats update)\n+  const parseMetadata = (text: string) => {\n+    const moodMatch = text.match(/MOOD:\\s*(\\w+)/);\n+    const bondMatch = text.match(/BOND_SCORE:\\s*(\\d+)/);\n+    const levelMatch = text.match(/PRO_LEVEL:\\s*(\\d+)/);\n \n-  useEffect(() => {\n-    scrollRef.current?.scrollIntoView({ behavior: \"smooth\" });\n-  }, [messages]);\n+    if (moodMatch) setCurrentMood(moodMatch[1].toUpperCase());\n+    if (bondMatch) setBondScore(parseInt(bondMatch[1]));\n+    if (levelMatch) setProLevel(parseInt(levelMatch[1]));\n \n-  // --- Natural Voice Output (ElevenLabs) ---\n-  const playNaturalVoice = async (text: string) => {\n+    return text.replace(/\\[METADATA\\][\\s\\S]*?\\[\\/METADATA\\]/, '').trim();\n+  };\n+\n+  const handleConnect = async () => {\n+    if (isConnected) { cleanup(); return; }\n     try {\n-      const res = await fetch(`${BACKEND_URL}/tts`, {\n-        method: \"POST\",\n-        headers: { \"Content-Type\": \"application/json\" },\n-        body: JSON.stringify({ text }),\n+      setIsProcessing(true);\n+      // @ts-ignore\n+      const apiKey = import.meta.env.VITE_GEMINI_API_KEY || process.env.GEMINI_API_KEY;\n+      const ai = new GoogleGenAI({ apiKey });\n+\n+      inputAudioContextRef.current = new AudioContext({ sampleRate: 16000 });\n+      outputAudioContextRef.current = new AudioContext({ sampleRate: 24000 });\n+      \n+      streamRef.current = await navigator.mediaDevices.getUserMedia({ \n+        audio: { channelCount: 1, sampleRate: 16000, echoCancellation: true } \n       });\n-      if (!res.ok) throw new Error(\"TTS failed\");\n-      \n-      const audioBlob = await res.blob();\n-      const audioUrl = URL.createObjectURL(audioBlob);\n-      const audio = new Audio(audioUrl);\n-      audio.play();\n-    } catch (err) {\n-      console.error(\"TTS Error:\", err);\n-    }\n-  };\n \n-  // --- Speech to Text Logic ---\n-  const startListening = () => {\n-    if (!recognition) return alert(\"Browser not supported\");\n-    setIsListening(true);\n-    recognition.start();\n-    \n-    recognition.onresult = (event: any) => {\n-      const transcript = event.results[0][0].transcript;\n-      setIsListening(false);\n-      handleSendMessage(transcript);\n-    };\n-    recognition.onerror = () => setIsListening(false);\n-  };\n+      const session = await ai.live.connect({\n+        model: 'gemini-2.0-flash-exp', // ‡¶è‡¶Ü‡¶á ‡¶∏‡ßç‡¶ü‡ßÅ‡¶°‡¶ø‡¶ì‡¶∞ ‡¶∏‡ßá‡¶á ‡¶∞‡¶ø‡ßü‡ßá‡¶≤‡¶ø‡¶∏‡ßç‡¶ü‡¶ø‡¶ï ‡¶Æ‡¶°‡ßá‡¶≤\n+        callbacks: {\n+          onopen: () => {\n+            setIsConnected(true); setIsProcessing(false);\n+            if (inputAudioContextRef.current && streamRef.current) {\n+              const source = inputAudioContextRef.current.createMediaStreamSource(streamRef.current);\n+              const processor = inputAudioContextRef.current.createScriptProcessor(2048, 1, 1);\n+              processor.onaudioprocess = (e) => {\n+                if (activeSessionRef.current) {\n+                  activeSessionRef.current.sendRealtimeInput({ media: createPcmBlob(e.inputBuffer.getChannelData(0)) });\n+                }\n+              };\n\\ No newline at end of file\n+              source.connect(processor);\n+              processor.connect(inputAudioContextRef.current.destination);\n+            }\n+          },\n+          onmessage: async (m: LiveServerMessage) => {\n+            if (m.serverContent?.outputTranscription) setCurrentOutput(p => p + m.serverContent!.outputTranscription!.text);\n+            \n+            const base64 = m.serverContent?.modelTurn?.parts[0]?.inlineData?.data;\n+            if (base64 && outputAudioContextRef.current) {\n+              const ctx = outputAudioContextRef.current;\n+              nextStartTimeRef.current = Math.max(nextStartTimeRef.current, ctx.currentTime);\n+              const buf = await decodeAudioData(decode(base64), ctx, 24000, 1);\n+              const src = ctx.createBufferSource();\n+              src.buffer = buf; src.connect(ctx.destination);\n+              src.start(nextStartTimeRef.current);\n+              nextStartTimeRef.current += buf.duration;\n+              sourcesRef.current.add(src);\n+            }\n \n-  const handleSendMessage = async (textToSend: string) => {\n-    if (!textToSend.trim()) return;\n-    setLoading(true);\n-    setMessages(prev => [...prev, { sender: \"You\", text: textToSend }]);\n+            if (m.serverContent?.interrupted) { \n+              sourcesRef.current.forEach(s => s.stop()); \n+              sourcesRef.current.clear(); \n+              nextStartTimeRef.current = 0; \n+            }\n \n-    try {\n-      const res = await fetch(`${BACKEND_URL}/chat`, {\n-        method: \"POST\",\n-        headers: { \"Content-Type\": \"application/json\" },\n-        body: JSON.stringify({ text: textToSend }),\n+            if (m.serverContent?.turnComplete) {\n+              setMessages(prev => [...prev, { role: 'model', text: parseMetadata(currentOutput) }]);\n+              setCurrentOutput('');\n+            }\n+          }\n+        },\n+        config: {\n+          responseModalities: [Modality.AUDIO],\n+          speechConfig: { voiceConfig: { prebuiltVoiceConfig: { voiceName: 'Aoide' } } }, // Aoide ‡¶π‡¶≤‡ßã ‡¶∏‡ßá‡¶á ‡¶Æ‡¶ø‡¶∑‡ßç‡¶ü‡¶ø ‡¶Æ‡ßá‡ßü‡ßá‡¶≤‡¶ø ‡¶ï‡¶®‡ßç‡¶†\n+          systemInstruction: SYSTEM_INSTRUCTION,\n+          safetySettings: SAFETY_SETTINGS as any,\n+          outputAudioTranscription: {},\n+        },\n       });\n-      const data = await res.json();\n-      \n-      if (data.reply) {\n-        setMessages(prev => [...prev, { sender: \"Buddy\", text: data.reply }]);\n-        playNaturalVoice(data.reply); // ElevenLabs ‡¶≠‡ßü‡ßá‡¶∏ ‡¶™‡ßç‡¶≤‡ßá ‡¶π‡¶¨‡ßá\n-      }\n-    } catch (err) {\n-      console.error(\"Chat Error:\", err);\n-    } finally {\n-      setLoading(false);\n-      setInput(\"\");\n-    }\n+      activeSessionRef.current = session;\n+    } catch (err) { console.error(err); setIsProcessing(false); }\n   };\n \n+  const cleanup = () => {\n+    setIsConnected(false);\n+    streamRef.current?.getTracks().forEach(t => t.stop());\n+    inputAudioContextRef.current?.close();\n+    outputAudioContextRef.current?.close();\n+    activeSessionRef.current?.close();\n+    sourcesRef.current.forEach(s => s.stop());\n+  };\n+\n+  useEffect(() => transcriptEndRef.current?.scrollIntoView({ behavior: 'smooth' }), [messages, currentOutput]);\n+\n   return (\n-    <div style={{ minHeight: \"100vh\", background: \"#000\", color: \"#fff\", display: \"flex\", flexDirection: \"column\", alignItems: \"center\", padding: \"20px\" }}>\n-      <h1 style={{ color: \"#ff0055\" }}>Hey Buddy üé§</h1>\n+    <div className=\"min-h-screen bg-[#050505] text-white flex flex-col font-sans\">\n+      {/* Header */}\n+      <header className=\"p-6 border-b border-white/5 flex justify-between items-center bg-black/50 backdrop-blur-xl sticky top-0 z-50\">\n+        <div className=\"flex flex-col\">\n+          <h1 className=\"text-2xl font-black tracking-tighter italic\">Hey <span className=\"text-rose-500\">Buddy</span></h1>\n+          <div className=\"flex gap-3 mt-1\">\n+            <span className=\"text-[10px] font-bold text-rose-400 uppercase tracking-widest flex items-center gap-1\">\n+              <Heart className=\"w-3 h-3 fill-current\" /> Bond: {bondScore}%\n+            </span>\n+            <span className=\"text-[10px] font-bold text-emerald-400 uppercase tracking-widest flex items-center gap-1\">\n+              <Zap className=\"w-3 h-3 fill-current\" /> Mastery: {proLevel}%\n+            </span>\n+          </div>\n+        </div>\n+        <div className={`px-4 py-1.5 rounded-full text-[10px] font-black uppercase tracking-[0.2em] ${isConnected ? 'bg-rose-500/20 text-rose-500 animate-pulse' : 'bg-white/5 text-neutral-500'}`}>\n+          {isConnected ? '‚Ä¢ Live Session' : 'Offline'}\n+        </div>\n+      </header>\n \n-      <div style={{ width: \"100%\", maxWidth: \"600px\", flex: 1, overflowY: \"auto\", padding: \"10px\" }}>\n+      {/* Main Chat Area */}\n+      <main className=\"flex-1 overflow-y-auto p-6 md:px-12 space-y-10 max-w-4xl mx-auto w-full\">\n+        {messages.length === 0 && !currentOutput && (\n+          <div className=\"h-full flex flex-col items-center justify-center opacity-20 text-center py-20\">\n+            <Heart className=\"w-20 h-20 mb-4 animate-pulse\" />\n+            <p className=\"text-xl font-medium italic\">\"I'm here... just waiting for you to say something.\"</p>\n+          </div>\n+        )}\n+\n         {messages.map((msg, i) => (\n-          <div key={i} style={{\n-            background: msg.sender === \"You\" ? \"#ff0055\" : \"#222\",\n-            padding: \"15px\", borderRadius: \"15px\", marginBottom: \"15px\", whiteSpace: \"pre-wrap\"\n-          }}>\n-            <strong>{msg.sender}:</strong> {msg.text}\n+          <div key={i} className={`flex ${msg.role === 'user' ? 'justify-end' : 'justify-start'} animate-in slide-in-from-bottom-2 duration-500`}>\n+            <div className={`max-w-[85%] p-6 rounded-[2rem] shadow-2xl ${msg.role === 'user' ? 'bg-rose-600 rounded-tr-none' : 'bg-neutral-900 border border-white/5 rounded-tl-none'}`}>\n+              <div className=\"text-[17px] leading-relaxed font-medium\">\n+                {msg.text.includes('[Coach\\'s Corner]') ? (\n+                  <>\n+                    <p className=\"mb-4\">{msg.text.split('[Coach\\'s Corner]')[0]}</p>\n+                    <div className=\"p-4 bg-white/5 rounded-2xl border-l-2 border-rose-500 italic text-sm text-neutral-300\">\n+                      <span className=\"text-[10px] font-black text-rose-500 uppercase block mb-1\">Coach's Insight</span>\n+                      {msg.text.split('[Coach\\'s Corner]')[1]}\n+                    </div>\n+                  </>\n+                ) : msg.text}\n+              </div>\n+            </div>\n           </div>\n         ))}\n-        <div ref={scrollRef} />\n-      </div>\n \n-      <div style={{ width: \"100%\", maxWidth: \"600px\", display: \"flex\", gap: \"10px\", padding: \"20px\", background: \"#000\", position: \"sticky\", bottom: 0 }}>\n-        <textarea \n-          value={input} \n-          onChange={(e) => setInput(e.target.value)}\n-          placeholder=\"Type something...\"\n-          style={{ flex: 1, padding: \"12px\", borderRadius: \"10px\", background: \"#111\", color: \"#fff\", border: \"1px solid #333\" }}\n-        />\n-        <button onClick={startListening} style={{ width: \"50px\", height: \"50px\", borderRadius: \"50%\", border: \"none\", background: isListening ? \"#fff\" : \"#ff0055\", cursor: \"pointer\", fontSize: \"20px\" }}>\n-          {isListening ? \"üõë\" : \"üé§\"}\n+        {currentOutput && (\n+          <div className=\"flex justify-start\">\n+             <div className=\"bg-neutral-900/50 border border-rose-500/20 px-8 py-5 rounded-[2rem] rounded-tl-none animate-pulse text-lg italic\">\n+               {currentOutput}\n+             </div>\n+          </div>\n+        )}\n+        <div ref={transcriptEndRef} />\n+      </main>\n+\n+      {/* Mic Controls */}\n+      <footer className=\"p-10 flex flex-col items-center gap-6 bg-gradient-to-t from-black to-transparent\">\n+        <button \n+          onClick={handleConnect} \n+          disabled={isProcessing}\n+          className={`w-28 h-28 rounded-[2.5rem] flex items-center justify-center transition-all duration-500 transform hover:scale-105 active:scale-95 shadow-2xl ${\n+            isConnected ? 'bg-white text-black' : 'bg-rose-600 text-white shadow-rose-500/30'\n+          }`}\n+        >\n+          {isConnected ? <MicOff className=\"w-12 h-12\" /> : <Mic className=\"w-12 h-12\" />}\n         </button>\n-        <button onClick={() => handleSendMessage(input)} disabled={loading} style={{ padding: \"0 20px\", borderRadius: \"10px\", border: \"none\", background: \"#fff\", fontWeight: \"bold\", cursor: \"pointer\" }}>\n-          {loading ? \"...\" : \"Send\"}\n-        </button>\n-      </div>\n+        <p className=\"text-[11px] font-black uppercase tracking-[0.4em] opacity-40\">\n+          {isConnected ? 'Buddy is listening...' : 'Tap to start talking'}\n+        </p>\n+      </footer>\n     </div>\n   );\n-}\n+};\n+\n+export default App;\n\\ No newline at end of file\n"
                },
                {
                    "date": 1769104917742,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,49 +1,53 @@\n-import React, { useState, useEffect, useRef, useMemo } from 'react';\n+import React, { useState, useEffect, useRef } from 'react';\n import { GoogleGenAI, Modality, LiveServerMessage } from '@google/genai';\n import { SYSTEM_INSTRUCTION, SAFETY_SETTINGS } from './constants';\n import { decode, decodeAudioData, createPcmBlob } from './services/audioUtils';\n-import { Mic, MicOff, Heart, Zap, User, Sparkles, MessageSquare } from 'lucide-react';\n+import { Mic, MicOff, Heart, Zap, Sparkles, AlertCircle } from 'lucide-react';\n \n const App: React.FC = () => {\n   const [isConnected, setIsConnected] = useState(false);\n   const [messages, setMessages] = useState<{role: string, text: string}[]>([]);\n   const [currentOutput, setCurrentOutput] = useState('');\n   const [isProcessing, setIsProcessing] = useState(false);\n+  const [connError, setConnError] = useState<string | null>(null);\n   \n   // Stats\n   const [bondScore, setBondScore] = useState(10);\n   const [proLevel, setProLevel] = useState(5);\n-  const [currentMood, setCurrentMood] = useState('HAPPY');\n \n-  // Audio Refs\n+  // Audio & Session Refs\n   const inputAudioContextRef = useRef<AudioContext | null>(null);\n   const outputAudioContextRef = useRef<AudioContext | null>(null);\n   const nextStartTimeRef = useRef(0);\n   const sourcesRef = useRef<Set<AudioBufferSourceNode>>(new Set());\n   const activeSessionRef = useRef<any>(null);\n   const streamRef = useRef<MediaStream | null>(null);\n   const transcriptEndRef = useRef<HTMLDivElement>(null);\n \n-  // Metadata Parsing (Stats update)\n   const parseMetadata = (text: string) => {\n-    const moodMatch = text.match(/MOOD:\\s*(\\w+)/);\n     const bondMatch = text.match(/BOND_SCORE:\\s*(\\d+)/);\n     const levelMatch = text.match(/PRO_LEVEL:\\s*(\\d+)/);\n-\n-    if (moodMatch) setCurrentMood(moodMatch[1].toUpperCase());\n     if (bondMatch) setBondScore(parseInt(bondMatch[1]));\n     if (levelMatch) setProLevel(parseInt(levelMatch[1]));\n-\n     return text.replace(/\\[METADATA\\][\\s\\S]*?\\[\\/METADATA\\]/, '').trim();\n   };\n \n   const handleConnect = async () => {\n     if (isConnected) { cleanup(); return; }\n+    \n     try {\n       setIsProcessing(true);\n+      setConnError(null);\n+\n+      // API Key ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡¶æ\n       // @ts-ignore\n       const apiKey = import.meta.env.VITE_GEMINI_API_KEY || process.env.GEMINI_API_KEY;\n+      \n+      if (!apiKey) {\n+        throw new Error(\"API Key not found! Please check your .env file.\");\n+      }\n+\n       const ai = new GoogleGenAI({ apiKey });\n \n       inputAudioContextRef.current = new AudioContext({ sampleRate: 16000 });\n       outputAudioContextRef.current = new AudioContext({ sampleRate: 24000 });\n@@ -51,27 +55,40 @@\n       streamRef.current = await navigator.mediaDevices.getUserMedia({ \n         audio: { channelCount: 1, sampleRate: 16000, echoCancellation: true } \n       });\n \n+      console.log(\"üöÄ Connecting to Gemini Live...\");\n+\n       const session = await ai.live.connect({\n-        model: 'gemini-2.0-flash-exp', // ‡¶è‡¶Ü‡¶á ‡¶∏‡ßç‡¶ü‡ßÅ‡¶°‡¶ø‡¶ì‡¶∞ ‡¶∏‡ßá‡¶á ‡¶∞‡¶ø‡ßü‡ßá‡¶≤‡¶ø‡¶∏‡ßç‡¶ü‡¶ø‡¶ï ‡¶Æ‡¶°‡ßá‡¶≤\n+        model: 'gemini-2.0-flash-exp', // ‡¶Ø‡¶¶‡¶ø ‡¶è‡¶ü‡¶æ ‡¶ï‡¶æ‡¶ú ‡¶®‡¶æ ‡¶ï‡¶∞‡ßá, ‡¶§‡¶¨‡ßá 'gemini-2.0-flash' ‡¶ü‡ßç‡¶∞‡¶æ‡¶á ‡¶ï‡¶∞‡ßÅ‡¶®\n         callbacks: {\n           onopen: () => {\n-            setIsConnected(true); setIsProcessing(false);\n+            console.log(\"‚úÖ WebSocket Connected!\");\n+            setIsConnected(true);\n+            setIsProcessing(false);\n+            \n             if (inputAudioContextRef.current && streamRef.current) {\n               const source = inputAudioContextRef.current.createMediaStreamSource(streamRef.current);\n-              const processor = inputAudioContextRef.current.createScriptProcessor(2048, 1, 1);\n+              // Buffer size 4096 ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶õ‡¶ø ‡¶Ø‡¶æ‡¶§‡ßá socket overflow ‡¶®‡¶æ ‡¶π‡ßü\n+              const processor = inputAudioContextRef.current.createScriptProcessor(4096, 1, 1);\n+              \n               processor.onaudioprocess = (e) => {\n-                if (activeSessionRef.current) {\n-                  activeSessionRef.current.sendRealtimeInput({ media: createPcmBlob(e.inputBuffer.getChannelData(0)) });\n+                // ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡¶õ‡¶ø ‡¶∏‡¶ï‡ßá‡¶ü ‡¶ì‡¶™‡ßá‡¶® ‡¶Ü‡¶õ‡ßá ‡¶ï‡¶ø ‡¶®‡¶æ\n+                if (activeSessionRef.current && activeSessionRef.current.ws?.readyState === 1) {\n+                  activeSessionRef.current.sendRealtimeInput({ \n+                    media: createPcmBlob(e.inputBuffer.getChannelData(0)) \n+                  });\n                 }\n               };\n+              \n               source.connect(processor);\n               processor.connect(inputAudioContextRef.current.destination);\n             }\n           },\n           onmessage: async (m: LiveServerMessage) => {\n-            if (m.serverContent?.outputTranscription) setCurrentOutput(p => p + m.serverContent!.outputTranscription!.text);\n+            if (m.serverContent?.outputTranscription) {\n+              setCurrentOutput(p => p + m.serverContent!.outputTranscription!.text);\n+            }\n             \n             const base64 = m.serverContent?.modelTurn?.parts[0]?.inlineData?.data;\n             if (base64 && outputAudioContextRef.current) {\n               const ctx = outputAudioContextRef.current;\n@@ -93,44 +110,62 @@\n             if (m.serverContent?.turnComplete) {\n               setMessages(prev => [...prev, { role: 'model', text: parseMetadata(currentOutput) }]);\n               setCurrentOutput('');\n             }\n+          },\n+          onclose: (e) => {\n+            console.warn(\"‚ùå Connection Closed:\", e);\n+            cleanup();\n+          },\n+          onerror: (e) => {\n+            console.error(\"üî• WebSocket Error:\", e);\n+            setConnError(\"Connection failed. Check your API key or network.\");\n+            cleanup();\n           }\n         },\n         config: {\n           responseModalities: [Modality.AUDIO],\n-          speechConfig: { voiceConfig: { prebuiltVoiceConfig: { voiceName: 'Aoide' } } }, // Aoide ‡¶π‡¶≤‡ßã ‡¶∏‡ßá‡¶á ‡¶Æ‡¶ø‡¶∑‡ßç‡¶ü‡¶ø ‡¶Æ‡ßá‡ßü‡ßá‡¶≤‡¶ø ‡¶ï‡¶®‡ßç‡¶†\n+          speechConfig: { voiceConfig: { prebuiltVoiceConfig: { voiceName: 'Aoide' } } },\n           systemInstruction: SYSTEM_INSTRUCTION,\n           safetySettings: SAFETY_SETTINGS as any,\n-          outputAudioTranscription: {},\n         },\n       });\n+      \n       activeSessionRef.current = session;\n-    } catch (err) { console.error(err); setIsProcessing(false); }\n+    } catch (err: any) { \n+      console.error(\"Connection Catch:\", err);\n+      setConnError(err.message || \"Failed to connect\");\n+      setIsProcessing(false); \n+      cleanup();\n+    }\n   };\n \n   const cleanup = () => {\n     setIsConnected(false);\n+    setIsProcessing(false);\n     streamRef.current?.getTracks().forEach(t => t.stop());\n     inputAudioContextRef.current?.close();\n     outputAudioContextRef.current?.close();\n-    activeSessionRef.current?.close();\n-    sourcesRef.current.forEach(s => s.stop());\n+    if (activeSessionRef.current) {\n+        try { activeSessionRef.current.close(); } catch(e) {}\n+    }\n+    activeSessionRef.current = null;\n+    sourcesRef.current.forEach(s => { try { s.stop(); } catch(e) {} });\n+    sourcesRef.current.clear();\n   };\n \n   useEffect(() => transcriptEndRef.current?.scrollIntoView({ behavior: 'smooth' }), [messages, currentOutput]);\n \n   return (\n-    <div className=\"min-h-screen bg-[#050505] text-white flex flex-col font-sans\">\n-      {/* Header */}\n+    <div className=\"min-h-screen bg-[#050505] text-white flex flex-col\">\n       <header className=\"p-6 border-b border-white/5 flex justify-between items-center bg-black/50 backdrop-blur-xl sticky top-0 z-50\">\n         <div className=\"flex flex-col\">\n-          <h1 className=\"text-2xl font-black tracking-tighter italic\">Hey <span className=\"text-rose-500\">Buddy</span></h1>\n+          <h1 className=\"text-2xl font-black italic\">Hey <span className=\"text-rose-500\">Buddy</span></h1>\n           <div className=\"flex gap-3 mt-1\">\n-            <span className=\"text-[10px] font-bold text-rose-400 uppercase tracking-widest flex items-center gap-1\">\n+            <span className=\"text-[10px] font-bold text-rose-400 uppercase flex items-center gap-1\">\n               <Heart className=\"w-3 h-3 fill-current\" /> Bond: {bondScore}%\n             </span>\n-            <span className=\"text-[10px] font-bold text-emerald-400 uppercase tracking-widest flex items-center gap-1\">\n+            <span className=\"text-[10px] font-bold text-emerald-400 uppercase flex items-center gap-1\">\n               <Zap className=\"w-3 h-3 fill-current\" /> Mastery: {proLevel}%\n             </span>\n           </div>\n         </div>\n@@ -138,27 +173,32 @@\n           {isConnected ? '‚Ä¢ Live Session' : 'Offline'}\n         </div>\n       </header>\n \n-      {/* Main Chat Area */}\n       <main className=\"flex-1 overflow-y-auto p-6 md:px-12 space-y-10 max-w-4xl mx-auto w-full\">\n+        {connError && (\n+          <div className=\"bg-rose-500/10 border border-rose-500/20 p-4 rounded-2xl flex items-center gap-3 text-rose-500 text-sm\">\n+            <AlertCircle className=\"w-5 h-5\" /> {connError}\n+          </div>\n+        )}\n+\n         {messages.length === 0 && !currentOutput && (\n           <div className=\"h-full flex flex-col items-center justify-center opacity-20 text-center py-20\">\n             <Heart className=\"w-20 h-20 mb-4 animate-pulse\" />\n-            <p className=\"text-xl font-medium italic\">\"I'm here... just waiting for you to say something.\"</p>\n+            <p className=\"text-xl italic\">\"Say something, I'm listening...\"</p>\n           </div>\n         )}\n \n         {messages.map((msg, i) => (\n-          <div key={i} className={`flex ${msg.role === 'user' ? 'justify-end' : 'justify-start'} animate-in slide-in-from-bottom-2 duration-500`}>\n-            <div className={`max-w-[85%] p-6 rounded-[2rem] shadow-2xl ${msg.role === 'user' ? 'bg-rose-600 rounded-tr-none' : 'bg-neutral-900 border border-white/5 rounded-tl-none'}`}>\n-              <div className=\"text-[17px] leading-relaxed font-medium\">\n-                {msg.text.includes('[Coach\\'s Corner]') ? (\n+          <div key={i} className={`flex ${msg.role === 'user' ? 'justify-end' : 'justify-start'}`}>\n+            <div className={`max-w-[85%] p-6 rounded-[2rem] ${msg.role === 'user' ? 'bg-rose-600 rounded-tr-none' : 'bg-neutral-900 border border-white/5 rounded-tl-none'}`}>\n+              <div className=\"text-[17px] font-medium leading-relaxed\">\n+                {msg.text.includes(\"[Coach's Corner]\") ? (\n                   <>\n-                    <p className=\"mb-4\">{msg.text.split('[Coach\\'s Corner]')[0]}</p>\n+                    <p className=\"mb-4\">{msg.text.split(\"[Coach's Corner]\")[0]}</p>\n                     <div className=\"p-4 bg-white/5 rounded-2xl border-l-2 border-rose-500 italic text-sm text-neutral-300\">\n                       <span className=\"text-[10px] font-black text-rose-500 uppercase block mb-1\">Coach's Insight</span>\n-                      {msg.text.split('[Coach\\'s Corner]')[1]}\n+                      {msg.text.split(\"[Coach's Corner]\")[1]}\n                     </div>\n                   </>\n                 ) : msg.text}\n               </div>\n@@ -175,21 +215,20 @@\n         )}\n         <div ref={transcriptEndRef} />\n       </main>\n \n-      {/* Mic Controls */}\n-      <footer className=\"p-10 flex flex-col items-center gap-6 bg-gradient-to-t from-black to-transparent\">\n+      <footer className=\"p-10 flex flex-col items-center gap-4\">\n         <button \n           onClick={handleConnect} \n           disabled={isProcessing}\n-          className={`w-28 h-28 rounded-[2.5rem] flex items-center justify-center transition-all duration-500 transform hover:scale-105 active:scale-95 shadow-2xl ${\n-            isConnected ? 'bg-white text-black' : 'bg-rose-600 text-white shadow-rose-500/30'\n-          }`}\n+          className={`w-24 h-24 rounded-[2.5rem] flex items-center justify-center transition-all duration-500 shadow-2xl ${\n+            isConnected ? 'bg-white text-black' : 'bg-rose-600 text-white'\n+          } ${isProcessing ? 'animate-pulse opacity-50' : ''}`}\n         >\n-          {isConnected ? <MicOff className=\"w-12 h-12\" /> : <Mic className=\"w-12 h-12\" />}\n+          {isConnected ? <MicOff className=\"w-10 h-10\" /> : <Mic className=\"w-10 h-10\" />}\n         </button>\n         <p className=\"text-[11px] font-black uppercase tracking-[0.4em] opacity-40\">\n-          {isConnected ? 'Buddy is listening...' : 'Tap to start talking'}\n+          {isProcessing ? 'Connecting...' : isConnected ? 'Listening...' : 'Tap to start talking'}\n         </p>\n       </footer>\n     </div>\n   );\n"
                },
                {
                    "date": 1769105126736,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -10,13 +10,11 @@\n   const [currentOutput, setCurrentOutput] = useState('');\n   const [isProcessing, setIsProcessing] = useState(false);\n   const [connError, setConnError] = useState<string | null>(null);\n   \n-  // Stats\n   const [bondScore, setBondScore] = useState(10);\n   const [proLevel, setProLevel] = useState(5);\n \n-  // Audio & Session Refs\n   const inputAudioContextRef = useRef<AudioContext | null>(null);\n   const outputAudioContextRef = useRef<AudioContext | null>(null);\n   const nextStartTimeRef = useRef(0);\n   const sourcesRef = useRef<Set<AudioBufferSourceNode>>(new Set());\n@@ -33,63 +31,40 @@\n   };\n \n   const handleConnect = async () => {\n     if (isConnected) { cleanup(); return; }\n-    \n     try {\n       setIsProcessing(true);\n       setConnError(null);\n-\n-      // API Key ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡¶æ\n+      \n       // @ts-ignore\n       const apiKey = import.meta.env.VITE_GEMINI_API_KEY || process.env.GEMINI_API_KEY;\n-      \n-      if (!apiKey) {\n-        throw new Error(\"API Key not found! Please check your .env file.\");\n-      }\n+      if (!apiKey) throw new Error(\"API Key missing! Check .env.local\");\n \n       const ai = new GoogleGenAI({ apiKey });\n-\n       inputAudioContextRef.current = new AudioContext({ sampleRate: 16000 });\n       outputAudioContextRef.current = new AudioContext({ sampleRate: 24000 });\n-      \n-      streamRef.current = await navigator.mediaDevices.getUserMedia({ \n-        audio: { channelCount: 1, sampleRate: 16000, echoCancellation: true } \n-      });\n+      streamRef.current = await navigator.mediaDevices.getUserMedia({ audio: { channelCount: 1, sampleRate: 16000 } });\n \n-      console.log(\"üöÄ Connecting to Gemini Live...\");\n-\n       const session = await ai.live.connect({\n-        model: 'gemini-2.0-flash-exp', // ‡¶Ø‡¶¶‡¶ø ‡¶è‡¶ü‡¶æ ‡¶ï‡¶æ‡¶ú ‡¶®‡¶æ ‡¶ï‡¶∞‡ßá, ‡¶§‡¶¨‡ßá 'gemini-2.0-flash' ‡¶ü‡ßç‡¶∞‡¶æ‡¶á ‡¶ï‡¶∞‡ßÅ‡¶®\n+        model: 'gemini-2.0-flash', // ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶≤‡¶ø‡¶∏‡ßç‡¶ü‡ßá‡¶∞ ‡¶∏‡¶¨‡¶ö‡ßá‡ßü‡ßá ‡¶∏‡ßç‡¶ü‡ßç‡¶Ø‡¶æ‡¶¨‡¶≤ ‡¶Æ‡¶°‡ßá‡¶≤\n         callbacks: {\n           onopen: () => {\n-            console.log(\"‚úÖ WebSocket Connected!\");\n-            setIsConnected(true);\n-            setIsProcessing(false);\n-            \n+            setIsConnected(true); setIsProcessing(false);\n             if (inputAudioContextRef.current && streamRef.current) {\n               const source = inputAudioContextRef.current.createMediaStreamSource(streamRef.current);\n-              // Buffer size 4096 ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶õ‡¶ø ‡¶Ø‡¶æ‡¶§‡ßá socket overflow ‡¶®‡¶æ ‡¶π‡ßü\n               const processor = inputAudioContextRef.current.createScriptProcessor(4096, 1, 1);\n-              \n               processor.onaudioprocess = (e) => {\n-                // ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡¶õ‡¶ø ‡¶∏‡¶ï‡ßá‡¶ü ‡¶ì‡¶™‡ßá‡¶® ‡¶Ü‡¶õ‡ßá ‡¶ï‡¶ø ‡¶®‡¶æ\n-                if (activeSessionRef.current && activeSessionRef.current.ws?.readyState === 1) {\n-                  activeSessionRef.current.sendRealtimeInput({ \n-                    media: createPcmBlob(e.inputBuffer.getChannelData(0)) \n-                  });\n+                if (activeSessionRef.current?.ws?.readyState === 1) {\n+                  activeSessionRef.current.sendRealtimeInput({ media: createPcmBlob(e.inputBuffer.getChannelData(0)) });\n                 }\n               };\n-              \n               source.connect(processor);\n               processor.connect(inputAudioContextRef.current.destination);\n             }\n           },\n           onmessage: async (m: LiveServerMessage) => {\n-            if (m.serverContent?.outputTranscription) {\n-              setCurrentOutput(p => p + m.serverContent!.outputTranscription!.text);\n-            }\n-            \n+            if (m.serverContent?.outputTranscription) setCurrentOutput(p => p + m.serverContent!.outputTranscription!.text);\n             const base64 = m.serverContent?.modelTurn?.parts[0]?.inlineData?.data;\n             if (base64 && outputAudioContextRef.current) {\n               const ctx = outputAudioContextRef.current;\n               nextStartTimeRef.current = Math.max(nextStartTimeRef.current, ctx.currentTime);\n@@ -99,73 +74,50 @@\n               src.start(nextStartTimeRef.current);\n               nextStartTimeRef.current += buf.duration;\n               sourcesRef.current.add(src);\n             }\n-\n-            if (m.serverContent?.interrupted) { \n-              sourcesRef.current.forEach(s => s.stop()); \n-              sourcesRef.current.clear(); \n-              nextStartTimeRef.current = 0; \n-            }\n-\n+            if (m.serverContent?.interrupted) { sourcesRef.current.forEach(s => s.stop()); sourcesRef.current.clear(); nextStartTimeRef.current = 0; }\n             if (m.serverContent?.turnComplete) {\n               setMessages(prev => [...prev, { role: 'model', text: parseMetadata(currentOutput) }]);\n               setCurrentOutput('');\n             }\n           },\n-          onclose: (e) => {\n-            console.warn(\"‚ùå Connection Closed:\", e);\n-            cleanup();\n-          },\n-          onerror: (e) => {\n-            console.error(\"üî• WebSocket Error:\", e);\n-            setConnError(\"Connection failed. Check your API key or network.\");\n-            cleanup();\n-          }\n+          onclose: (e) => { setConnError(`Closed: ${e.reason || 'Quota limit'}`); cleanup(); },\n+          onerror: (e) => { setConnError(\"WebSocket Error\"); cleanup(); }\n         },\n         config: {\n           responseModalities: [Modality.AUDIO],\n-          speechConfig: { voiceConfig: { prebuiltVoiceConfig: { voiceName: 'Aoide' } } },\n+          speechConfig: { voiceConfig: { prebuiltVoiceConfig: { voiceName: 'Kore' } } }, // ‡¶Æ‡¶ø‡¶∑‡ßç‡¶ü‡¶ø ‡¶Æ‡ßá‡ßü‡ßá‡¶≤‡¶ø ‡¶ï‡¶£‡ßç‡¶†\n           systemInstruction: SYSTEM_INSTRUCTION,\n           safetySettings: SAFETY_SETTINGS as any,\n+          outputAudioTranscription: {},\n         },\n       });\n-      \n       activeSessionRef.current = session;\n-    } catch (err: any) { \n-      console.error(\"Connection Catch:\", err);\n-      setConnError(err.message || \"Failed to connect\");\n-      setIsProcessing(false); \n-      cleanup();\n-    }\n+    } catch (err: any) { setConnError(err.message); setIsProcessing(false); cleanup(); }\n   };\n \n   const cleanup = () => {\n-    setIsConnected(false);\n-    setIsProcessing(false);\n+    setIsConnected(false); setIsProcessing(false);\n     streamRef.current?.getTracks().forEach(t => t.stop());\n-    inputAudioContextRef.current?.close();\n-    outputAudioContextRef.current?.close();\n-    if (activeSessionRef.current) {\n-        try { activeSessionRef.current.close(); } catch(e) {}\n-    }\n+    inputAudioContextRef.current?.close(); outputAudioContextRef.current?.close();\n+    if (activeSessionRef.current) try { activeSessionRef.current.close(); } catch(e) {}\n     activeSessionRef.current = null;\n-    sourcesRef.current.forEach(s => { try { s.stop(); } catch(e) {} });\n-    sourcesRef.current.clear();\n+    sourcesRef.current.forEach(s => { try { s.stop(); } catch(e) {} }); sourcesRef.current.clear();\n   };\n \n   useEffect(() => transcriptEndRef.current?.scrollIntoView({ behavior: 'smooth' }), [messages, currentOutput]);\n \n   return (\n-    <div className=\"min-h-screen bg-[#050505] text-white flex flex-col\">\n-      <header className=\"p-6 border-b border-white/5 flex justify-between items-center bg-black/50 backdrop-blur-xl sticky top-0 z-50\">\n+    <div className=\"min-h-screen bg-[#050505] text-white flex flex-col font-sans overflow-hidden\">\n+      <header className=\"p-6 border-b border-white/5 flex justify-between items-center bg-black/50 backdrop-blur-xl\">\n         <div className=\"flex flex-col\">\n-          <h1 className=\"text-2xl font-black italic\">Hey <span className=\"text-rose-500\">Buddy</span></h1>\n-          <div className=\"flex gap-3 mt-1\">\n-            <span className=\"text-[10px] font-bold text-rose-400 uppercase flex items-center gap-1\">\n+          <h1 className=\"text-2xl font-black italic tracking-tighter\">Hey <span className=\"text-rose-500\">Buddy</span></h1>\n+          <div className=\"flex gap-4 mt-1 opacity-80\">\n+            <span className=\"text-[10px] font-bold text-rose-400 uppercase tracking-widest flex items-center gap-1\">\n               <Heart className=\"w-3 h-3 fill-current\" /> Bond: {bondScore}%\n             </span>\n-            <span className=\"text-[10px] font-bold text-emerald-400 uppercase flex items-center gap-1\">\n+            <span className=\"text-[10px] font-bold text-emerald-400 uppercase tracking-widest flex items-center gap-1\">\n               <Zap className=\"w-3 h-3 fill-current\" /> Mastery: {proLevel}%\n             </span>\n           </div>\n         </div>\n@@ -174,30 +126,26 @@\n         </div>\n       </header>\n \n       <main className=\"flex-1 overflow-y-auto p-6 md:px-12 space-y-10 max-w-4xl mx-auto w-full\">\n-        {connError && (\n-          <div className=\"bg-rose-500/10 border border-rose-500/20 p-4 rounded-2xl flex items-center gap-3 text-rose-500 text-sm\">\n-            <AlertCircle className=\"w-5 h-5\" /> {connError}\n-          </div>\n-        )}\n-\n+        {connError && <div className=\"bg-rose-500/10 border border-rose-500/20 p-4 rounded-2xl flex items-center gap-3 text-rose-500 text-sm\"><AlertCircle className=\"w-5 h-5\" /> {connError}</div>}\n+        \n         {messages.length === 0 && !currentOutput && (\n-          <div className=\"h-full flex flex-col items-center justify-center opacity-20 text-center py-20\">\n+          <div className=\"h-full flex flex-col items-center justify-center opacity-20 text-center py-24\">\n             <Heart className=\"w-20 h-20 mb-4 animate-pulse\" />\n-            <p className=\"text-xl italic\">\"Say something, I'm listening...\"</p>\n+            <p className=\"text-xl italic\">\"I'm waiting to hear your voice...\"</p>\n           </div>\n         )}\n \n         {messages.map((msg, i) => (\n           <div key={i} className={`flex ${msg.role === 'user' ? 'justify-end' : 'justify-start'}`}>\n-            <div className={`max-w-[85%] p-6 rounded-[2rem] ${msg.role === 'user' ? 'bg-rose-600 rounded-tr-none' : 'bg-neutral-900 border border-white/5 rounded-tl-none'}`}>\n-              <div className=\"text-[17px] font-medium leading-relaxed\">\n+            <div className={`max-w-[85%] p-7 rounded-[2.5rem] shadow-2xl ${msg.role === 'user' ? 'bg-rose-600 rounded-tr-none' : 'bg-neutral-900 border border-white/5 rounded-tl-none'}`}>\n+              <div className=\"text-[18px] leading-relaxed font-medium\">\n                 {msg.text.includes(\"[Coach's Corner]\") ? (\n                   <>\n-                    <p className=\"mb-4\">{msg.text.split(\"[Coach's Corner]\")[0]}</p>\n-                    <div className=\"p-4 bg-white/5 rounded-2xl border-l-2 border-rose-500 italic text-sm text-neutral-300\">\n-                      <span className=\"text-[10px] font-black text-rose-500 uppercase block mb-1\">Coach's Insight</span>\n+                    <p className=\"mb-5\">{msg.text.split(\"[Coach's Corner]\")[0]}</p>\n+                    <div className=\"p-5 bg-white/5 rounded-3xl border-l-4 border-rose-500 italic text-sm text-neutral-300\">\n+                      <span className=\"text-[10px] font-black text-rose-500 uppercase block mb-1\">Coach's Corner</span>\n                       {msg.text.split(\"[Coach's Corner]\")[1]}\n                     </div>\n                   </>\n                 ) : msg.text}\n@@ -207,28 +155,28 @@\n         ))}\n \n         {currentOutput && (\n           <div className=\"flex justify-start\">\n-             <div className=\"bg-neutral-900/50 border border-rose-500/20 px-8 py-5 rounded-[2rem] rounded-tl-none animate-pulse text-lg italic\">\n+             <div className=\"bg-neutral-900/50 border border-rose-500/20 px-8 py-6 rounded-[2.5rem] rounded-tl-none animate-pulse text-lg italic\">\n                {currentOutput}\n              </div>\n           </div>\n         )}\n         <div ref={transcriptEndRef} />\n       </main>\n \n-      <footer className=\"p-10 flex flex-col items-center gap-4\">\n+      <footer className=\"p-12 flex flex-col items-center gap-6 bg-gradient-to-t from-black to-transparent\">\n         <button \n           onClick={handleConnect} \n           disabled={isProcessing}\n-          className={`w-24 h-24 rounded-[2.5rem] flex items-center justify-center transition-all duration-500 shadow-2xl ${\n-            isConnected ? 'bg-white text-black' : 'bg-rose-600 text-white'\n-          } ${isProcessing ? 'animate-pulse opacity-50' : ''}`}\n+          className={`w-32 h-32 rounded-[3rem] flex items-center justify-center transition-all duration-500 transform hover:scale-105 active:scale-95 shadow-3xl ${\n+            isConnected ? 'bg-white text-black' : 'bg-rose-600 text-white shadow-rose-500/40'\n+          }`}\n         >\n-          {isConnected ? <MicOff className=\"w-10 h-10\" /> : <Mic className=\"w-10 h-10\" />}\n+          {isConnected ? <MicOff className=\"w-14 h-14\" /> : <Mic className=\"w-14 h-14\" />}\n         </button>\n-        <p className=\"text-[11px] font-black uppercase tracking-[0.4em] opacity-40\">\n-          {isProcessing ? 'Connecting...' : isConnected ? 'Listening...' : 'Tap to start talking'}\n+        <p className=\"text-[11px] font-black uppercase tracking-[0.5em] opacity-40\">\n+          {isConnected ? 'Buddy is Listening' : 'Connect with Buddy'}\n         </p>\n       </footer>\n     </div>\n   );\n"
                }
            ],
            "date": 1768924820751,
            "name": "Commit-0",
            "content": "import React, { useState, useEffect, useRef, useMemo } from 'react';\nimport { GoogleGenAI, Modality, LiveServerMessage } from '@google/genai';\nimport { SYSTEM_INSTRUCTION, SAFETY_SETTINGS } from './constants';\nimport { decode, decodeAudioData, createPcmBlob } from './services/audioUtils';\nimport { ChatMessage } from './types';\nimport {\n  Mic,\n  MicOff,\n  Heart,\n  Menu,\n  X,\n  Plus,\n  Trash2,\n  User,\n  Sparkles,\n  Zap\n} from 'lucide-react';\n\n/* -------------------- TYPES -------------------- */\ninterface SavedChat {\n  id: string;\n  timestamp: number;\n  preview: string;\n  messages: ChatMessage[];\n  stats?: { bond: number; level: number; mood: string };\n}\n\n/* -------------------- APP -------------------- */\nconst App: React.FC = () => {\n  const [isConnected, setIsConnected] = useState(false);\n  const [isProcessing, setIsProcessing] = useState(false);\n  const [messages, setMessages] = useState<ChatMessage[]>([]);\n  const [currentInput, setCurrentInput] = useState('');\n  const [currentOutput, setCurrentOutput] = useState('');\n  const [error, setError] = useState<string | null>(null);\n\n  const [bondScore, setBondScore] = useState(10);\n  const [proLevel, setProLevel] = useState(5);\n  const [currentMood, setCurrentMood] = useState('NEUTRAL');\n\n  const [isSidebarOpen, setIsSidebarOpen] = useState(false);\n  const [chatHistory, setChatHistory] = useState<SavedChat[]>([]);\n  const [activeChatId, setActiveChatId] = useState<string | null>(null);\n\n  /* -------- AUDIO & SESSION REFS -------- */\n  const inputAudioCtx = useRef<AudioContext | null>(null);\n  const outputAudioCtx = useRef<AudioContext | null>(null);\n  const micStream = useRef<MediaStream | null>(null);\n  const processorNode = useRef<ScriptProcessorNode | null>(null);\n  const nextStartTime = useRef(0);\n  const audioSources = useRef<Set<AudioBufferSourceNode>>(new Set());\n\n  const sessionPromiseRef = useRef<any>(null);\n  const activeSessionRef = useRef<any>(null);\n\n  const transcriptEndRef = useRef<HTMLDivElement>(null);\n\n  /* -------------------- EFFECTS -------------------- */\n  useEffect(() => {\n    const saved = localStorage.getItem('hey_buddy_history_v2');\n    if (saved) setChatHistory(JSON.parse(saved));\n  }, []);\n\n  useEffect(() => {\n    transcriptEndRef.current?.scrollIntoView({ behavior: 'smooth' });\n  }, [messages, currentInput, currentOutput]);\n\n  /* -------------------- HELPERS -------------------- */\n  const parseMetadata = (text: string) => {\n    const match = text.match(/\\[METADATA\\]([\\s\\S]*?)\\[\\/METADATA\\]/);\n    if (!match) return text;\n\n    const meta = match[1];\n    const mood = meta.match(/MOOD:\\s*(\\w+)/);\n    const bond = meta.match(/BOND_SCORE:\\s*(\\d+)/);\n    const level = meta.match(/PRO_LEVEL:\\s*(\\d+)/);\n\n    if (mood) setCurrentMood(mood[1]);\n    if (bond) setBondScore(+bond[1]);\n    if (level) setProLevel(+level[1]);\n\n    return text.replace(match[0], '').trim();\n  };\n\n  const moodColor = useMemo(() => {\n    switch (currentMood) {\n      case 'ROMANTIC': return 'rose';\n      case 'DEEP': return 'indigo';\n      case 'HAPPY': return 'amber';\n      default: return 'rose';\n    }\n  }, [currentMood]);\n\n  /* -------------------- CLEANUP -------------------- */\n  const cleanup = () => {\n    setIsConnected(false);\n    setIsProcessing(false);\n\n    processorNode.current?.disconnect();\n    processorNode.current = null;\n\n    micStream.current?.getTracks().forEach(t => t.stop());\n    micStream.current = null;\n\n    inputAudioCtx.current?.close();\n    outputAudioCtx.current?.close();\n    inputAudioCtx.current = null;\n    outputAudioCtx.current = null;\n\n    audioSources.current.forEach(s => s.stop());\n    audioSources.current.clear();\n    nextStartTime.current = 0;\n\n    sessionPromiseRef.current?.then((s: any) => s.close?.());\n    sessionPromiseRef.current = null;\n    activeSessionRef.current = null;\n  };\n\n  /* -------------------- MAIN CONNECT -------------------- */\n  const handleConnect = async () => {\n    if (isConnected) {\n      cleanup();\n      return;\n    }\n\n    try {\n      setError(null);\n      setIsProcessing(true);\n\n      /* üîê VITE ENV FIX */\n      const ai = new GoogleGenAI({\n        apiKey: import.meta.env.VITE_GEMINI_API_KEY,\n      });\n\n      /* üéß AUDIO CONTEXT (USER GESTURE SAFE) */\n      inputAudioCtx.current = new AudioContext({ sampleRate: 16000 });\n      outputAudioCtx.current = new AudioContext({ sampleRate: 24000 });\n\n      /* üî• CRITICAL FIX */\n      await inputAudioCtx.current.resume();\n      await outputAudioCtx.current.resume();\n\n      /* üé§ MIC ACCESS */\n      micStream.current = await navigator.mediaDevices.getUserMedia({\n        audio: {\n          channelCount: 1,\n          sampleRate: 16000,\n          echoCancellation: true,\n          noiseSuppression: true,\n          autoGainControl: true,\n        },\n      });\n\n      const source = inputAudioCtx.current.createMediaStreamSource(micStream.current);\n      processorNode.current = inputAudioCtx.current.createScriptProcessor(256, 1, 1);\n\n      processorNode.current.onaudioprocess = e => {\n        if (!activeSessionRef.current) return;\n        const input = e.inputBuffer.getChannelData(0);\n        activeSessionRef.current.sendRealtimeInput({\n          media: createPcmBlob(input),\n        });\n      };\n\n      source.connect(processorNode.current);\n      processorNode.current.connect(inputAudioCtx.current.destination);\n\n      /* ü§ñ GEMINI LIVE SESSION */\n      const sessionPromise = ai.live.connect({\n        model: 'gemini-2.5-flash-native-audio-preview-12-2025',\n        callbacks: {\n          onopen: () => {\n            setIsConnected(true);\n            setIsProcessing(false);\n          },\n          onmessage: async (m: LiveServerMessage) => {\n            if (m.serverContent?.outputTranscription)\n              setCurrentOutput(p => p + m.serverContent.outputTranscription!.text);\n\n            if (m.serverContent?.inputTranscription)\n              setCurrentInput(p => p + m.serverContent.inputTranscription!.text);\n\n            const audio = m.serverContent?.modelTurn?.parts?.[0]?.inlineData?.data;\n            if (audio && outputAudioCtx.current) {\n              const ctx = outputAudioCtx.current;\n              nextStartTime.current = Math.max(ctx.currentTime, nextStartTime.current);\n\n              const buf = await decodeAudioData(decode(audio), ctx, 24000, 1);\n              const src = ctx.createBufferSource();\n              src.buffer = buf;\n              src.connect(ctx.destination);\n              src.start(nextStartTime.current);\n              nextStartTime.current += buf.duration;\n              audioSources.current.add(src);\n              src.onended = () => audioSources.current.delete(src);\n            }\n\n            if (m.serverContent?.turnComplete) {\n              setMessages(prev => [\n                ...prev,\n                { role: 'user', text: currentInput.trim() },\n                { role: 'model', text: parseMetadata(currentOutput.trim()) },\n              ]);\n              setCurrentInput('');\n              setCurrentOutput('');\n            }\n          },\n          onclose: cleanup,\n          onerror: () => {\n            setError('Connection error');\n            cleanup();\n          },\n        },\n        config: {\n          systemInstruction: SYSTEM_INSTRUCTION,\n          safetySettings: SAFETY_SETTINGS,\n          responseModalities: [Modality.AUDIO],\n          inputAudioTranscription: {},\n          outputAudioTranscription: {},\n          speechConfig: {\n            voiceConfig: {\n              prebuiltVoiceConfig: { voiceName: 'Kore' },\n            },\n          },\n        },\n      });\n\n      sessionPromiseRef.current = sessionPromise;\n      sessionPromise.then((s: any) => (activeSessionRef.current = s));\n    } catch (err) {\n      setError('Microphone permission blocked');\n      setIsProcessing(false);\n    }\n  };\n\n  /* -------------------- UI -------------------- */\n  return (\n    <div className=\"flex flex-col h-screen bg-black text-white\">\n      <header className=\"p-6 flex justify-between items-center\">\n        <h1 className=\"text-2xl font-black\">\n          Hey <span className=\"text-rose-500 italic\">Buddy</span>\n        </h1>\n        <div className=\"flex gap-3\">\n          <span className=\"text-xs bg-rose-500/10 px-3 py-1 rounded-full\">\n            <Zap className=\"inline w-3 h-3 mr-1\" /> Level {proLevel}%\n          </span>\n          <span className=\"text-xs bg-indigo-500/10 px-3 py-1 rounded-full\">\n            <Heart className=\"inline w-3 h-3 mr-1\" /> Bond {bondScore}%\n          </span>\n        </div>\n      </header>\n\n      <main className=\"flex-1 overflow-y-auto px-6 space-y-6\">\n        {messages.map((m, i) => (\n          <div key={i} className={`flex ${m.role === 'user' ? 'justify-end' : 'justify-start'}`}>\n            <div className={`max-w-[80%] p-5 rounded-3xl ${\n              m.role === 'user' ? 'bg-rose-600' : 'bg-white/10'\n            }`}>\n              {m.text}\n            </div>\n          </div>\n        ))}\n        {currentInput && <div className=\"italic opacity-50\">{currentInput}</div>}\n        {currentOutput && <div className=\"opacity-70\">{currentOutput}</div>}\n        <div ref={transcriptEndRef} />\n      </main>\n\n      <footer className=\"p-8 flex flex-col items-center\">\n        <button\n          onClick={handleConnect}\n          disabled={isProcessing}\n          className={`w-24 h-24 rounded-full flex items-center justify-center transition-all ${\n            isConnected ? 'bg-neutral-800' : 'bg-rose-600'\n          }`}\n        >\n          {isConnected ? <MicOff size={40} /> : <Mic size={40} />}\n        </button>\n        <p className=\"mt-4 text-xs uppercase tracking-widest\">\n          {isConnected ? 'Listening...' : 'Connect with Buddy'}\n        </p>\n        {error && <p className=\"text-red-400 text-xs mt-2\">{error}</p>}\n      </footer>\n    </div>\n  );\n};\n\nexport default App;\n"
        }
    ]
}