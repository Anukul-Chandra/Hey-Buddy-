{
    "sourceFile": "App.tsx",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 12,
            "patches": [
                {
                    "date": 1768924820751,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1768927092014,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,14 +6,8 @@\n import {\n   Mic,\n   MicOff,\n   Heart,\n-  Menu,\n-  X,\n-  Plus,\n-  Trash2,\n-  User,\n-  Sparkles,\n   Zap\n } from 'lucide-react';\n \n /* -------------------- TYPES -------------------- */\n@@ -33,16 +27,12 @@\n   const [currentInput, setCurrentInput] = useState('');\n   const [currentOutput, setCurrentOutput] = useState('');\n   const [error, setError] = useState<string | null>(null);\n \n-  const [bondScore, setBondScore] = useState(10);\n-  const [proLevel, setProLevel] = useState(5);\n+  const [bondScore] = useState(10);\n+  const [proLevel] = useState(5);\n   const [currentMood, setCurrentMood] = useState('NEUTRAL');\n \n-  const [isSidebarOpen, setIsSidebarOpen] = useState(false);\n-  const [chatHistory, setChatHistory] = useState<SavedChat[]>([]);\n-  const [activeChatId, setActiveChatId] = useState<string | null>(null);\n-\n   /* -------- AUDIO & SESSION REFS -------- */\n   const inputAudioCtx = useRef<AudioContext | null>(null);\n   const outputAudioCtx = useRef<AudioContext | null>(null);\n   const micStream = useRef<MediaStream | null>(null);\n@@ -54,44 +44,21 @@\n   const activeSessionRef = useRef<any>(null);\n \n   const transcriptEndRef = useRef<HTMLDivElement>(null);\n \n-  /* -------------------- EFFECTS -------------------- */\n   useEffect(() => {\n-    const saved = localStorage.getItem('hey_buddy_history_v2');\n-    if (saved) setChatHistory(JSON.parse(saved));\n-  }, []);\n-\n-  useEffect(() => {\n     transcriptEndRef.current?.scrollIntoView({ behavior: 'smooth' });\n   }, [messages, currentInput, currentOutput]);\n \n-  /* -------------------- HELPERS -------------------- */\n-  const parseMetadata = (text: string) => {\n-    const match = text.match(/\\[METADATA\\]([\\s\\S]*?)\\[\\/METADATA\\]/);\n-    if (!match) return text;\n-\n-    const meta = match[1];\n-    const mood = meta.match(/MOOD:\\s*(\\w+)/);\n-    const bond = meta.match(/BOND_SCORE:\\s*(\\d+)/);\n-    const level = meta.match(/PRO_LEVEL:\\s*(\\d+)/);\n-\n-    if (mood) setCurrentMood(mood[1]);\n-    if (bond) setBondScore(+bond[1]);\n-    if (level) setProLevel(+level[1]);\n-\n-    return text.replace(match[0], '').trim();\n+  /* -------------------- PERMISSION CHECK -------------------- */\n+  const getMicPermissionState = async (): Promise<'granted' | 'prompt' | 'denied'> => {\n+    if (!navigator.permissions) return 'prompt';\n+    const status = await navigator.permissions.query({\n+      name: 'microphone' as PermissionName,\n+    });\n+    return status.state;\n   };\n \n-  const moodColor = useMemo(() => {\n-    switch (currentMood) {\n-      case 'ROMANTIC': return 'rose';\n-      case 'DEEP': return 'indigo';\n-      case 'HAPPY': return 'amber';\n-      default: return 'rose';\n-    }\n-  }, [currentMood]);\n-\n   /* -------------------- CLEANUP -------------------- */\n   const cleanup = () => {\n     setIsConnected(false);\n     setIsProcessing(false);\n@@ -126,22 +93,30 @@\n     try {\n       setError(null);\n       setIsProcessing(true);\n \n-      /* üîê VITE ENV FIX */\n+      // üîç CHECK PERMISSION STATE FIRST\n+      const permission = await getMicPermissionState();\n+      if (permission === 'denied') {\n+        setError(\n+          'Microphone is blocked. Please enable microphone access from browser or system settings and reload the page.'\n+        );\n+        setIsProcessing(false);\n+        return;\n+      }\n+\n+      /* üîê GEMINI INIT */\n       const ai = new GoogleGenAI({\n         apiKey: import.meta.env.VITE_GEMINI_API_KEY,\n       });\n \n-      /* üéß AUDIO CONTEXT (USER GESTURE SAFE) */\n+      /* üéß AUDIO CONTEXT (must be resumed on click) */\n       inputAudioCtx.current = new AudioContext({ sampleRate: 16000 });\n       outputAudioCtx.current = new AudioContext({ sampleRate: 24000 });\n-\n-      /* üî• CRITICAL FIX */\n       await inputAudioCtx.current.resume();\n       await outputAudioCtx.current.resume();\n \n-      /* üé§ MIC ACCESS */\n+      /* üé§ THIS LINE TRIGGERS THE BROWSER POPUP */\n       micStream.current = await navigator.mediaDevices.getUserMedia({\n         audio: {\n           channelCount: 1,\n           sampleRate: 16000,\n@@ -198,9 +173,9 @@\n             if (m.serverContent?.turnComplete) {\n               setMessages(prev => [\n                 ...prev,\n                 { role: 'user', text: currentInput.trim() },\n-                { role: 'model', text: parseMetadata(currentOutput.trim()) },\n+                { role: 'model', text: currentOutput.trim() },\n               ]);\n               setCurrentInput('');\n               setCurrentOutput('');\n             }\n@@ -226,10 +201,10 @@\n       });\n \n       sessionPromiseRef.current = sessionPromise;\n       sessionPromise.then((s: any) => (activeSessionRef.current = s));\n-    } catch (err) {\n-      setError('Microphone permission blocked');\n+    } catch {\n+      setError('Microphone permission was not granted.');\n       setIsProcessing(false);\n     }\n   };\n \n@@ -249,38 +224,37 @@\n           </span>\n         </div>\n       </header>\n \n-      <main className=\"flex-1 overflow-y-auto px-6 space-y-6\">\n-        {messages.map((m, i) => (\n-          <div key={i} className={`flex ${m.role === 'user' ? 'justify-end' : 'justify-start'}`}>\n-            <div className={`max-w-[80%] p-5 rounded-3xl ${\n-              m.role === 'user' ? 'bg-rose-600' : 'bg-white/10'\n-            }`}>\n-              {m.text}\n-            </div>\n-          </div>\n-        ))}\n-        {currentInput && <div className=\"italic opacity-50\">{currentInput}</div>}\n-        {currentOutput && <div className=\"opacity-70\">{currentOutput}</div>}\n-        <div ref={transcriptEndRef} />\n-      </main>\n-\n-      <footer className=\"p-8 flex flex-col items-center\">\n+      <main className=\"flex-1 flex flex-col justify-center items-center gap-6\">\n         <button\n           onClick={handleConnect}\n           disabled={isProcessing}\n-          className={`w-24 h-24 rounded-full flex items-center justify-center transition-all ${\n-            isConnected ? 'bg-neutral-800' : 'bg-rose-600'\n+          className={`w-28 h-28 rounded-full flex items-center justify-center transition-all ${\n+            isConnected ? 'bg-neutral-800' : 'bg-rose-600 hover:scale-110'\n           }`}\n         >\n-          {isConnected ? <MicOff size={40} /> : <Mic size={40} />}\n+          {isConnected ? <MicOff size={42} /> : <Mic size={42} />}\n         </button>\n-        <p className=\"mt-4 text-xs uppercase tracking-widest\">\n+\n+        <p className=\"text-xs uppercase tracking-widest opacity-80\">\n           {isConnected ? 'Listening...' : 'Connect with Buddy'}\n         </p>\n-        {error && <p className=\"text-red-400 text-xs mt-2\">{error}</p>}\n-      </footer>\n+\n+        {!isConnected && (\n+          <p className=\"text-[11px] opacity-50 text-center max-w-xs\">\n+            Click the button. Your browser will ask for microphone permission.\n+          </p>\n+        )}\n+\n+        {error && (\n+          <p className=\"text-red-400 text-xs text-center max-w-xs\">\n+            {error}\n+          </p>\n+        )}\n+      </main>\n+\n+      <div ref={transcriptEndRef} />\n     </div>\n   );\n };\n \n"
                },
                {
                    "date": 1768927331387,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,8 +6,14 @@\n import {\n   Mic,\n   MicOff,\n   Heart,\n+  Menu,\n+  X,\n+  Plus,\n+  Trash2,\n+  User,\n+  Sparkles,\n   Zap\n } from 'lucide-react';\n \n /* -------------------- TYPES -------------------- */\n@@ -27,12 +33,16 @@\n   const [currentInput, setCurrentInput] = useState('');\n   const [currentOutput, setCurrentOutput] = useState('');\n   const [error, setError] = useState<string | null>(null);\n \n-  const [bondScore] = useState(10);\n-  const [proLevel] = useState(5);\n+  const [bondScore, setBondScore] = useState(10);\n+  const [proLevel, setProLevel] = useState(5);\n   const [currentMood, setCurrentMood] = useState('NEUTRAL');\n \n+  const [isSidebarOpen, setIsSidebarOpen] = useState(false);\n+  const [chatHistory, setChatHistory] = useState<SavedChat[]>([]);\n+  const [activeChatId, setActiveChatId] = useState<string | null>(null);\n+\n   /* -------- AUDIO & SESSION REFS -------- */\n   const inputAudioCtx = useRef<AudioContext | null>(null);\n   const outputAudioCtx = useRef<AudioContext | null>(null);\n   const micStream = useRef<MediaStream | null>(null);\n@@ -44,13 +54,45 @@\n   const activeSessionRef = useRef<any>(null);\n \n   const transcriptEndRef = useRef<HTMLDivElement>(null);\n \n+  /* -------------------- EFFECTS -------------------- */\n   useEffect(() => {\n+    const saved = localStorage.getItem('hey_buddy_history_v2');\n+    if (saved) setChatHistory(JSON.parse(saved));\n+  }, []);\n+\n+  useEffect(() => {\n     transcriptEndRef.current?.scrollIntoView({ behavior: 'smooth' });\n   }, [messages, currentInput, currentOutput]);\n \n-  /* -------------------- PERMISSION CHECK -------------------- */\n+  /* -------------------- HELPERS -------------------- */\n+  const parseMetadata = (text: string) => {\n+    const match = text.match(/\\[METADATA\\]([\\s\\S]*?)\\[\\/METADATA\\]/);\n+    if (!match) return text;\n+\n+    const meta = match[1];\n+    const mood = meta.match(/MOOD:\\s*(\\w+)/);\n+    const bond = meta.match(/BOND_SCORE:\\s*(\\d+)/);\n+    const level = meta.match(/PRO_LEVEL:\\s*(\\d+)/);\n+\n+    if (mood) setCurrentMood(mood[1]);\n+    if (bond) setBondScore(+bond[1]);\n+    if (level) setProLevel(+level[1]);\n+\n+    return text.replace(match[0], '').trim();\n+  };\n+\n+  const moodColor = useMemo(() => {\n+    switch (currentMood) {\n+      case 'ROMANTIC': return 'rose';\n+      case 'DEEP': return 'indigo';\n+      case 'HAPPY': return 'amber';\n+      default: return 'rose';\n+    }\n+  }, [currentMood]);\n+\n+  /* -------------------- MIC PERMISSION HELPER -------------------- */\n   const getMicPermissionState = async (): Promise<'granted' | 'prompt' | 'denied'> => {\n     if (!navigator.permissions) return 'prompt';\n     const status = await navigator.permissions.query({\n       name: 'microphone' as PermissionName,\n@@ -93,30 +135,29 @@\n     try {\n       setError(null);\n       setIsProcessing(true);\n \n-      // üîç CHECK PERMISSION STATE FIRST\n+      // ‚úÖ Permission state check\n       const permission = await getMicPermissionState();\n       if (permission === 'denied') {\n         setError(\n-          'Microphone is blocked. Please enable microphone access from browser or system settings and reload the page.'\n+          'Microphone is blocked. Please enable microphone access from browser or system settings and reload.'\n         );\n         setIsProcessing(false);\n         return;\n       }\n \n-      /* üîê GEMINI INIT */\n       const ai = new GoogleGenAI({\n         apiKey: import.meta.env.VITE_GEMINI_API_KEY,\n       });\n \n-      /* üéß AUDIO CONTEXT (must be resumed on click) */\n       inputAudioCtx.current = new AudioContext({ sampleRate: 16000 });\n       outputAudioCtx.current = new AudioContext({ sampleRate: 24000 });\n+\n       await inputAudioCtx.current.resume();\n       await outputAudioCtx.current.resume();\n \n-      /* üé§ THIS LINE TRIGGERS THE BROWSER POPUP */\n+      // üé§ THIS triggers browser popup\n       micStream.current = await navigator.mediaDevices.getUserMedia({\n         audio: {\n           channelCount: 1,\n           sampleRate: 16000,\n@@ -139,9 +180,8 @@\n \n       source.connect(processorNode.current);\n       processorNode.current.connect(inputAudioCtx.current.destination);\n \n-      /* ü§ñ GEMINI LIVE SESSION */\n       const sessionPromise = ai.live.connect({\n         model: 'gemini-2.5-flash-native-audio-preview-12-2025',\n         callbacks: {\n           onopen: () => {\n@@ -173,9 +213,9 @@\n             if (m.serverContent?.turnComplete) {\n               setMessages(prev => [\n                 ...prev,\n                 { role: 'user', text: currentInput.trim() },\n-                { role: 'model', text: currentOutput.trim() },\n+                { role: 'model', text: parseMetadata(currentOutput.trim()) },\n               ]);\n               setCurrentInput('');\n               setCurrentOutput('');\n             }\n@@ -207,9 +247,9 @@\n       setIsProcessing(false);\n     }\n   };\n \n-  /* -------------------- UI -------------------- */\n+  /* -------------------- UI (UNCHANGED) -------------------- */\n   return (\n     <div className=\"flex flex-col h-screen bg-black text-white\">\n       <header className=\"p-6 flex justify-between items-center\">\n         <h1 className=\"text-2xl font-black\">\n@@ -224,37 +264,38 @@\n           </span>\n         </div>\n       </header>\n \n-      <main className=\"flex-1 flex flex-col justify-center items-center gap-6\">\n+      <main className=\"flex-1 overflow-y-auto px-6 space-y-6\">\n+        {messages.map((m, i) => (\n+          <div key={i} className={`flex ${m.role === 'user' ? 'justify-end' : 'justify-start'}`}>\n+            <div className={`max-w-[80%] p-5 rounded-3xl ${\n+              m.role === 'user' ? 'bg-rose-600' : 'bg-white/10'\n+            }`}>\n+              {m.text}\n+            </div>\n+          </div>\n+        ))}\n+        {currentInput && <div className=\"italic opacity-50\">{currentInput}</div>}\n+        {currentOutput && <div className=\"opacity-70\">{currentOutput}</div>}\n+        <div ref={transcriptEndRef} />\n+      </main>\n+\n+      <footer className=\"p-8 flex flex-col items-center\">\n         <button\n           onClick={handleConnect}\n           disabled={isProcessing}\n-          className={`w-28 h-28 rounded-full flex items-center justify-center transition-all ${\n-            isConnected ? 'bg-neutral-800' : 'bg-rose-600 hover:scale-110'\n+          className={`w-24 h-24 rounded-full flex items-center justify-center transition-all ${\n+            isConnected ? 'bg-neutral-800' : 'bg-rose-600'\n           }`}\n         >\n-          {isConnected ? <MicOff size={42} /> : <Mic size={42} />}\n+          {isConnected ? <MicOff size={40} /> : <Mic size={40} />}\n         </button>\n-\n-        <p className=\"text-xs uppercase tracking-widest opacity-80\">\n+        <p className=\"mt-4 text-xs uppercase tracking-widest\">\n           {isConnected ? 'Listening...' : 'Connect with Buddy'}\n         </p>\n-\n-        {!isConnected && (\n-          <p className=\"text-[11px] opacity-50 text-center max-w-xs\">\n-            Click the button. Your browser will ask for microphone permission.\n-          </p>\n-        )}\n-\n-        {error && (\n-          <p className=\"text-red-400 text-xs text-center max-w-xs\">\n-            {error}\n-          </p>\n-        )}\n-      </main>\n-\n-      <div ref={transcriptEndRef} />\n+        {error && <p className=\"text-red-400 text-xs mt-2\">{error}</p>}\n+      </footer>\n     </div>\n   );\n };\n \n"
                },
                {
                    "date": 1768927572717,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,14 +6,8 @@\n import {\n   Mic,\n   MicOff,\n   Heart,\n-  Menu,\n-  X,\n-  Plus,\n-  Trash2,\n-  User,\n-  Sparkles,\n   Zap\n } from 'lucide-react';\n \n /* -------------------- TYPES -------------------- */\n@@ -37,12 +31,8 @@\n   const [bondScore, setBondScore] = useState(10);\n   const [proLevel, setProLevel] = useState(5);\n   const [currentMood, setCurrentMood] = useState('NEUTRAL');\n \n-  const [isSidebarOpen, setIsSidebarOpen] = useState(false);\n-  const [chatHistory, setChatHistory] = useState<SavedChat[]>([]);\n-  const [activeChatId, setActiveChatId] = useState<string | null>(null);\n-\n   /* -------- AUDIO & SESSION REFS -------- */\n   const inputAudioCtx = useRef<AudioContext | null>(null);\n   const outputAudioCtx = useRef<AudioContext | null>(null);\n   const micStream = useRef<MediaStream | null>(null);\n@@ -56,17 +46,12 @@\n   const transcriptEndRef = useRef<HTMLDivElement>(null);\n \n   /* -------------------- EFFECTS -------------------- */\n   useEffect(() => {\n-    const saved = localStorage.getItem('hey_buddy_history_v2');\n-    if (saved) setChatHistory(JSON.parse(saved));\n-  }, []);\n-\n-  useEffect(() => {\n     transcriptEndRef.current?.scrollIntoView({ behavior: 'smooth' });\n   }, [messages, currentInput, currentOutput]);\n \n-  /* -------------------- HELPERS -------------------- */\n+  /* -------------------- METADATA PARSER -------------------- */\n   const parseMetadata = (text: string) => {\n     const match = text.match(/\\[METADATA\\]([\\s\\S]*?)\\[\\/METADATA\\]/);\n     if (!match) return text;\n \n@@ -81,26 +66,8 @@\n \n     return text.replace(match[0], '').trim();\n   };\n \n-  const moodColor = useMemo(() => {\n-    switch (currentMood) {\n-      case 'ROMANTIC': return 'rose';\n-      case 'DEEP': return 'indigo';\n-      case 'HAPPY': return 'amber';\n-      default: return 'rose';\n-    }\n-  }, [currentMood]);\n-\n-  /* -------------------- MIC PERMISSION HELPER -------------------- */\n-  const getMicPermissionState = async (): Promise<'granted' | 'prompt' | 'denied'> => {\n-    if (!navigator.permissions) return 'prompt';\n-    const status = await navigator.permissions.query({\n-      name: 'microphone' as PermissionName,\n-    });\n-    return status.state;\n-  };\n-\n   /* -------------------- CLEANUP -------------------- */\n   const cleanup = () => {\n     setIsConnected(false);\n     setIsProcessing(false);\n@@ -135,29 +102,20 @@\n     try {\n       setError(null);\n       setIsProcessing(true);\n \n-      // ‚úÖ Permission state check\n-      const permission = await getMicPermissionState();\n-      if (permission === 'denied') {\n-        setError(\n-          'Microphone is blocked. Please enable microphone access from browser or system settings and reload.'\n-        );\n-        setIsProcessing(false);\n-        return;\n-      }\n-\n       const ai = new GoogleGenAI({\n         apiKey: import.meta.env.VITE_GEMINI_API_KEY,\n       });\n \n+      /* üéß AUDIO CONTEXTS (must be inside click) */\n       inputAudioCtx.current = new AudioContext({ sampleRate: 16000 });\n       outputAudioCtx.current = new AudioContext({ sampleRate: 24000 });\n \n       await inputAudioCtx.current.resume();\n       await outputAudioCtx.current.resume();\n \n-      // üé§ THIS triggers browser popup\n+      /* üî• THIS LINE TRIGGERS BROWSER POPUP */\n       micStream.current = await navigator.mediaDevices.getUserMedia({\n         audio: {\n           channelCount: 1,\n           sampleRate: 16000,\n@@ -242,9 +200,9 @@\n \n       sessionPromiseRef.current = sessionPromise;\n       sessionPromise.then((s: any) => (activeSessionRef.current = s));\n     } catch {\n-      setError('Microphone permission was not granted.');\n+      setError('Please allow microphone access to continue.');\n       setIsProcessing(false);\n     }\n   };\n \n@@ -267,11 +225,13 @@\n \n       <main className=\"flex-1 overflow-y-auto px-6 space-y-6\">\n         {messages.map((m, i) => (\n           <div key={i} className={`flex ${m.role === 'user' ? 'justify-end' : 'justify-start'}`}>\n-            <div className={`max-w-[80%] p-5 rounded-3xl ${\n-              m.role === 'user' ? 'bg-rose-600' : 'bg-white/10'\n-            }`}>\n+            <div\n+              className={`max-w-[80%] p-5 rounded-3xl ${\n+                m.role === 'user' ? 'bg-rose-600' : 'bg-white/10'\n+              }`}\n+            >\n               {m.text}\n             </div>\n           </div>\n         ))}\n"
                },
                {
                    "date": 1768928127059,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,9 +6,9 @@\n import {\n   Mic,\n   MicOff,\n   Heart,\n-  Zap\n+  Zap,\n } from 'lucide-react';\n \n /* -------------------- TYPES -------------------- */\n interface SavedChat {\n@@ -31,8 +31,11 @@\n   const [bondScore, setBondScore] = useState(10);\n   const [proLevel, setProLevel] = useState(5);\n   const [currentMood, setCurrentMood] = useState('NEUTRAL');\n \n+  // üîê SOFT MIC GATE\n+  const [showMicGate, setShowMicGate] = useState(false);\n+\n   /* -------- AUDIO & SESSION REFS -------- */\n   const inputAudioCtx = useRef<AudioContext | null>(null);\n   const outputAudioCtx = useRef<AudioContext | null>(null);\n   const micStream = useRef<MediaStream | null>(null);\n@@ -49,9 +52,9 @@\n   useEffect(() => {\n     transcriptEndRef.current?.scrollIntoView({ behavior: 'smooth' });\n   }, [messages, currentInput, currentOutput]);\n \n-  /* -------------------- METADATA PARSER -------------------- */\n+  /* -------------------- HELPERS -------------------- */\n   const parseMetadata = (text: string) => {\n     const match = text.match(/\\[METADATA\\]([\\s\\S]*?)\\[\\/METADATA\\]/);\n     if (!match) return text;\n \n@@ -102,30 +105,25 @@\n     try {\n       setError(null);\n       setIsProcessing(true);\n \n-      const ai = new GoogleGenAI({\n-        apiKey: import.meta.env.VITE_GEMINI_API_KEY,\n-      });\n-\n-      /* üéß AUDIO CONTEXTS (must be inside click) */\n-      inputAudioCtx.current = new AudioContext({ sampleRate: 16000 });\n-      outputAudioCtx.current = new AudioContext({ sampleRate: 24000 });\n-\n-      await inputAudioCtx.current.resume();\n-      await outputAudioCtx.current.resume();\n-\n-      /* üî• THIS LINE TRIGGERS BROWSER POPUP */\n+      // üé§ STEP 1: ASK MIC PERMISSION FIRST (popup here)\n       micStream.current = await navigator.mediaDevices.getUserMedia({\n         audio: {\n           channelCount: 1,\n-          sampleRate: 16000,\n           echoCancellation: true,\n           noiseSuppression: true,\n           autoGainControl: true,\n         },\n       });\n \n+      // üéß STEP 2: AUDIO CONTEXTS (AFTER permission)\n+      inputAudioCtx.current = new AudioContext({ sampleRate: 16000 });\n+      outputAudioCtx.current = new AudioContext({ sampleRate: 24000 });\n+\n+      await inputAudioCtx.current.resume();\n+      await outputAudioCtx.current.resume();\n+\n       const source = inputAudioCtx.current.createMediaStreamSource(micStream.current);\n       processorNode.current = inputAudioCtx.current.createScriptProcessor(256, 1, 1);\n \n       processorNode.current.onaudioprocess = e => {\n@@ -138,8 +136,13 @@\n \n       source.connect(processorNode.current);\n       processorNode.current.connect(inputAudioCtx.current.destination);\n \n+      // ü§ñ GEMINI\n+      const ai = new GoogleGenAI({\n+        apiKey: import.meta.env.VITE_GEMINI_API_KEY,\n+      });\n+\n       const sessionPromise = ai.live.connect({\n         model: 'gemini-2.5-flash-native-audio-preview-12-2025',\n         callbacks: {\n           onopen: () => {\n@@ -205,9 +208,9 @@\n       setIsProcessing(false);\n     }\n   };\n \n-  /* -------------------- UI (UNCHANGED) -------------------- */\n+  /* -------------------- UI -------------------- */\n   return (\n     <div className=\"flex flex-col h-screen bg-black text-white\">\n       <header className=\"p-6 flex justify-between items-center\">\n         <h1 className=\"text-2xl font-black\">\n@@ -225,13 +228,11 @@\n \n       <main className=\"flex-1 overflow-y-auto px-6 space-y-6\">\n         {messages.map((m, i) => (\n           <div key={i} className={`flex ${m.role === 'user' ? 'justify-end' : 'justify-start'}`}>\n-            <div\n-              className={`max-w-[80%] p-5 rounded-3xl ${\n-                m.role === 'user' ? 'bg-rose-600' : 'bg-white/10'\n-              }`}\n-            >\n+            <div className={`max-w-[80%] p-5 rounded-3xl ${\n+              m.role === 'user' ? 'bg-rose-600' : 'bg-white/10'\n+            }`}>\n               {m.text}\n             </div>\n           </div>\n         ))}\n@@ -241,21 +242,50 @@\n       </main>\n \n       <footer className=\"p-8 flex flex-col items-center\">\n         <button\n-          onClick={handleConnect}\n+          onClick={() => setShowMicGate(true)}\n           disabled={isProcessing}\n-          className={`w-24 h-24 rounded-full flex items-center justify-center transition-all ${\n-            isConnected ? 'bg-neutral-800' : 'bg-rose-600'\n-          }`}\n+          className=\"w-24 h-24 rounded-full bg-rose-600 flex items-center justify-center\"\n         >\n           {isConnected ? <MicOff size={40} /> : <Mic size={40} />}\n         </button>\n         <p className=\"mt-4 text-xs uppercase tracking-widest\">\n           {isConnected ? 'Listening...' : 'Connect with Buddy'}\n         </p>\n         {error && <p className=\"text-red-400 text-xs mt-2\">{error}</p>}\n       </footer>\n+\n+      {/* üé§ SOFT PRE-SCREEN */}\n+      {showMicGate && (\n+        <div className=\"fixed inset-0 bg-black/80 backdrop-blur-sm z-50 flex items-center justify-center\">\n+          <div className=\"bg-[#0b0b0b] border border-white/10 rounded-3xl p-8 w-[90%] max-w-sm text-center space-y-6\">\n+            <div className=\"text-5xl\">üé§</div>\n+            <h2 className=\"text-xl font-black\">Microphone Access Needed</h2>\n+            <p className=\"text-sm text-neutral-400\">\n+              To talk with Buddy, your browser will ask for microphone permission\n+              on the next step.\n+            </p>\n+            <div className=\"flex gap-4 justify-center\">\n+              <button\n+                onClick={() => setShowMicGate(false)}\n+                className=\"px-5 py-2 rounded-full bg-white/5\"\n+              >\n+                Cancel\n+              </button>\n+              <button\n+                onClick={() => {\n+                  setShowMicGate(false);\n+                  handleConnect();\n+                }}\n+                className=\"px-6 py-2 rounded-full bg-rose-600 font-bold\"\n+              >\n+                Continue\n+              </button>\n+            </div>\n+          </div>\n+        </div>\n+      )}\n     </div>\n   );\n };\n \n"
                },
                {
                    "date": 1768928541050,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,25 +1,11 @@\n-import React, { useState, useEffect, useRef, useMemo } from 'react';\n+import React, { useState, useEffect, useRef } from 'react';\n import { GoogleGenAI, Modality, LiveServerMessage } from '@google/genai';\n import { SYSTEM_INSTRUCTION, SAFETY_SETTINGS } from './constants';\n import { decode, decodeAudioData, createPcmBlob } from './services/audioUtils';\n import { ChatMessage } from './types';\n-import {\n-  Mic,\n-  MicOff,\n-  Heart,\n-  Zap,\n-} from 'lucide-react';\n+import { Mic, MicOff, Heart, Zap } from 'lucide-react';\n \n-/* -------------------- TYPES -------------------- */\n-interface SavedChat {\n-  id: string;\n-  timestamp: number;\n-  preview: string;\n-  messages: ChatMessage[];\n-  stats?: { bond: number; level: number; mood: string };\n-}\n-\n /* -------------------- APP -------------------- */\n const App: React.FC = () => {\n   const [isConnected, setIsConnected] = useState(false);\n   const [isProcessing, setIsProcessing] = useState(false);\n@@ -27,11 +13,10 @@\n   const [currentInput, setCurrentInput] = useState('');\n   const [currentOutput, setCurrentOutput] = useState('');\n   const [error, setError] = useState<string | null>(null);\n \n-  const [bondScore, setBondScore] = useState(10);\n-  const [proLevel, setProLevel] = useState(5);\n-  const [currentMood, setCurrentMood] = useState('NEUTRAL');\n+  const [bondScore] = useState(10);\n+  const [proLevel] = useState(5);\n \n   // üîê SOFT MIC GATE\n   const [showMicGate, setShowMicGate] = useState(false);\n \n@@ -47,30 +32,12 @@\n   const activeSessionRef = useRef<any>(null);\n \n   const transcriptEndRef = useRef<HTMLDivElement>(null);\n \n-  /* -------------------- EFFECTS -------------------- */\n   useEffect(() => {\n     transcriptEndRef.current?.scrollIntoView({ behavior: 'smooth' });\n   }, [messages, currentInput, currentOutput]);\n \n-  /* -------------------- HELPERS -------------------- */\n-  const parseMetadata = (text: string) => {\n-    const match = text.match(/\\[METADATA\\]([\\s\\S]*?)\\[\\/METADATA\\]/);\n-    if (!match) return text;\n-\n-    const meta = match[1];\n-    const mood = meta.match(/MOOD:\\s*(\\w+)/);\n-    const bond = meta.match(/BOND_SCORE:\\s*(\\d+)/);\n-    const level = meta.match(/PRO_LEVEL:\\s*(\\d+)/);\n-\n-    if (mood) setCurrentMood(mood[1]);\n-    if (bond) setBondScore(+bond[1]);\n-    if (level) setProLevel(+level[1]);\n-\n-    return text.replace(match[0], '').trim();\n-  };\n-\n   /* -------------------- CLEANUP -------------------- */\n   const cleanup = () => {\n     setIsConnected(false);\n     setIsProcessing(false);\n@@ -101,23 +68,29 @@\n       cleanup();\n       return;\n     }\n \n+    setError(null);\n+    setIsProcessing(true);\n+\n+    // 1Ô∏è‚É£ MIC PERMISSION (ONLY mic error here)\n     try {\n-      setError(null);\n-      setIsProcessing(true);\n-\n-      // üé§ STEP 1: ASK MIC PERMISSION FIRST (popup here)\n       micStream.current = await navigator.mediaDevices.getUserMedia({\n         audio: {\n           channelCount: 1,\n           echoCancellation: true,\n           noiseSuppression: true,\n           autoGainControl: true,\n         },\n       });\n+    } catch {\n+      setError('Microphone access was denied.');\n+      setIsProcessing(false);\n+      return;\n+    }\n \n-      // üéß STEP 2: AUDIO CONTEXTS (AFTER permission)\n+    try {\n+      // 2Ô∏è‚É£ AUDIO CONTEXT (AFTER permission)\n       inputAudioCtx.current = new AudioContext({ sampleRate: 16000 });\n       outputAudioCtx.current = new AudioContext({ sampleRate: 24000 });\n \n       await inputAudioCtx.current.resume();\n@@ -136,17 +109,18 @@\n \n       source.connect(processorNode.current);\n       processorNode.current.connect(inputAudioCtx.current.destination);\n \n-      // ü§ñ GEMINI\n+      // 3Ô∏è‚É£ GEMINI CONNECT\n       const ai = new GoogleGenAI({\n         apiKey: import.meta.env.VITE_GEMINI_API_KEY,\n       });\n \n       const sessionPromise = ai.live.connect({\n         model: 'gemini-2.5-flash-native-audio-preview-12-2025',\n         callbacks: {\n           onopen: () => {\n+            setError(null);              // üî• FIX: clear old error\n             setIsConnected(true);\n             setIsProcessing(false);\n           },\n           onmessage: async (m: LiveServerMessage) => {\n@@ -174,17 +148,17 @@\n             if (m.serverContent?.turnComplete) {\n               setMessages(prev => [\n                 ...prev,\n                 { role: 'user', text: currentInput.trim() },\n-                { role: 'model', text: parseMetadata(currentOutput.trim()) },\n+                { role: 'model', text: currentOutput.trim() },\n               ]);\n               setCurrentInput('');\n               setCurrentOutput('');\n             }\n           },\n           onclose: cleanup,\n           onerror: () => {\n-            setError('Connection error');\n+            setError('Voice service is currently unavailable.');\n             cleanup();\n           },\n         },\n         config: {\n@@ -202,11 +176,12 @@\n       });\n \n       sessionPromiseRef.current = sessionPromise;\n       sessionPromise.then((s: any) => (activeSessionRef.current = s));\n-    } catch {\n-      setError('Please allow microphone access to continue.');\n-      setIsProcessing(false);\n+    } catch (err) {\n+      console.error(err);\n+      setError('Voice service is currently unavailable.');\n+      cleanup();\n     }\n   };\n \n   /* -------------------- UI -------------------- */\n@@ -254,17 +229,16 @@\n         </p>\n         {error && <p className=\"text-red-400 text-xs mt-2\">{error}</p>}\n       </footer>\n \n-      {/* üé§ SOFT PRE-SCREEN */}\n+      {/* üé§ SOFT MIC GATE */}\n       {showMicGate && (\n         <div className=\"fixed inset-0 bg-black/80 backdrop-blur-sm z-50 flex items-center justify-center\">\n           <div className=\"bg-[#0b0b0b] border border-white/10 rounded-3xl p-8 w-[90%] max-w-sm text-center space-y-6\">\n             <div className=\"text-5xl\">üé§</div>\n             <h2 className=\"text-xl font-black\">Microphone Access Needed</h2>\n             <p className=\"text-sm text-neutral-400\">\n-              To talk with Buddy, your browser will ask for microphone permission\n-              on the next step.\n+              Your browser will ask for microphone permission on the next step.\n             </p>\n             <div className=\"flex gap-4 justify-center\">\n               <button\n                 onClick={() => setShowMicGate(false)}\n"
                },
                {
                    "date": 1768928854438,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -4,41 +4,32 @@\n import { decode, decodeAudioData, createPcmBlob } from './services/audioUtils';\n import { ChatMessage } from './types';\n import { Mic, MicOff, Heart, Zap } from 'lucide-react';\n \n-/* -------------------- APP -------------------- */\n const App: React.FC = () => {\n   const [isConnected, setIsConnected] = useState(false);\n   const [isProcessing, setIsProcessing] = useState(false);\n   const [messages, setMessages] = useState<ChatMessage[]>([]);\n   const [currentInput, setCurrentInput] = useState('');\n   const [currentOutput, setCurrentOutput] = useState('');\n   const [error, setError] = useState<string | null>(null);\n \n-  const [bondScore] = useState(10);\n-  const [proLevel] = useState(5);\n-\n-  // üîê SOFT MIC GATE\n   const [showMicGate, setShowMicGate] = useState(false);\n \n-  /* -------- AUDIO & SESSION REFS -------- */\n   const inputAudioCtx = useRef<AudioContext | null>(null);\n   const outputAudioCtx = useRef<AudioContext | null>(null);\n   const micStream = useRef<MediaStream | null>(null);\n   const processorNode = useRef<ScriptProcessorNode | null>(null);\n-  const nextStartTime = useRef(0);\n-  const audioSources = useRef<Set<AudioBufferSourceNode>>(new Set());\n \n-  const sessionPromiseRef = useRef<any>(null);\n+  const sessionRef = useRef<any>(null);\n   const activeSessionRef = useRef<any>(null);\n \n   const transcriptEndRef = useRef<HTMLDivElement>(null);\n \n   useEffect(() => {\n     transcriptEndRef.current?.scrollIntoView({ behavior: 'smooth' });\n   }, [messages, currentInput, currentOutput]);\n \n-  /* -------------------- CLEANUP -------------------- */\n   const cleanup = () => {\n     setIsConnected(false);\n     setIsProcessing(false);\n \n@@ -52,18 +43,13 @@\n     outputAudioCtx.current?.close();\n     inputAudioCtx.current = null;\n     outputAudioCtx.current = null;\n \n-    audioSources.current.forEach(s => s.stop());\n-    audioSources.current.clear();\n-    nextStartTime.current = 0;\n-\n-    sessionPromiseRef.current?.then((s: any) => s.close?.());\n-    sessionPromiseRef.current = null;\n+    sessionRef.current?.then((s: any) => s.close?.());\n+    sessionRef.current = null;\n     activeSessionRef.current = null;\n   };\n \n-  /* -------------------- MAIN CONNECT -------------------- */\n   const handleConnect = async () => {\n     if (isConnected) {\n       cleanup();\n       return;\n@@ -71,13 +57,12 @@\n \n     setError(null);\n     setIsProcessing(true);\n \n-    // 1Ô∏è‚É£ MIC PERMISSION (ONLY mic error here)\n+    /* -------- MIC PERMISSION -------- */\n     try {\n       micStream.current = await navigator.mediaDevices.getUserMedia({\n         audio: {\n-          channelCount: 1,\n           echoCancellation: true,\n           noiseSuppression: true,\n           autoGainControl: true,\n         },\n@@ -88,9 +73,9 @@\n       return;\n     }\n \n     try {\n-      // 2Ô∏è‚É£ AUDIO CONTEXT (AFTER permission)\n+      /* -------- AUDIO CONTEXT -------- */\n       inputAudioCtx.current = new AudioContext({ sampleRate: 16000 });\n       outputAudioCtx.current = new AudioContext({ sampleRate: 24000 });\n \n       await inputAudioCtx.current.resume();\n@@ -109,43 +94,26 @@\n \n       source.connect(processorNode.current);\n       processorNode.current.connect(inputAudioCtx.current.destination);\n \n-      // 3Ô∏è‚É£ GEMINI CONNECT\n+      /* -------- GEMINI (PUBLIC STABLE MODEL) -------- */\n       const ai = new GoogleGenAI({\n         apiKey: import.meta.env.VITE_GEMINI_API_KEY,\n       });\n \n       const sessionPromise = ai.live.connect({\n-        model: 'gemini-2.5-flash-native-audio-preview-12-2025',\n+        // üî• FIXED MODEL (PUBLIC + STABLE)\n+        model: 'gemini-1.5-pro',\n         callbacks: {\n           onopen: () => {\n-            setError(null);              // üî• FIX: clear old error\n+            setError(null);\n             setIsConnected(true);\n             setIsProcessing(false);\n           },\n           onmessage: async (m: LiveServerMessage) => {\n-            if (m.serverContent?.outputTranscription)\n-              setCurrentOutput(p => p + m.serverContent.outputTranscription!.text);\n+            if (m.serverContent?.outputText)\n+              setCurrentOutput(p => p + m.serverContent.outputText);\n \n-            if (m.serverContent?.inputTranscription)\n-              setCurrentInput(p => p + m.serverContent.inputTranscription!.text);\n-\n-            const audio = m.serverContent?.modelTurn?.parts?.[0]?.inlineData?.data;\n-            if (audio && outputAudioCtx.current) {\n-              const ctx = outputAudioCtx.current;\n-              nextStartTime.current = Math.max(ctx.currentTime, nextStartTime.current);\n-\n-              const buf = await decodeAudioData(decode(audio), ctx, 24000, 1);\n-              const src = ctx.createBufferSource();\n-              src.buffer = buf;\n-              src.connect(ctx.destination);\n-              src.start(nextStartTime.current);\n-              nextStartTime.current += buf.duration;\n-              audioSources.current.add(src);\n-              src.onended = () => audioSources.current.delete(src);\n-            }\n-\n             if (m.serverContent?.turnComplete) {\n               setMessages(prev => [\n                 ...prev,\n                 { role: 'user', text: currentInput.trim() },\n@@ -163,41 +131,33 @@\n         },\n         config: {\n           systemInstruction: SYSTEM_INSTRUCTION,\n           safetySettings: SAFETY_SETTINGS,\n-          responseModalities: [Modality.AUDIO],\n-          inputAudioTranscription: {},\n-          outputAudioTranscription: {},\n-          speechConfig: {\n-            voiceConfig: {\n-              prebuiltVoiceConfig: { voiceName: 'Kore' },\n-            },\n-          },\n+          responseModalities: [Modality.TEXT],\n         },\n       });\n \n-      sessionPromiseRef.current = sessionPromise;\n+      sessionRef.current = sessionPromise;\n       sessionPromise.then((s: any) => (activeSessionRef.current = s));\n     } catch (err) {\n       console.error(err);\n       setError('Voice service is currently unavailable.');\n       cleanup();\n     }\n   };\n \n-  /* -------------------- UI -------------------- */\n   return (\n     <div className=\"flex flex-col h-screen bg-black text-white\">\n       <header className=\"p-6 flex justify-between items-center\">\n         <h1 className=\"text-2xl font-black\">\n           Hey <span className=\"text-rose-500 italic\">Buddy</span>\n         </h1>\n         <div className=\"flex gap-3\">\n           <span className=\"text-xs bg-rose-500/10 px-3 py-1 rounded-full\">\n-            <Zap className=\"inline w-3 h-3 mr-1\" /> Level {proLevel}%\n+            <Zap className=\"inline w-3 h-3 mr-1\" /> Level 5%\n           </span>\n           <span className=\"text-xs bg-indigo-500/10 px-3 py-1 rounded-full\">\n-            <Heart className=\"inline w-3 h-3 mr-1\" /> Bond {bondScore}%\n+            <Heart className=\"inline w-3 h-3 mr-1\" /> Bond 10%\n           </span>\n         </div>\n       </header>\n \n@@ -210,10 +170,8 @@\n               {m.text}\n             </div>\n           </div>\n         ))}\n-        {currentInput && <div className=\"italic opacity-50\">{currentInput}</div>}\n-        {currentOutput && <div className=\"opacity-70\">{currentOutput}</div>}\n         <div ref={transcriptEndRef} />\n       </main>\n \n       <footer className=\"p-8 flex flex-col items-center\">\n@@ -229,9 +187,9 @@\n         </p>\n         {error && <p className=\"text-red-400 text-xs mt-2\">{error}</p>}\n       </footer>\n \n-      {/* üé§ SOFT MIC GATE */}\n+      {/* MIC PRE-SCREEN */}\n       {showMicGate && (\n         <div className=\"fixed inset-0 bg-black/80 backdrop-blur-sm z-50 flex items-center justify-center\">\n           <div className=\"bg-[#0b0b0b] border border-white/10 rounded-3xl p-8 w-[90%] max-w-sm text-center space-y-6\">\n             <div className=\"text-5xl\">üé§</div>\n"
                },
                {
                    "date": 1768929095114,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,153 +1,115 @@\n-import React, { useState, useEffect, useRef } from 'react';\n-import { GoogleGenAI, Modality, LiveServerMessage } from '@google/genai';\n-import { SYSTEM_INSTRUCTION, SAFETY_SETTINGS } from './constants';\n-import { decode, decodeAudioData, createPcmBlob } from './services/audioUtils';\n-import { ChatMessage } from './types';\n-import { Mic, MicOff, Heart, Zap } from 'lucide-react';\n+import React, { useEffect, useRef, useState } from \"react\";\n+import { GoogleGenAI } from \"@google/genai\";\n+import { Mic, MicOff, Heart, Zap } from \"lucide-react\";\n \n+type ChatMessage = {\n+  role: \"user\" | \"model\";\n+  text: string;\n+};\n+\n const App: React.FC = () => {\n-  const [isConnected, setIsConnected] = useState(false);\n-  const [isProcessing, setIsProcessing] = useState(false);\n+  const [isListening, setIsListening] = useState(false);\n+  const [error, setError] = useState<string | null>(null);\n   const [messages, setMessages] = useState<ChatMessage[]>([]);\n-  const [currentInput, setCurrentInput] = useState('');\n-  const [currentOutput, setCurrentOutput] = useState('');\n-  const [error, setError] = useState<string | null>(null);\n \n-  const [showMicGate, setShowMicGate] = useState(false);\n+  const recognitionRef = useRef<any>(null);\n \n-  const inputAudioCtx = useRef<AudioContext | null>(null);\n-  const outputAudioCtx = useRef<AudioContext | null>(null);\n-  const micStream = useRef<MediaStream | null>(null);\n-  const processorNode = useRef<ScriptProcessorNode | null>(null);\n+  /* ---------------- Gemini ---------------- */\n+  const ai = new GoogleGenAI({\n+    apiKey: import.meta.env.VITE_GEMINI_API_KEY,\n+  });\n \n-  const sessionRef = useRef<any>(null);\n-  const activeSessionRef = useRef<any>(null);\n-\n-  const transcriptEndRef = useRef<HTMLDivElement>(null);\n-\n+  /* ---------------- INIT SPEECH RECOGNITION ---------------- */\n   useEffect(() => {\n-    transcriptEndRef.current?.scrollIntoView({ behavior: 'smooth' });\n-  }, [messages, currentInput, currentOutput]);\n+    const SpeechRecognition =\n+      (window as any).SpeechRecognition ||\n+      (window as any).webkitSpeechRecognition;\n \n-  const cleanup = () => {\n-    setIsConnected(false);\n-    setIsProcessing(false);\n+    if (!SpeechRecognition) {\n+      setError(\"Speech Recognition not supported in this browser.\");\n+      return;\n+    }\n \n-    processorNode.current?.disconnect();\n-    processorNode.current = null;\n+    const recognition = new SpeechRecognition();\n+    recognition.lang = \"en-US\";\n+    recognition.interimResults = false;\n+    recognition.continuous = false;\n \n-    micStream.current?.getTracks().forEach(t => t.stop());\n-    micStream.current = null;\n+    recognition.onresult = async (event: any) => {\n+      const userText = event.results[0][0].transcript;\n \n-    inputAudioCtx.current?.close();\n-    outputAudioCtx.current?.close();\n-    inputAudioCtx.current = null;\n-    outputAudioCtx.current = null;\n+      setMessages((prev) => [...prev, { role: \"user\", text: userText }]);\n \n-    sessionRef.current?.then((s: any) => s.close?.());\n-    sessionRef.current = null;\n-    activeSessionRef.current = null;\n-  };\n+      try {\n+        const model = ai.getGenerativeModel({\n+          model: \"gemini-1.5-flash\",\n+        });\n \n-  const handleConnect = async () => {\n-    if (isConnected) {\n-      cleanup();\n-      return;\n-    }\n+        const result = await model.generateContent(userText);\n+        const reply = result.response.text();\n \n-    setError(null);\n-    setIsProcessing(true);\n+        setMessages((prev) => [\n+          ...prev,\n+          { role: \"model\", text: reply },\n+        ]);\n \n-    /* -------- MIC PERMISSION -------- */\n-    try {\n-      micStream.current = await navigator.mediaDevices.getUserMedia({\n-        audio: {\n-          echoCancellation: true,\n-          noiseSuppression: true,\n-          autoGainControl: true,\n-        },\n-      });\n-    } catch {\n-      setError('Microphone access was denied.');\n-      setIsProcessing(false);\n-      return;\n-    }\n+        speak(reply);\n+      } catch (e) {\n+        setError(\"AI response failed.\");\n+      }\n+    };\n \n-    try {\n-      /* -------- AUDIO CONTEXT -------- */\n-      inputAudioCtx.current = new AudioContext({ sampleRate: 16000 });\n-      outputAudioCtx.current = new AudioContext({ sampleRate: 24000 });\n+    recognition.onerror = () => {\n+      setError(\"Microphone permission denied or unavailable.\");\n+      setIsListening(false);\n+    };\n \n-      await inputAudioCtx.current.resume();\n-      await outputAudioCtx.current.resume();\n+    recognition.onend = () => {\n+      setIsListening(false);\n+    };\n \n-      const source = inputAudioCtx.current.createMediaStreamSource(micStream.current);\n-      processorNode.current = inputAudioCtx.current.createScriptProcessor(256, 1, 1);\n+    recognitionRef.current = recognition;\n+  }, []);\n \n-      processorNode.current.onaudioprocess = e => {\n-        if (!activeSessionRef.current) return;\n-        const input = e.inputBuffer.getChannelData(0);\n-        activeSessionRef.current.sendRealtimeInput({\n-          media: createPcmBlob(input),\n-        });\n-      };\n+  /* ---------------- TEXT TO SPEECH ---------------- */\n+  const speak = (text: string) => {\n+    const utterance = new SpeechSynthesisUtterance(text);\n+    utterance.lang = \"en-US\";\n+    utterance.rate = 0.95;\n+    utterance.pitch = 1;\n \n-      source.connect(processorNode.current);\n-      processorNode.current.connect(inputAudioCtx.current.destination);\n+    window.speechSynthesis.cancel();\n+    window.speechSynthesis.speak(utterance);\n+  };\n \n-      /* -------- GEMINI (PUBLIC STABLE MODEL) -------- */\n-      const ai = new GoogleGenAI({\n-        apiKey: import.meta.env.VITE_GEMINI_API_KEY,\n-      });\n+  /* ---------------- BUTTON HANDLER ---------------- */\n+  const handleConnect = () => {\n+    setError(null);\n \n-      const sessionPromise = ai.live.connect({\n-        // üî• FIXED MODEL (PUBLIC + STABLE)\n-        model: 'gemini-1.5-pro',\n-        callbacks: {\n-          onopen: () => {\n-            setError(null);\n-            setIsConnected(true);\n-            setIsProcessing(false);\n-          },\n-          onmessage: async (m: LiveServerMessage) => {\n-            if (m.serverContent?.outputText)\n-              setCurrentOutput(p => p + m.serverContent.outputText);\n+    if (!recognitionRef.current) {\n+      setError(\"Speech Recognition not available.\");\n+      return;\n+    }\n \n-            if (m.serverContent?.turnComplete) {\n-              setMessages(prev => [\n-                ...prev,\n-                { role: 'user', text: currentInput.trim() },\n-                { role: 'model', text: currentOutput.trim() },\n-              ]);\n-              setCurrentInput('');\n-              setCurrentOutput('');\n-            }\n-          },\n-          onclose: cleanup,\n-          onerror: () => {\n-            setError('Voice service is currently unavailable.');\n-            cleanup();\n-          },\n-        },\n-        config: {\n-          systemInstruction: SYSTEM_INSTRUCTION,\n-          safetySettings: SAFETY_SETTINGS,\n-          responseModalities: [Modality.TEXT],\n-        },\n-      });\n+    if (isListening) {\n+      recognitionRef.current.stop();\n+      setIsListening(false);\n+      return;\n+    }\n \n-      sessionRef.current = sessionPromise;\n-      sessionPromise.then((s: any) => (activeSessionRef.current = s));\n-    } catch (err) {\n-      console.error(err);\n-      setError('Voice service is currently unavailable.');\n-      cleanup();\n+    try {\n+      recognitionRef.current.start();\n+      setIsListening(true);\n+    } catch {\n+      setError(\"Unable to start microphone.\");\n     }\n   };\n \n+  /* ---------------- UI ---------------- */\n   return (\n     <div className=\"flex flex-col h-screen bg-black text-white\">\n+      {/* Header */}\n       <header className=\"p-6 flex justify-between items-center\">\n         <h1 className=\"text-2xl font-black\">\n           Hey <span className=\"text-rose-500 italic\">Buddy</span>\n         </h1>\n@@ -160,64 +122,49 @@\n           </span>\n         </div>\n       </header>\n \n+      {/* Chat */}\n       <main className=\"flex-1 overflow-y-auto px-6 space-y-6\">\n         {messages.map((m, i) => (\n-          <div key={i} className={`flex ${m.role === 'user' ? 'justify-end' : 'justify-start'}`}>\n-            <div className={`max-w-[80%] p-5 rounded-3xl ${\n-              m.role === 'user' ? 'bg-rose-600' : 'bg-white/10'\n-            }`}>\n+          <div\n+            key={i}\n+            className={`flex ${\n+              m.role === \"user\" ? \"justify-end\" : \"justify-start\"\n+            }`}\n+          >\n+            <div\n+              className={`max-w-[80%] p-5 rounded-3xl ${\n+                m.role === \"user\"\n+                  ? \"bg-rose-600\"\n+                  : \"bg-white/10\"\n+              }`}\n+            >\n               {m.text}\n             </div>\n           </div>\n         ))}\n-        <div ref={transcriptEndRef} />\n       </main>\n \n+      {/* Footer */}\n       <footer className=\"p-8 flex flex-col items-center\">\n         <button\n-          onClick={() => setShowMicGate(true)}\n-          disabled={isProcessing}\n-          className=\"w-24 h-24 rounded-full bg-rose-600 flex items-center justify-center\"\n+          onClick={handleConnect}\n+          className={`w-24 h-24 rounded-full flex items-center justify-center transition-all ${\n+            isListening ? \"bg-neutral-800\" : \"bg-rose-600\"\n+          }`}\n         >\n-          {isConnected ? <MicOff size={40} /> : <Mic size={40} />}\n+          {isListening ? <MicOff size={40} /> : <Mic size={40} />}\n         </button>\n+\n         <p className=\"mt-4 text-xs uppercase tracking-widest\">\n-          {isConnected ? 'Listening...' : 'Connect with Buddy'}\n+          {isListening ? \"Listening...\" : \"Connect with Buddy\"}\n         </p>\n-        {error && <p className=\"text-red-400 text-xs mt-2\">{error}</p>}\n+\n+        {error && (\n+          <p className=\"text-red-400 text-xs mt-2\">{error}</p>\n+        )}\n       </footer>\n-\n-      {/* MIC PRE-SCREEN */}\n-      {showMicGate && (\n-        <div className=\"fixed inset-0 bg-black/80 backdrop-blur-sm z-50 flex items-center justify-center\">\n-          <div className=\"bg-[#0b0b0b] border border-white/10 rounded-3xl p-8 w-[90%] max-w-sm text-center space-y-6\">\n-            <div className=\"text-5xl\">üé§</div>\n-            <h2 className=\"text-xl font-black\">Microphone Access Needed</h2>\n-            <p className=\"text-sm text-neutral-400\">\n-              Your browser will ask for microphone permission on the next step.\n-            </p>\n-            <div className=\"flex gap-4 justify-center\">\n-              <button\n-                onClick={() => setShowMicGate(false)}\n-                className=\"px-5 py-2 rounded-full bg-white/5\"\n-              >\n-                Cancel\n-              </button>\n-              <button\n-                onClick={() => {\n-                  setShowMicGate(false);\n-                  handleConnect();\n-                }}\n-                className=\"px-6 py-2 rounded-full bg-rose-600 font-bold\"\n-              >\n-                Continue\n-              </button>\n-            </div>\n-          </div>\n-        </div>\n-      )}\n     </div>\n   );\n };\n \n"
                },
                {
                    "date": 1768929464574,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,23 +1,32 @@\n import React, { useEffect, useRef, useState } from \"react\";\n import { GoogleGenAI } from \"@google/genai\";\n import { Mic, MicOff, Heart, Zap } from \"lucide-react\";\n \n+/* ---------------- TYPES ---------------- */\n type ChatMessage = {\n   role: \"user\" | \"model\";\n   text: string;\n };\n \n+/* ---------------- API KEY SAFE GUARD ---------------- */\n+const API_KEY = import.meta.env.VITE_GEMINI_API_KEY;\n+\n+if (!API_KEY) {\n+  console.error(\"‚ùå VITE_GEMINI_API_KEY is missing\");\n+}\n+\n+/* ---------------- APP ---------------- */\n const App: React.FC = () => {\n   const [isListening, setIsListening] = useState(false);\n   const [error, setError] = useState<string | null>(null);\n   const [messages, setMessages] = useState<ChatMessage[]>([]);\n \n   const recognitionRef = useRef<any>(null);\n \n-  /* ---------------- Gemini ---------------- */\n+  /* ---------------- INIT GEMINI ---------------- */\n   const ai = new GoogleGenAI({\n-    apiKey: import.meta.env.VITE_GEMINI_API_KEY,\n+    apiKey: API_KEY,\n   });\n \n   /* ---------------- INIT SPEECH RECOGNITION ---------------- */\n   useEffect(() => {\n@@ -25,16 +34,16 @@\n       (window as any).SpeechRecognition ||\n       (window as any).webkitSpeechRecognition;\n \n     if (!SpeechRecognition) {\n-      setError(\"Speech Recognition not supported in this browser.\");\n+      setError(\"Speech recognition is not supported in this browser.\");\n       return;\n     }\n \n     const recognition = new SpeechRecognition();\n     recognition.lang = \"en-US\";\n+    recognition.continuous = false;\n     recognition.interimResults = false;\n-    recognition.continuous = false;\n \n     recognition.onresult = async (event: any) => {\n       const userText = event.results[0][0].transcript;\n \n@@ -53,9 +62,10 @@\n           { role: \"model\", text: reply },\n         ]);\n \n         speak(reply);\n-      } catch (e) {\n+      } catch (err) {\n+        console.error(err);\n         setError(\"AI response failed.\");\n       }\n     };\n \n@@ -81,14 +91,19 @@\n     window.speechSynthesis.cancel();\n     window.speechSynthesis.speak(utterance);\n   };\n \n-  /* ---------------- BUTTON HANDLER ---------------- */\n+  /* ---------------- MIC BUTTON ---------------- */\n   const handleConnect = () => {\n     setError(null);\n \n+    if (!API_KEY) {\n+      setError(\"API key is missing. Please check .env.local\");\n+      return;\n+    }\n+\n     if (!recognitionRef.current) {\n-      setError(\"Speech Recognition not available.\");\n+      setError(\"Speech recognition not available.\");\n       return;\n     }\n \n     if (isListening) {\n@@ -100,9 +115,9 @@\n     try {\n       recognitionRef.current.start();\n       setIsListening(true);\n     } catch {\n-      setError(\"Unable to start microphone.\");\n+      setError(\"Unable to access microphone.\");\n     }\n   };\n \n   /* ---------------- UI ---------------- */\n@@ -160,9 +175,11 @@\n           {isListening ? \"Listening...\" : \"Connect with Buddy\"}\n         </p>\n \n         {error && (\n-          <p className=\"text-red-400 text-xs mt-2\">{error}</p>\n+          <p className=\"text-red-400 text-xs mt-2 text-center\">\n+            {error}\n+          </p>\n         )}\n       </footer>\n     </div>\n   );\n"
                },
                {
                    "date": 1768929761819,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,131 +1,87 @@\n import React, { useEffect, useRef, useState } from \"react\";\n-import { GoogleGenAI } from \"@google/genai\";\n import { Mic, MicOff, Heart, Zap } from \"lucide-react\";\n \n-/* ---------------- TYPES ---------------- */\n type ChatMessage = {\n   role: \"user\" | \"model\";\n   text: string;\n };\n \n-/* ---------------- API KEY SAFE GUARD ---------------- */\n-const API_KEY = import.meta.env.VITE_GEMINI_API_KEY;\n-\n-if (!API_KEY) {\n-  console.error(\"‚ùå VITE_GEMINI_API_KEY is missing\");\n-}\n-\n-/* ---------------- APP ---------------- */\n const App: React.FC = () => {\n   const [isListening, setIsListening] = useState(false);\n   const [error, setError] = useState<string | null>(null);\n   const [messages, setMessages] = useState<ChatMessage[]>([]);\n \n   const recognitionRef = useRef<any>(null);\n \n-  /* ---------------- INIT GEMINI ---------------- */\n-  const ai = new GoogleGenAI({\n-    apiKey: API_KEY,\n-  });\n-\n-  /* ---------------- INIT SPEECH RECOGNITION ---------------- */\n   useEffect(() => {\n     const SpeechRecognition =\n       (window as any).SpeechRecognition ||\n       (window as any).webkitSpeechRecognition;\n \n     if (!SpeechRecognition) {\n-      setError(\"Speech recognition is not supported in this browser.\");\n+      setError(\"Speech recognition not supported.\");\n       return;\n     }\n \n     const recognition = new SpeechRecognition();\n     recognition.lang = \"en-US\";\n     recognition.continuous = false;\n-    recognition.interimResults = false;\n \n     recognition.onresult = async (event: any) => {\n       const userText = event.results[0][0].transcript;\n+      setMessages((p) => [...p, { role: \"user\", text: userText }]);\n \n-      setMessages((prev) => [...prev, { role: \"user\", text: userText }]);\n-\n       try {\n-        const model = ai.getGenerativeModel({\n-          model: \"gemini-1.5-flash\",\n+        const res = await fetch(\"http://localhost:3001/api/chat\", {\n+          method: \"POST\",\n+          headers: { \"Content-Type\": \"application/json\" },\n+          body: JSON.stringify({ text: userText }),\n         });\n \n-        const result = await model.generateContent(userText);\n-        const reply = result.response.text();\n+        const data = await res.json();\n+        const reply = data.reply;\n \n-        setMessages((prev) => [\n-          ...prev,\n-          { role: \"model\", text: reply },\n-        ]);\n-\n+        setMessages((p) => [...p, { role: \"model\", text: reply }]);\n         speak(reply);\n-      } catch (err) {\n-        console.error(err);\n+      } catch {\n         setError(\"AI response failed.\");\n       }\n     };\n \n     recognition.onerror = () => {\n-      setError(\"Microphone permission denied or unavailable.\");\n+      setError(\"Mic permission issue.\");\n       setIsListening(false);\n     };\n \n-    recognition.onend = () => {\n-      setIsListening(false);\n-    };\n+    recognition.onend = () => setIsListening(false);\n \n     recognitionRef.current = recognition;\n   }, []);\n \n-  /* ---------------- TEXT TO SPEECH ---------------- */\n   const speak = (text: string) => {\n-    const utterance = new SpeechSynthesisUtterance(text);\n-    utterance.lang = \"en-US\";\n-    utterance.rate = 0.95;\n-    utterance.pitch = 1;\n-\n-    window.speechSynthesis.cancel();\n-    window.speechSynthesis.speak(utterance);\n+    const u = new SpeechSynthesisUtterance(text);\n+    u.lang = \"en-US\";\n+    speechSynthesis.cancel();\n+    speechSynthesis.speak(u);\n   };\n \n-  /* ---------------- MIC BUTTON ---------------- */\n   const handleConnect = () => {\n     setError(null);\n+    if (!recognitionRef.current) return;\n \n-    if (!API_KEY) {\n-      setError(\"API key is missing. Please check .env.local\");\n-      return;\n-    }\n-\n-    if (!recognitionRef.current) {\n-      setError(\"Speech recognition not available.\");\n-      return;\n-    }\n-\n     if (isListening) {\n       recognitionRef.current.stop();\n-      setIsListening(false);\n       return;\n     }\n \n-    try {\n-      recognitionRef.current.start();\n-      setIsListening(true);\n-    } catch {\n-      setError(\"Unable to access microphone.\");\n-    }\n+    recognitionRef.current.start();\n+    setIsListening(true);\n   };\n \n-  /* ---------------- UI ---------------- */\n   return (\n     <div className=\"flex flex-col h-screen bg-black text-white\">\n-      {/* Header */}\n-      <header className=\"p-6 flex justify-between items-center\">\n+      <header className=\"p-6 flex justify-between\">\n         <h1 className=\"text-2xl font-black\">\n           Hey <span className=\"text-rose-500 italic\">Buddy</span>\n         </h1>\n         <div className=\"flex gap-3\">\n@@ -137,19 +93,18 @@\n           </span>\n         </div>\n       </header>\n \n-      {/* Chat */}\n-      <main className=\"flex-1 overflow-y-auto px-6 space-y-6\">\n+      <main className=\"flex-1 px-6 space-y-4 overflow-y-auto\">\n         {messages.map((m, i) => (\n           <div\n             key={i}\n             className={`flex ${\n               m.role === \"user\" ? \"justify-end\" : \"justify-start\"\n             }`}\n           >\n             <div\n-              className={`max-w-[80%] p-5 rounded-3xl ${\n+              className={`p-4 rounded-3xl max-w-[75%] ${\n                 m.role === \"user\"\n                   ? \"bg-rose-600\"\n                   : \"bg-white/10\"\n               }`}\n@@ -159,28 +114,21 @@\n           </div>\n         ))}\n       </main>\n \n-      {/* Footer */}\n       <footer className=\"p-8 flex flex-col items-center\">\n         <button\n           onClick={handleConnect}\n-          className={`w-24 h-24 rounded-full flex items-center justify-center transition-all ${\n+          className={`w-24 h-24 rounded-full flex items-center justify-center ${\n             isListening ? \"bg-neutral-800\" : \"bg-rose-600\"\n           }`}\n         >\n           {isListening ? <MicOff size={40} /> : <Mic size={40} />}\n         </button>\n-\n-        <p className=\"mt-4 text-xs uppercase tracking-widest\">\n+        <p className=\"mt-4 text-xs uppercase\">\n           {isListening ? \"Listening...\" : \"Connect with Buddy\"}\n         </p>\n-\n-        {error && (\n-          <p className=\"text-red-400 text-xs mt-2 text-center\">\n-            {error}\n-          </p>\n-        )}\n+        {error && <p className=\"text-red-400 text-xs mt-2\">{error}</p>}\n       </footer>\n     </div>\n   );\n };\n"
                },
                {
                    "date": 1768931850829,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,134 +1,87 @@\n-import React, { useEffect, useRef, useState } from \"react\";\n-import { Mic, MicOff, Heart, Zap } from \"lucide-react\";\n+import React, { useState } from \"react\";\n \n-type ChatMessage = {\n+interface Message {\n   role: \"user\" | \"model\";\n   text: string;\n-};\n+}\n \n const App: React.FC = () => {\n-  const [isListening, setIsListening] = useState(false);\n+  const [messages, setMessages] = useState<Message[]>([]);\n+  const [input, setInput] = useState(\"\");\n+  const [loading, setLoading] = useState(false);\n   const [error, setError] = useState<string | null>(null);\n-  const [messages, setMessages] = useState<ChatMessage[]>([]);\n \n-  const recognitionRef = useRef<any>(null);\n+  const sendMessage = async () => {\n+    if (!input.trim()) return;\n \n-  useEffect(() => {\n-    const SpeechRecognition =\n-      (window as any).SpeechRecognition ||\n-      (window as any).webkitSpeechRecognition;\n+    setLoading(true);\n+    setError(null);\n \n-    if (!SpeechRecognition) {\n-      setError(\"Speech recognition not supported.\");\n-      return;\n-    }\n+    const userMsg: Message = { role: \"user\", text: input };\n+    setMessages(prev => [...prev, userMsg]);\n \n-    const recognition = new SpeechRecognition();\n-    recognition.lang = \"en-US\";\n-    recognition.continuous = false;\n+    try {\n+      const res = await fetch(\"http://localhost:3001/chat\", {\n+        method: \"POST\",\n+        headers: { \"Content-Type\": \"application/json\" },\n+        body: JSON.stringify({ text: input }),\n+      });\n \n-    recognition.onresult = async (event: any) => {\n-      const userText = event.results[0][0].transcript;\n-      setMessages((p) => [...p, { role: \"user\", text: userText }]);\n-\n-      try {\n-        const res = await fetch(\"http://localhost:3001/api/chat\", {\n-          method: \"POST\",\n-          headers: { \"Content-Type\": \"application/json\" },\n-          body: JSON.stringify({ text: userText }),\n-        });\n-\n-        const data = await res.json();\n-        const reply = data.reply;\n-\n-        setMessages((p) => [...p, { role: \"model\", text: reply }]);\n-        speak(reply);\n-      } catch {\n-        setError(\"AI response failed.\");\n+      if (!res.ok) {\n+        throw new Error(\"Backend error\");\n       }\n-    };\n \n-    recognition.onerror = () => {\n-      setError(\"Mic permission issue.\");\n-      setIsListening(false);\n-    };\n+      const data = await res.json();\n \n-    recognition.onend = () => setIsListening(false);\n-\n-    recognitionRef.current = recognition;\n-  }, []);\n-\n-  const speak = (text: string) => {\n-    const u = new SpeechSynthesisUtterance(text);\n-    u.lang = \"en-US\";\n-    speechSynthesis.cancel();\n-    speechSynthesis.speak(u);\n-  };\n-\n-  const handleConnect = () => {\n-    setError(null);\n-    if (!recognitionRef.current) return;\n-\n-    if (isListening) {\n-      recognitionRef.current.stop();\n-      return;\n+      setMessages(prev => [\n+        ...prev,\n+        { role: \"model\", text: data.reply },\n+      ]);\n+    } catch (err) {\n+      setError(\"AI response failed\");\n+    } finally {\n+      setLoading(false);\n+      setInput(\"\");\n     }\n-\n-    recognitionRef.current.start();\n-    setIsListening(true);\n   };\n \n   return (\n-    <div className=\"flex flex-col h-screen bg-black text-white\">\n-      <header className=\"p-6 flex justify-between\">\n-        <h1 className=\"text-2xl font-black\">\n-          Hey <span className=\"text-rose-500 italic\">Buddy</span>\n-        </h1>\n-        <div className=\"flex gap-3\">\n-          <span className=\"text-xs bg-rose-500/10 px-3 py-1 rounded-full\">\n-            <Zap className=\"inline w-3 h-3 mr-1\" /> Level 5%\n-          </span>\n-          <span className=\"text-xs bg-indigo-500/10 px-3 py-1 rounded-full\">\n-            <Heart className=\"inline w-3 h-3 mr-1\" /> Bond 10%\n-          </span>\n-        </div>\n+    <div className=\"min-h-screen bg-black text-white flex flex-col\">\n+      <header className=\"p-6 text-2xl font-bold\">\n+        Hey <span className=\"text-rose-500\">Buddy</span>\n       </header>\n \n-      <main className=\"flex-1 px-6 space-y-4 overflow-y-auto\">\n+      <main className=\"flex-1 p-6 space-y-4\">\n         {messages.map((m, i) => (\n           <div\n             key={i}\n-            className={`flex ${\n-              m.role === \"user\" ? \"justify-end\" : \"justify-start\"\n+            className={`max-w-xl p-4 rounded-2xl ${\n+              m.role === \"user\"\n+                ? \"bg-rose-600 ml-auto\"\n+                : \"bg-white/10\"\n             }`}\n           >\n-            <div\n-              className={`p-4 rounded-3xl max-w-[75%] ${\n-                m.role === \"user\"\n-                  ? \"bg-rose-600\"\n-                  : \"bg-white/10\"\n-              }`}\n-            >\n-              {m.text}\n-            </div>\n+            {m.text}\n           </div>\n         ))}\n+        {error && <p className=\"text-red-400\">{error}</p>}\n       </main>\n \n-      <footer className=\"p-8 flex flex-col items-center\">\n+      <footer className=\"p-6 flex gap-3\">\n+        <input\n+          className=\"flex-1 p-3 rounded-xl text-black\"\n+          value={input}\n+          onChange={e => setInput(e.target.value)}\n+          placeholder=\"Type something...\"\n+        />\n         <button\n-          onClick={handleConnect}\n-          className={`w-24 h-24 rounded-full flex items-center justify-center ${\n-            isListening ? \"bg-neutral-800\" : \"bg-rose-600\"\n-          }`}\n+          onClick={sendMessage}\n+          disabled={loading}\n+          className=\"px-6 py-3 bg-rose-600 rounded-xl\"\n         >\n-          {isListening ? <MicOff size={40} /> : <Mic size={40} />}\n+          {loading ? \"...\" : \"Send\"}\n         </button>\n-        <p className=\"mt-4 text-xs uppercase\">\n-          {isListening ? \"Listening...\" : \"Connect with Buddy\"}\n-        </p>\n-        {error && <p className=\"text-red-400 text-xs mt-2\">{error}</p>}\n       </footer>\n     </div>\n   );\n };\n"
                },
                {
                    "date": 1768932320592,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,89 +1,132 @@\n-import React, { useState } from \"react\";\n+import { useState, useRef } from \"react\";\n \n-interface Message {\n-  role: \"user\" | \"model\";\n-  text: string;\n-}\n+const BACKEND_URL = \"http://localhost:3001/chat\";\n \n-const App: React.FC = () => {\n-  const [messages, setMessages] = useState<Message[]>([]);\n-  const [input, setInput] = useState(\"\");\n-  const [loading, setLoading] = useState(false);\n-  const [error, setError] = useState<string | null>(null);\n+export default function App() {\n+  const [status, setStatus] = useState<\n+    \"idle\" | \"requesting\" | \"listening\" | \"thinking\" | \"error\"\n+  >(\"idle\");\n+  const [message, setMessage] = useState(\"\");\n+  const mediaRecorderRef = useRef<MediaRecorder | null>(null);\n+  const audioChunksRef = useRef<Blob[]>([]);\n \n-  const sendMessage = async () => {\n-    if (!input.trim()) return;\n+  const requestMicAndStart = async () => {\n+    try {\n+      setStatus(\"requesting\");\n \n-    setLoading(true);\n-    setError(null);\n+      const stream = await navigator.mediaDevices.getUserMedia({\n+        audio: true,\n+      });\n \n-    const userMsg: Message = { role: \"user\", text: input };\n-    setMessages(prev => [...prev, userMsg]);\n+      const mediaRecorder = new MediaRecorder(stream);\n+      mediaRecorderRef.current = mediaRecorder;\n+      audioChunksRef.current = [];\n \n+      mediaRecorder.ondataavailable = (e) => {\n+        if (e.data.size > 0) audioChunksRef.current.push(e.data);\n+      };\n+\n+      mediaRecorder.onstop = sendAudioToBackend;\n+\n+      mediaRecorder.start();\n+      setStatus(\"listening\");\n+\n+      // auto stop after 4 seconds\n+      setTimeout(() => {\n+        mediaRecorder.stop();\n+        stream.getTracks().forEach((t) => t.stop());\n+        setStatus(\"thinking\");\n+      }, 4000);\n+    } catch (err) {\n+      console.error(err);\n+      setStatus(\"error\");\n+      setMessage(\"Microphone permission denied.\");\n+    }\n+  };\n+\n+  const sendAudioToBackend = async () => {\n     try {\n-      const res = await fetch(\"http://localhost:3001/chat\", {\n+      const audioBlob = new Blob(audioChunksRef.current, {\n+        type: \"audio/webm\",\n+      });\n+\n+      const formData = new FormData();\n+      formData.append(\"audio\", audioBlob);\n+\n+      const res = await fetch(BACKEND_URL, {\n         method: \"POST\",\n-        headers: { \"Content-Type\": \"application/json\" },\n-        body: JSON.stringify({ text: input }),\n+        body: formData,\n       });\n \n-      if (!res.ok) {\n-        throw new Error(\"Backend error\");\n-      }\n+      if (!res.ok) throw new Error(\"Backend error\");\n \n       const data = await res.json();\n-\n-      setMessages(prev => [\n-        ...prev,\n-        { role: \"model\", text: data.reply },\n-      ]);\n+      setMessage(data.reply || \"No response\");\n+      setStatus(\"idle\");\n     } catch (err) {\n-      setError(\"AI response failed\");\n-    } finally {\n-      setLoading(false);\n-      setInput(\"\");\n+      console.error(err);\n+      setStatus(\"error\");\n+      setMessage(\"AI response failed.\");\n     }\n   };\n \n   return (\n-    <div className=\"min-h-screen bg-black text-white flex flex-col\">\n-      <header className=\"p-6 text-2xl font-bold\">\n-        Hey <span className=\"text-rose-500\">Buddy</span>\n-      </header>\n+    <div\n+      style={{\n+        background: \"#000\",\n+        color: \"#fff\",\n+        minHeight: \"100vh\",\n+        display: \"flex\",\n+        flexDirection: \"column\",\n+        alignItems: \"center\",\n+        justifyContent: \"center\",\n+        gap: 20,\n+        fontFamily: \"sans-serif\",\n+      }}\n+    >\n+      <h1>\n+        Hey <span style={{ color: \"#ff2d55\" }}>Buddy</span>\n+      </h1>\n \n-      <main className=\"flex-1 p-6 space-y-4\">\n-        {messages.map((m, i) => (\n-          <div\n-            key={i}\n-            className={`max-w-xl p-4 rounded-2xl ${\n-              m.role === \"user\"\n-                ? \"bg-rose-600 ml-auto\"\n-                : \"bg-white/10\"\n-            }`}\n-          >\n-            {m.text}\n-          </div>\n-        ))}\n-        {error && <p className=\"text-red-400\">{error}</p>}\n-      </main>\n+      <button\n+        onClick={requestMicAndStart}\n+        disabled={status === \"listening\" || status === \"thinking\"}\n+        style={{\n+          width: 120,\n+          height: 120,\n+          borderRadius: \"50%\",\n+          border: \"none\",\n+          background: \"#ff2d55\",\n+          color: \"#fff\",\n+          fontSize: 30,\n+          cursor: \"pointer\",\n+        }}\n+      >\n+        üé§\n+      </button>\n \n-      <footer className=\"p-6 flex gap-3\">\n-        <input\n-          className=\"flex-1 p-3 rounded-xl text-black\"\n-          value={input}\n-          onChange={e => setInput(e.target.value)}\n-          placeholder=\"Type something...\"\n-        />\n-        <button\n-          onClick={sendMessage}\n-          disabled={loading}\n-          className=\"px-6 py-3 bg-rose-600 rounded-xl\"\n+      {status === \"idle\" && <p>CONNECT WITH BUDDY</p>}\n+      {status === \"requesting\" && <p>Requesting microphone‚Ä¶</p>}\n+      {status === \"listening\" && <p>Listening‚Ä¶</p>}\n+      {status === \"thinking\" && <p>Thinking‚Ä¶</p>}\n+      {status === \"error\" && (\n+        <p style={{ color: \"red\" }}>{message}</p>\n+      )}\n+\n+      {message && status === \"idle\" && (\n+        <div\n+          style={{\n+            marginTop: 20,\n+            padding: 16,\n+            background: \"#111\",\n+            borderRadius: 8,\n+            maxWidth: 400,\n+            textAlign: \"center\",\n+          }}\n         >\n-          {loading ? \"...\" : \"Send\"}\n-        </button>\n-      </footer>\n+          ü§ñ {message}\n+        </div>\n+      )}\n     </div>\n   );\n-};\n-\n-export default App;\n+}\n"
                },
                {
                    "date": 1768933004000,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,132 +1,73 @@\n-import { useState, useRef } from \"react\";\n+import { useState } from \"react\";\n \n-const BACKEND_URL = \"http://localhost:3001/chat\";\n+const BACKEND_URL = \"http://localhost:3001\";\n \n export default function App() {\n-  const [status, setStatus] = useState<\n-    \"idle\" | \"requesting\" | \"listening\" | \"thinking\" | \"error\"\n-  >(\"idle\");\n-  const [message, setMessage] = useState(\"\");\n-  const mediaRecorderRef = useRef<MediaRecorder | null>(null);\n-  const audioChunksRef = useRef<Blob[]>([]);\n+  const [input, setInput] = useState(\"\");\n+  const [reply, setReply] = useState(\"\");\n+  const [error, setError] = useState(\"\");\n+  const [loading, setLoading] = useState(false);\n \n-  const requestMicAndStart = async () => {\n-    try {\n-      setStatus(\"requesting\");\n+  const sendMessage = async () => {\n+    setError(\"\");\n+    setReply(\"\");\n+    setLoading(true);\n \n-      const stream = await navigator.mediaDevices.getUserMedia({\n-        audio: true,\n-      });\n-\n-      const mediaRecorder = new MediaRecorder(stream);\n-      mediaRecorderRef.current = mediaRecorder;\n-      audioChunksRef.current = [];\n-\n-      mediaRecorder.ondataavailable = (e) => {\n-        if (e.data.size > 0) audioChunksRef.current.push(e.data);\n-      };\n-\n-      mediaRecorder.onstop = sendAudioToBackend;\n-\n-      mediaRecorder.start();\n-      setStatus(\"listening\");\n-\n-      // auto stop after 4 seconds\n-      setTimeout(() => {\n-        mediaRecorder.stop();\n-        stream.getTracks().forEach((t) => t.stop());\n-        setStatus(\"thinking\");\n-      }, 4000);\n-    } catch (err) {\n-      console.error(err);\n-      setStatus(\"error\");\n-      setMessage(\"Microphone permission denied.\");\n-    }\n-  };\n-\n-  const sendAudioToBackend = async () => {\n     try {\n-      const audioBlob = new Blob(audioChunksRef.current, {\n-        type: \"audio/webm\",\n+      const res = await fetch(`${BACKEND_URL}/chat`, {\n+        method: \"POST\",\n+        headers: {\n+          \"Content-Type\": \"application/json\",\n+        },\n+        body: JSON.stringify({\n+          text: input,\n+        }),\n       });\n \n-      const formData = new FormData();\n-      formData.append(\"audio\", audioBlob);\n+      const data = await res.json();\n \n-      const res = await fetch(BACKEND_URL, {\n-        method: \"POST\",\n-        body: formData,\n-      });\n+      if (!res.ok) {\n+        throw new Error(data.error || \"Request failed\");\n+      }\n \n-      if (!res.ok) throw new Error(\"Backend error\");\n-\n-      const data = await res.json();\n-      setMessage(data.reply || \"No response\");\n-      setStatus(\"idle\");\n-    } catch (err) {\n-      console.error(err);\n-      setStatus(\"error\");\n-      setMessage(\"AI response failed.\");\n+      setReply(data.reply);\n+    } catch (err: any) {\n+      setError(err.message || \"AI response failed\");\n+    } finally {\n+      setLoading(false);\n     }\n   };\n \n   return (\n-    <div\n-      style={{\n-        background: \"#000\",\n-        color: \"#fff\",\n-        minHeight: \"100vh\",\n-        display: \"flex\",\n-        flexDirection: \"column\",\n-        alignItems: \"center\",\n-        justifyContent: \"center\",\n-        gap: 20,\n-        fontFamily: \"sans-serif\",\n-      }}\n-    >\n-      <h1>\n-        Hey <span style={{ color: \"#ff2d55\" }}>Buddy</span>\n+    <div className=\"min-h-screen bg-black text-white flex flex-col items-center justify-center gap-6 p-6\">\n+      <h1 className=\"text-3xl font-bold\">\n+        Hey <span className=\"text-rose-500\">Buddy</span>\n       </h1>\n \n+      <textarea\n+        className=\"w-full max-w-md p-3 rounded bg-neutral-900 text-white outline-none\"\n+        rows={3}\n+        placeholder=\"Type something...\"\n+        value={input}\n+        onChange={(e) => setInput(e.target.value)}\n+      />\n+\n       <button\n-        onClick={requestMicAndStart}\n-        disabled={status === \"listening\" || status === \"thinking\"}\n-        style={{\n-          width: 120,\n-          height: 120,\n-          borderRadius: \"50%\",\n-          border: \"none\",\n-          background: \"#ff2d55\",\n-          color: \"#fff\",\n-          fontSize: 30,\n-          cursor: \"pointer\",\n-        }}\n+        onClick={sendMessage}\n+        disabled={loading || !input.trim()}\n+        className=\"px-6 py-3 rounded-full bg-rose-600 hover:bg-rose-700 disabled:opacity-50\"\n       >\n-        üé§\n+        {loading ? \"Thinking...\" : \"Send\"}\n       </button>\n \n-      {status === \"idle\" && <p>CONNECT WITH BUDDY</p>}\n-      {status === \"requesting\" && <p>Requesting microphone‚Ä¶</p>}\n-      {status === \"listening\" && <p>Listening‚Ä¶</p>}\n-      {status === \"thinking\" && <p>Thinking‚Ä¶</p>}\n-      {status === \"error\" && (\n-        <p style={{ color: \"red\" }}>{message}</p>\n+      {reply && (\n+        <div className=\"max-w-md bg-white/10 p-4 rounded-xl\">\n+          <strong>Buddy:</strong>\n+          <p className=\"mt-2\">{reply}</p>\n+        </div>\n       )}\n \n-      {message && status === \"idle\" && (\n-        <div\n-          style={{\n-            marginTop: 20,\n-            padding: 16,\n-            background: \"#111\",\n-            borderRadius: 8,\n-            maxWidth: 400,\n-            textAlign: \"center\",\n-          }}\n-        >\n-          ü§ñ {message}\n-        </div>\n-      )}\n+      {error && <p className=\"text-red-500\">{error}</p>}\n     </div>\n   );\n }\n"
                }
            ],
            "date": 1768924820751,
            "name": "Commit-0",
            "content": "import React, { useState, useEffect, useRef, useMemo } from 'react';\nimport { GoogleGenAI, Modality, LiveServerMessage } from '@google/genai';\nimport { SYSTEM_INSTRUCTION, SAFETY_SETTINGS } from './constants';\nimport { decode, decodeAudioData, createPcmBlob } from './services/audioUtils';\nimport { ChatMessage } from './types';\nimport {\n  Mic,\n  MicOff,\n  Heart,\n  Menu,\n  X,\n  Plus,\n  Trash2,\n  User,\n  Sparkles,\n  Zap\n} from 'lucide-react';\n\n/* -------------------- TYPES -------------------- */\ninterface SavedChat {\n  id: string;\n  timestamp: number;\n  preview: string;\n  messages: ChatMessage[];\n  stats?: { bond: number; level: number; mood: string };\n}\n\n/* -------------------- APP -------------------- */\nconst App: React.FC = () => {\n  const [isConnected, setIsConnected] = useState(false);\n  const [isProcessing, setIsProcessing] = useState(false);\n  const [messages, setMessages] = useState<ChatMessage[]>([]);\n  const [currentInput, setCurrentInput] = useState('');\n  const [currentOutput, setCurrentOutput] = useState('');\n  const [error, setError] = useState<string | null>(null);\n\n  const [bondScore, setBondScore] = useState(10);\n  const [proLevel, setProLevel] = useState(5);\n  const [currentMood, setCurrentMood] = useState('NEUTRAL');\n\n  const [isSidebarOpen, setIsSidebarOpen] = useState(false);\n  const [chatHistory, setChatHistory] = useState<SavedChat[]>([]);\n  const [activeChatId, setActiveChatId] = useState<string | null>(null);\n\n  /* -------- AUDIO & SESSION REFS -------- */\n  const inputAudioCtx = useRef<AudioContext | null>(null);\n  const outputAudioCtx = useRef<AudioContext | null>(null);\n  const micStream = useRef<MediaStream | null>(null);\n  const processorNode = useRef<ScriptProcessorNode | null>(null);\n  const nextStartTime = useRef(0);\n  const audioSources = useRef<Set<AudioBufferSourceNode>>(new Set());\n\n  const sessionPromiseRef = useRef<any>(null);\n  const activeSessionRef = useRef<any>(null);\n\n  const transcriptEndRef = useRef<HTMLDivElement>(null);\n\n  /* -------------------- EFFECTS -------------------- */\n  useEffect(() => {\n    const saved = localStorage.getItem('hey_buddy_history_v2');\n    if (saved) setChatHistory(JSON.parse(saved));\n  }, []);\n\n  useEffect(() => {\n    transcriptEndRef.current?.scrollIntoView({ behavior: 'smooth' });\n  }, [messages, currentInput, currentOutput]);\n\n  /* -------------------- HELPERS -------------------- */\n  const parseMetadata = (text: string) => {\n    const match = text.match(/\\[METADATA\\]([\\s\\S]*?)\\[\\/METADATA\\]/);\n    if (!match) return text;\n\n    const meta = match[1];\n    const mood = meta.match(/MOOD:\\s*(\\w+)/);\n    const bond = meta.match(/BOND_SCORE:\\s*(\\d+)/);\n    const level = meta.match(/PRO_LEVEL:\\s*(\\d+)/);\n\n    if (mood) setCurrentMood(mood[1]);\n    if (bond) setBondScore(+bond[1]);\n    if (level) setProLevel(+level[1]);\n\n    return text.replace(match[0], '').trim();\n  };\n\n  const moodColor = useMemo(() => {\n    switch (currentMood) {\n      case 'ROMANTIC': return 'rose';\n      case 'DEEP': return 'indigo';\n      case 'HAPPY': return 'amber';\n      default: return 'rose';\n    }\n  }, [currentMood]);\n\n  /* -------------------- CLEANUP -------------------- */\n  const cleanup = () => {\n    setIsConnected(false);\n    setIsProcessing(false);\n\n    processorNode.current?.disconnect();\n    processorNode.current = null;\n\n    micStream.current?.getTracks().forEach(t => t.stop());\n    micStream.current = null;\n\n    inputAudioCtx.current?.close();\n    outputAudioCtx.current?.close();\n    inputAudioCtx.current = null;\n    outputAudioCtx.current = null;\n\n    audioSources.current.forEach(s => s.stop());\n    audioSources.current.clear();\n    nextStartTime.current = 0;\n\n    sessionPromiseRef.current?.then((s: any) => s.close?.());\n    sessionPromiseRef.current = null;\n    activeSessionRef.current = null;\n  };\n\n  /* -------------------- MAIN CONNECT -------------------- */\n  const handleConnect = async () => {\n    if (isConnected) {\n      cleanup();\n      return;\n    }\n\n    try {\n      setError(null);\n      setIsProcessing(true);\n\n      /* üîê VITE ENV FIX */\n      const ai = new GoogleGenAI({\n        apiKey: import.meta.env.VITE_GEMINI_API_KEY,\n      });\n\n      /* üéß AUDIO CONTEXT (USER GESTURE SAFE) */\n      inputAudioCtx.current = new AudioContext({ sampleRate: 16000 });\n      outputAudioCtx.current = new AudioContext({ sampleRate: 24000 });\n\n      /* üî• CRITICAL FIX */\n      await inputAudioCtx.current.resume();\n      await outputAudioCtx.current.resume();\n\n      /* üé§ MIC ACCESS */\n      micStream.current = await navigator.mediaDevices.getUserMedia({\n        audio: {\n          channelCount: 1,\n          sampleRate: 16000,\n          echoCancellation: true,\n          noiseSuppression: true,\n          autoGainControl: true,\n        },\n      });\n\n      const source = inputAudioCtx.current.createMediaStreamSource(micStream.current);\n      processorNode.current = inputAudioCtx.current.createScriptProcessor(256, 1, 1);\n\n      processorNode.current.onaudioprocess = e => {\n        if (!activeSessionRef.current) return;\n        const input = e.inputBuffer.getChannelData(0);\n        activeSessionRef.current.sendRealtimeInput({\n          media: createPcmBlob(input),\n        });\n      };\n\n      source.connect(processorNode.current);\n      processorNode.current.connect(inputAudioCtx.current.destination);\n\n      /* ü§ñ GEMINI LIVE SESSION */\n      const sessionPromise = ai.live.connect({\n        model: 'gemini-2.5-flash-native-audio-preview-12-2025',\n        callbacks: {\n          onopen: () => {\n            setIsConnected(true);\n            setIsProcessing(false);\n          },\n          onmessage: async (m: LiveServerMessage) => {\n            if (m.serverContent?.outputTranscription)\n              setCurrentOutput(p => p + m.serverContent.outputTranscription!.text);\n\n            if (m.serverContent?.inputTranscription)\n              setCurrentInput(p => p + m.serverContent.inputTranscription!.text);\n\n            const audio = m.serverContent?.modelTurn?.parts?.[0]?.inlineData?.data;\n            if (audio && outputAudioCtx.current) {\n              const ctx = outputAudioCtx.current;\n              nextStartTime.current = Math.max(ctx.currentTime, nextStartTime.current);\n\n              const buf = await decodeAudioData(decode(audio), ctx, 24000, 1);\n              const src = ctx.createBufferSource();\n              src.buffer = buf;\n              src.connect(ctx.destination);\n              src.start(nextStartTime.current);\n              nextStartTime.current += buf.duration;\n              audioSources.current.add(src);\n              src.onended = () => audioSources.current.delete(src);\n            }\n\n            if (m.serverContent?.turnComplete) {\n              setMessages(prev => [\n                ...prev,\n                { role: 'user', text: currentInput.trim() },\n                { role: 'model', text: parseMetadata(currentOutput.trim()) },\n              ]);\n              setCurrentInput('');\n              setCurrentOutput('');\n            }\n          },\n          onclose: cleanup,\n          onerror: () => {\n            setError('Connection error');\n            cleanup();\n          },\n        },\n        config: {\n          systemInstruction: SYSTEM_INSTRUCTION,\n          safetySettings: SAFETY_SETTINGS,\n          responseModalities: [Modality.AUDIO],\n          inputAudioTranscription: {},\n          outputAudioTranscription: {},\n          speechConfig: {\n            voiceConfig: {\n              prebuiltVoiceConfig: { voiceName: 'Kore' },\n            },\n          },\n        },\n      });\n\n      sessionPromiseRef.current = sessionPromise;\n      sessionPromise.then((s: any) => (activeSessionRef.current = s));\n    } catch (err) {\n      setError('Microphone permission blocked');\n      setIsProcessing(false);\n    }\n  };\n\n  /* -------------------- UI -------------------- */\n  return (\n    <div className=\"flex flex-col h-screen bg-black text-white\">\n      <header className=\"p-6 flex justify-between items-center\">\n        <h1 className=\"text-2xl font-black\">\n          Hey <span className=\"text-rose-500 italic\">Buddy</span>\n        </h1>\n        <div className=\"flex gap-3\">\n          <span className=\"text-xs bg-rose-500/10 px-3 py-1 rounded-full\">\n            <Zap className=\"inline w-3 h-3 mr-1\" /> Level {proLevel}%\n          </span>\n          <span className=\"text-xs bg-indigo-500/10 px-3 py-1 rounded-full\">\n            <Heart className=\"inline w-3 h-3 mr-1\" /> Bond {bondScore}%\n          </span>\n        </div>\n      </header>\n\n      <main className=\"flex-1 overflow-y-auto px-6 space-y-6\">\n        {messages.map((m, i) => (\n          <div key={i} className={`flex ${m.role === 'user' ? 'justify-end' : 'justify-start'}`}>\n            <div className={`max-w-[80%] p-5 rounded-3xl ${\n              m.role === 'user' ? 'bg-rose-600' : 'bg-white/10'\n            }`}>\n              {m.text}\n            </div>\n          </div>\n        ))}\n        {currentInput && <div className=\"italic opacity-50\">{currentInput}</div>}\n        {currentOutput && <div className=\"opacity-70\">{currentOutput}</div>}\n        <div ref={transcriptEndRef} />\n      </main>\n\n      <footer className=\"p-8 flex flex-col items-center\">\n        <button\n          onClick={handleConnect}\n          disabled={isProcessing}\n          className={`w-24 h-24 rounded-full flex items-center justify-center transition-all ${\n            isConnected ? 'bg-neutral-800' : 'bg-rose-600'\n          }`}\n        >\n          {isConnected ? <MicOff size={40} /> : <Mic size={40} />}\n        </button>\n        <p className=\"mt-4 text-xs uppercase tracking-widest\">\n          {isConnected ? 'Listening...' : 'Connect with Buddy'}\n        </p>\n        {error && <p className=\"text-red-400 text-xs mt-2\">{error}</p>}\n      </footer>\n    </div>\n  );\n};\n\nexport default App;\n"
        }
    ]
}